{"cells":[{"cell_type":"markdown","metadata":{"id":"s_jEsE8W6HbZ"},"source":["# Logistic Regression Model\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"8W2YIMIi7SyF"},"outputs":[],"source":["# Tratamiento de datos\n","# ==============================================================================\n","import numpy as np\n","import pandas as pd\n","\n","# Gráficos\n","# ==============================================================================\n","import matplotlib.pyplot as plt\n","from matplotlib import style\n","import seaborn as sns\n","\n","# Preprocesado y modelado\n","# ==============================================================================\n","from sklearn.datasets import load_breast_cancer\n","from sklearn.model_selection import RandomizedSearchCV, KFold, GridSearchCV, cross_val_score\n","from sklearn.linear_model import LogisticRegression\n","from sklearn.model_selection import train_test_split\n","from sklearn.metrics import accuracy_score\n","from sklearn.preprocessing import StandardScaler\n","from sklearn.pipeline import Pipeline\n","from sklearn import metrics\n","\n","# Configuración matplotlib\n","# ==============================================================================\n","plt.rcParams['image.cmap'] = \"bwr\"\n","plt.rcParams['savefig.bbox'] = \"tight\"\n","style.use('ggplot') or plt.style.use('ggplot')\n","\n","# Configuración warnings\n","# ==============================================================================\n","import warnings\n","warnings.filterwarnings('ignore')\n"]},{"cell_type":"markdown","metadata":{"id":"T5bni26R7hn3"},"source":["Utilizaremos el dataset de cáncer de mama (breast cancer)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"ZL6u47Le7jNh"},"outputs":[],"source":["data = load_breast_cancer()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"hJQzFXQErj1V"},"outputs":[],"source":["print(data.DESCR)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"6CHRdtGYEov0"},"outputs":[],"source":["print(data.keys())"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Fg_liBNfEov0"},"outputs":[],"source":["# Pasamos a un Data frame\n","df = pd.DataFrame(data.data, columns=data.feature_names)\n","# Añadimos una columna con la variable objetivo\n","df['target'] = data.target\n","# Mostramos las primeras filas\n","df.head()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"sc7r5GSFEov0"},"outputs":[],"source":["df.info()"]},{"cell_type":"markdown","metadata":{"collapsed":false,"id":"DwvhkoajEov0"},"source":["Vemos como todas las variables de entradas son `float64`. Además, no faltan valores, todas las columnas tienen 569 valores."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"3IARVc7xEov1"},"outputs":[],"source":["# Número de muestras por clase\n","# ==============================================================================\n","df.target.value_counts().sort_index()"]},{"cell_type":"markdown","metadata":{"collapsed":false,"id":"b1EXioRtEov1"},"source":["Tenemos 212 muestras que no tienen cáncer y 357 que sí tienen cáncer. Aunque están un poco desbalanceadas las clases, lo vamos a dejar así."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"_zbKc7pvEov1"},"outputs":[],"source":["corr_Matrix = df.corr ()\n","f,ax = plt.subplots(figsize=(18, 18))\n","sns.heatmap (corr_Matrix, linewidths = 0.5, annot = True, fmt= '.1f',ax=ax)\n","plt.show ()"]},{"cell_type":"markdown","metadata":{"collapsed":false,"id":"KtmvSnioEov1"},"source":["De la matriz de correlación vemos que las variables \"_mean\" y \"_worst\" se encuentran fuertemente correladas con la variable \"target\" y, además, también se encuentra fuertemente correladas entre ellas (\"mean radius\" tiene una correlación de 1.0 con \"worst radio\"). Por lo tanto, nos vamos a quedar con las variables \"mean\" que tengan al menos una correlación superior a 0.4 con la variable de salida (\"target\")"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"nZYGlE6-Eov1"},"outputs":[],"source":["label = []\n","for i in range (30):\n","    if np.abs(corr_Matrix.target[i]) < 0.4 or i>=10 :\n","        label.append (df.columns.values[i])\n","df.drop (labels = label, axis = 1, inplace = True)\n","df.head ()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"d6jj1SLbEov1"},"outputs":[],"source":["sns.pairplot(df, hue = \"target\")\n","plt.show()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"j7PQK5_08EYF"},"outputs":[],"source":["# Pasamos de dataframe a numpy para poder trabajar con sklearn\n","X = df.iloc [:, 0:6].values\n","y = df.iloc [:, 7].values\n","# dividimos las muestras en entrenamiento y test\n","X_train, X_test, y_train, y_test = train_test_split(X, y,test_size=0.33, random_state=42, shuffle = True)"]},{"cell_type":"markdown","metadata":{"id":"PpkoVv3l-GjJ"},"source":["## LogisticRegression\n","Los parámetros más importantes de la implantación de sklearn (`LogisticRegression`) son:\n","\n","- `penalty`: El tipo de aplicación de regularización. Sus valores pueden ser:{None, 'l2' (por defecto), 'l1', 'elascticnet'}\n","- `C`: (por defecto 1.0) Inverso de la fuerza de regularización; Valores más pequeños especifican una regularización más fuerte.\n","- `solver`: Algoritmo a utilizar en el problema de optimización. Sus valores pueden ser: {‘lbfgs’ (por defecto), ‘liblinear’, ‘newton-cg’, ‘newton-cholesky’, ‘sag’, ‘saga’}. Algunas consideraciones:"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"XeXx3Bsf-LAF"},"outputs":[],"source":["# Probamos el modelo sin aplicar ninguna regularización y con los parámetros por efecto\n","# ==============================================================================\n","scaler = StandardScaler()\n","lr = LogisticRegression(penalty=None, random_state = 42)\n","#lr = LogisticRegression(random_state = 42)\n","\n","pipe_scale_lr = Pipeline([\n","    ('scale', scaler),\n","    ('lr', lr)])\n","\n","# Entrenamiento del modelo\n","# ==============================================================================\n","cv = KFold(n_splits=5, shuffle=True, random_state = 42 )\n","scores = cross_val_score(pipe_scale_lr, X_train, y_train, scoring='accuracy', cv = cv) #OJO!!! scoring=‘balanced_accuracy’\n","                                                                                       # ¿Qué diferencia cv=5?\n","print(f\"All the accuracies are: {scores}\")\n","print(f\"And the average crossvalidation accuracy is: {scores.mean():.2f} +- {scores.std():.2f}\")\n"]},{"cell_type":"markdown","metadata":{"collapsed":false,"id":"fxPP5r9nEov2"},"source":["Ahora procedemos a la búsqueda de los hiperparámteros"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"E896pSb9Eov2"},"outputs":[],"source":["param_grid = [{'lr__penalty': ['l1', 'l2', 'elascticnet'],\n","               'lr__C': [0.001, 0.01, 0.1, 1, 10, 100, 1000],\n","               'lr__solver': ['newton-cg', 'lbfgs', 'liblinear', 'sag', 'saga']}]\n","\n","\n","inner = KFold(n_splits=3, shuffle=True, random_state=42)\n","\n","#budget = 40\n","# Cross-validation (3-fold) para la búsqueda de hiper-parámetros\n","clf = GridSearchCV (estimator  = pipe_scale_lr,\n","                    param_grid = param_grid,\n","                    scoring='accuracy', #OJO!!! scoring=‘balanced_accuracy’\n","                    cv=inner,\n","                    refit=True,\n","                    n_jobs=-1,\n","                    verbose=1,\n","                    return_train_score=True)\n","\n","np.random.seed(42)\n","\n","clf.fit(X=X_train, y=y_train)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"2AvReI_KEov3"},"outputs":[],"source":["resultados = pd.DataFrame(clf.cv_results_)\n","resultados.filter(regex = '(param.*|mean_t|std_t)') \\\n","    .drop(columns = 'params') \\\n","    .sort_values('mean_test_score', ascending = False) \\\n","    .head()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"MIhBzcJ_Eov3"},"outputs":[],"source":["clf.best_params_, clf.best_score_"]},{"cell_type":"markdown","metadata":{"collapsed":false,"id":"A6TSw2M2Eov3"},"source":["Al poner el parámetro `refit=True` se reentrena el modelo indicando los valores óptimos en sus argumentos. Este reentrenamiento se hace automáticamente y el modelo resultante se encuentra almacenado en `.best_estimator_`."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"_E1eWUSTEov3"},"outputs":[],"source":["# Información del modelo\n","# ==============================================================================\n","modelo_final = clf.best_estimator_\n","print(\"Intercept:\", modelo_final['lr'].intercept_)\n","print(\"Coeficientes:\", list(zip(df.columns, modelo_final['lr'].coef_.flatten(), )))\n","print(\"Accuracy de test:\", modelo_final.score(X_test, y_test))"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"yWQW6Iq8Eov3"},"outputs":[],"source":["y_test_pred = modelo_final.predict(X_test)\n","result = metrics.classification_report(y_test, y_test_pred)\n","print(\"Classification Report:\",)\n","print (result)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"xrC7_lQJEov3"},"outputs":[],"source":["# Creates a confusion matrix\n","cm = metrics.confusion_matrix(y_test, y_test_pred)\n","accuracy = metrics.accuracy_score(y_test, y_test_pred)\n","# Transform to df for easier plotting\n","cm_df = pd.DataFrame(cm,\n","                     index = ['Benigno','Maligno'],\n","                     columns = ['Benigno','Maligno'])\n","plt.figure(figsize=(5.5,4))\n","sns.heatmap(cm_df, annot=True)\n","plt.title('Accuracy:{0:.3f}'.format(accuracy))\n","plt.ylabel('True label')\n","plt.xlabel('Predicted label')\n","plt.show()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"CDLFyDDDEov3"},"outputs":[],"source":["#Obtenemos las curva ROC y el área bajo la curva (AUC)\n","\n","probs = modelo_final.predict_proba(X_test)[:, 1]\n","\n","auc = metrics.roc_auc_score(y_test, probs)\n","fpr, tpr, thresholds = metrics.roc_curve(y_test, probs)\n","\n","plt.figure(figsize=(8, 5))\n","plt.plot(fpr, tpr, label=f'AUC  = {auc:.2f}')\n","plt.plot([0, 1], [0, 1], color='blue', linestyle='--', label='Baseline')\n","plt.title('Curva ROC', size=20)\n","plt.xlabel('Falsos Positivos', size=14)\n","plt.ylabel('Verdaderos Positivos', size=14)\n","plt.legend();"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"hWe0TQbgEov3"},"outputs":[],"source":["# Entrenamos con todos los datos para el modelo final\n","_ = modelo_final.fit(X,y)"]},{"cell_type":"markdown","metadata":{"collapsed":false,"id":"vq04-mazEov3"},"source":["# Selección de características mediante métododos de filtrado"]},{"cell_type":"markdown","metadata":{"collapsed":false,"id":"APPkGFTyEov4"},"source":["## Basados en la varianza"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"pysE2IfsEov4"},"outputs":[],"source":["from sklearn.feature_selection import VarianceThreshold\n","selector = VarianceThreshold(threshold=0.01) # Umbral de varianza\n","\n","# Dividimos los df de las variables de entrada y la variable objetivo\n","df_t = df['target']\n","df_f = df.drop('target',axis=1)\n","\n","sel = selector.fit(df_f)\n","sel_index = sel.get_support()\n","df_vt = df_f.iloc[:, sel_index]\n","print(df_vt.columns)"]},{"cell_type":"markdown","metadata":{"collapsed":false,"id":"4Ik_rE3bEov4"},"source":["## Mutual info\n","Calcula el valor de información mutua de cada una de las variables independientes con respecto a la variable dependiente y selecciona las que tienen mayor ganancia de información (al estilo de la ganancia de información de los árboles de decisión). En otras palabras, básicamente mide la dependencia de las características con el valor objetivo. Cuanto mayor sea la puntuación, mayor será la dependencia."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"MW8nD4CcEov4"},"outputs":[],"source":["from sklearn.feature_selection import mutual_info_classif\n","\n","threshold = 4  # the number of most relevant features\n","high_score_features = []\n","feature_scores = mutual_info_classif(df_f, df_t, random_state=42)\n","for score, f_name in sorted(zip(feature_scores, df_f.columns), reverse=True)[:threshold]:\n","    print(f_name, score)\n","    high_score_features.append(f_name)\n","df_mic = df_f[high_score_features]\n","print(df_mic.columns)"]},{"cell_type":"markdown","metadata":{"id":"0AbwOJcPMfKo"},"source":["También podemos seleccionar, por ejemplo, aquellas variable cuya IM sea mayor que un valor (por ejemplo 0.2). Además, como ejemplo, voy a buscar el mejor clasificador usando sólo las variables seleccionadas. Esto mismo lo podemos hacer con el resto de métodos Filter."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"v4mAWCHfMoEX"},"outputs":[],"source":["mi_score_selected_index = np.where(feature_scores >0.2)[0]\n","df_mic2 = df.iloc[:,mi_score_selected_index]\n","X_train2,X_test2,y_train,y_test = train_test_split(df_mic2,df_t,test_size=0.33,\n","                                                     random_state=42,\n","                                                     shuffle = True)\n","# Recordar que \"clf\" hace una búsqueda para los mejores parámetros\n","np.random.seed(42)\n","clf.fit(X=X_train2, y=y_train)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"o9X5gqUuTBdf"},"outputs":[],"source":["clf.best_params_, clf.best_score_"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"n1QzkH22TZsw"},"outputs":[],"source":["y_test_pred = clf.predict(X_test2)\n","result = metrics.classification_report(y_test, y_test_pred)\n","print(\"Classification Report with filtered features:\",)\n","print (result)"]},{"cell_type":"markdown","metadata":{"collapsed":false,"id":"vVNscsXcEov4"},"source":["## F_classif\n","Utiliza el test f de ANOVA para las características y sólo tiene en cuenta la dependencia lineal, a diferencia de la selección de características basada en la información mutua, que puede capturar cualquier tipo de dependencia estadística. Observar que las puntuaciones obtenidas por los distintos métodos son totalmente diferentes. No os quedeis en este punto, cada método ordena internamente la importancia de las características y devuelve las mejores.\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"zoHbmw7SEov4"},"outputs":[],"source":["from sklearn.feature_selection import f_classif\n","threshold = 4 # the number of most relevant features\n","\n","high_score_features = []\n","feature_scores = f_classif(df_f, df_t)[0]\n","for score, f_name in sorted(zip(feature_scores, df_f.columns), reverse=True)[:threshold]:\n","    print(f_name, score)\n","    high_score_features.append(f_name)\n","df_fc = df_f[high_score_features]\n","print(df_fc.columns)"]},{"cell_type":"markdown","metadata":{"collapsed":false,"id":"nEGgVbFkEov4"},"source":["## Chi2\n","Esta clase es en realidad un enfoque más general en comparación con las clases antes mencionadas, ya que toma un parámetro adicional de función de puntuación que establece qué función utilizar en la selección de características. Por lo tanto, se puede considerar como una especie de envoltorio. También podemos utilizar f_classif o mutual_info_class_if dentro de este objeto. Por otra parte, se utiliza típicamente con la función chi2. Este objeto devuelve también los p-values de cada característica según la función de puntuación elegida.\n","\n","El test chi2 mide la dependencia entre variables estocásticas, por lo que podemos eliminar las características que tienen más probabilidades de ser independientes del objetivo utilizando esta función. Sirve básicamente para evaluar si la diferencia entre dos grupos separados de muestras no negativas se debe al azar o no.\n","\n","El test chi2 parte de la hipótesis nula de que dos variables son independientes y de la hipótesis alternativa de que dos variables son dependientes, como la mayoría de las pruebas estadísticas. Mediante el test chi2, se calculan los p-values de cada característica en relación con el objetivo. De forma sencilla, p es la probabilidad de que dos variables sean independientes. Lo que se busca es determinar si las características que dependen del objetivo, es decir, rechazan la hipótesis nula. Por este motivo, seleccionamos las características que suelen tener un p-value inferior a 0,05. El valor umbral de 0,05 es sólo un comportamiento común, puedes establecer valores umbral más pequeños como 0,01 para estar más seguro de que dos grupos son dependientes.\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"-SdVkFVOEov5"},"outputs":[],"source":["from sklearn.feature_selection import SelectKBest\n","from sklearn.feature_selection import chi2\n","threshold = 4  # the number of most relevant features\n","\n","skb = SelectKBest(score_func=chi2, k=threshold)\n","sel_skb = skb.fit(df_f, df_t)\n","sel_skb_index = sel_skb.get_support()\n","df_skb = df_f.iloc[:, sel_skb_index]\n","print('p_values', sel_skb.pvalues_)\n","print(df_skb.columns)"]}],"metadata":{"accelerator":"TPU","colab":{"toc_visible":true,"provenance":[],"gpuType":"V28"},"kernelspec":{"display_name":"Python 3 (ipykernel)","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.9.12"}},"nbformat":4,"nbformat_minor":0}