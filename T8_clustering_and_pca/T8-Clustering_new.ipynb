{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "E0d1NdTBxjrd"
   },
   "source": [
    "# Clustering\n",
    "\n",
    "En este tutorial aprenderemos a trabajar t√©cnicas de clustering. En el tutorial veremos distintos modelos:\n",
    "- Basados en particiones: K-Medias, K-medoids.\n",
    "- M√©todos jer√°rquicos.\n",
    "- Basados en densidad: DBSCAN\n",
    "- An√°lisis de Componentes Princiales y Clustering\n",
    "- Basados en distribuciones: GMMs\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "id": "DAQBHI07C5D6"
   },
   "source": [
    "# Kmeans"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "id": "7gKQ39eiC5D6"
   },
   "source": [
    "En este ejemplo, vamos a generar un conjunto de muestras aleatorias. Comenzamos a generar estos datos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "lKVrLiuUC5D6"
   },
   "outputs": [],
   "source": [
    "# Tratamiento de datos\n",
    "# ==============================================================================\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.datasets import make_blobs\n",
    "\n",
    "# Gr√°ficos\n",
    "# ==============================================================================\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.cm as cm\n",
    "from matplotlib import style\n",
    "style.use('ggplot') or plt.style.use('ggplot')\n",
    "\n",
    "# Preprocesado y modelado\n",
    "# ==============================================================================\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.preprocessing import scale\n",
    "from sklearn.metrics import silhouette_score, silhouette_samples\n",
    "\n",
    "# Configuraci√≥n warnings\n",
    "# ==============================================================================\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "_9IvlRMXC5D7"
   },
   "outputs": [],
   "source": [
    "# Simulaci√≥n de datos\n",
    "# ==============================================================================\n",
    "X, y = make_blobs(\n",
    "    n_samples    = 300,\n",
    "    n_features   = 2,\n",
    "    centers      = 4,\n",
    "    cluster_std  = 0.60,\n",
    "    shuffle      = True,\n",
    "    random_state = 0\n",
    ")\n",
    "\n",
    "fig, ax = plt.subplots(1, 1, figsize=(6, 3.84))\n",
    "ax.scatter(\n",
    "    x = X[:, 0],\n",
    "    y = X[:, 1],\n",
    "    c = 'white',\n",
    "    marker    = 'o',\n",
    "    edgecolor = 'black',\n",
    ")\n",
    "ax.set_title('Datos simulados');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "id": "qfAeQyxlC5D7"
   },
   "source": [
    "La implementaci√≥n de `sklearn.cluster.KMeans` tiene los siguientes par√°metros:\n",
    "- `n_clusters`: determina el n√∫mero *ùêæ* de clusters que se van a generar.\n",
    "- `init`: estrategia para asignar los centroides iniciales. Por defecto se emplea 'k-means++', una estrategia que trata de alejar los centroides lo m√°ximo posible facilitando la convergencia. Sin embargo, esta estrategia puede ralentizar el proceso cuando hay muchos datos, si esto ocurre, es mejor utilizar 'random'.\n",
    "- `n_init`: determina el n√∫mero de veces que se va a repetir el proceso, cada vez con una asignaci√≥n aleatoria inicial distinta. Es recomendable que este √∫ltimo valor sea alto, entre 10-25, para no obtener resultados sub√≥ptimos debido a una iniciaci√≥n poco afortunada del proceso.\n",
    "- `max_iter`: n√∫mero m√°ximo de iteraciones permitidas.\n",
    "- `random_state`: semilla para garantizar la reproducibilidad de los resultados."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "P0XxPVxZC5D7"
   },
   "outputs": [],
   "source": [
    "# Escalado de datos\n",
    "# ==============================================================================\n",
    "X_scaled = scale(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "7AUutnRyC5D7"
   },
   "outputs": [],
   "source": [
    "# Modelo\n",
    "# ==============================================================================\n",
    "modelo_kmeans = KMeans(n_clusters=4, n_init=25, random_state=42)\n",
    "modelo_kmeans.fit(X=X_scaled)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "id": "puv74RouC5D7"
   },
   "source": [
    "El objeto devuelto por `KMeans()` contiene entre otros datos: la media de cada una de las variables para cada cluster (`cluster_centers_`), es decir, los centroides. Un vector indicando a qu√© cluster se ha asignado cada observaci√≥n (`.labels_`) y la suma de los errores cuadr√°ticos dentro del cluster (within-cluster sum of squares o WCSS en ingl√©s) (`.inertia_`). `.inertia_` mide qu√© tan compactos son los clusters. Es decir, cu√°n cerca est√°n los puntos de datos de los centroides de sus respectivos clusters. Una inercia baja indica clusters m√°s compactos y una mejor agrupaci√≥n, en principio. Pero una menor inercia no siempre significa un mejor modelo, porque agregar m√°s clusters siempre reduce la inercia (hasta que cada punto es su propio cluster)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "id": "GZXaNvDaC5D7"
   },
   "source": [
    "Vamos a pintar como nos quedan los clusters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "JxpXou29C5D7"
   },
   "outputs": [],
   "source": [
    "# Clasificaci√≥n con el modelo kmeans\n",
    "# ==============================================================================\n",
    "y_predict = modelo_kmeans.predict(X=X_scaled)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "doDMzOWrC5D8"
   },
   "outputs": [],
   "source": [
    "# Representaci√≥n gr√°fica: grupos originales vs clusters creados\n",
    "# ==============================================================================\n",
    "fig, ax = plt.subplots(1, 1, figsize=(6, 5))\n",
    "for i in np.unique(y):\n",
    "    ax.scatter(\n",
    "        x = X_scaled[y == i, 0],\n",
    "        y = X_scaled[y == i, 1],\n",
    "        c = plt.rcParams['axes.prop_cycle'].by_key()['color'][i],\n",
    "        marker    = 'o',\n",
    "        edgecolor = 'black',\n",
    "        label= f\"Grupo {i}\"\n",
    "    )\n",
    "\n",
    "ax.scatter(\n",
    "    x = modelo_kmeans.cluster_centers_[:, 0],\n",
    "    y = modelo_kmeans.cluster_centers_[:, 1],\n",
    "    c = 'black',\n",
    "    s = 200,\n",
    "    marker = '*',\n",
    "    label  = 'Centroides'\n",
    ")\n",
    "ax.set_title('Clusters generados por Kmeans')\n",
    "ax.legend();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "id": "k37ZV6vfC5D8"
   },
   "source": [
    "Vamos a ver que pasar√≠a si en vez de decirle a KMeans que son 4 clusters, le decimos que K=2 √≥ K=6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "m9BB9BwyC5D8"
   },
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(1, 2, figsize=(10, 4))\n",
    "\n",
    "# Resultados para K = 2\n",
    "# ==============================================================================\n",
    "y_predict = KMeans(n_clusters=2, n_init=25, random_state=42).fit_predict(X=X_scaled)\n",
    "ax[0].scatter(\n",
    "    x = X_scaled[:, 0],\n",
    "    y = X_scaled[:, 1],\n",
    "    c = y_predict,\n",
    "    #cmap='viridis',\n",
    "    marker    = 'o',\n",
    "    edgecolor = 'black'\n",
    ")\n",
    "ax[0].set_title('KMeans K=2');\n",
    "\n",
    "# Resultados para K = 6\n",
    "# ==============================================================================\n",
    "y_predict = KMeans(n_clusters=6, n_init=25, random_state=42).fit_predict(X=X_scaled)\n",
    "ax[1].scatter(\n",
    "    x = X_scaled[:, 0],\n",
    "    y = X_scaled[:, 1],\n",
    "    c = y_predict,\n",
    "    #cmap='viridis',\n",
    "    marker    = 'o',\n",
    "    edgecolor = 'black'\n",
    ")\n",
    "ax[1].set_title('KMeans K=6');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "P4m6LNIfPyiT"
   },
   "source": [
    "## N√∫mero de clusters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "id": "CfmaaeFxC5D8"
   },
   "source": [
    "### M√©todo del codo (Elbow method)\n",
    "La soluci√≥n al problema anterior es hacer una b√∫squeda de cual ser√≠a el K mejor. La idea es obtener para valor de K la suma de los cuadrados de la distancia de cada punto con el centroide al que se encuentra asignado (se obtiene de `.inertia_`). Es una medida de lo coherentes que son internamente los clusters. La idea es que si K crece el valor de `.inertia_` es menor porque cada cluster es m√°s peque√±o. Sin embargo, a partir de cierto punto, la adici√≥n de m√°s clusters proporciona rendimientos decrecientes en t√©rminos de reducci√≥n de `.inertia_`, y la tasa de disminuci√≥n se ralentiza, formando un codo en el gr√°fico. Ese ser√≠a el valor √≥ptimo para seleccionar."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "IOU7J99wC5D8"
   },
   "outputs": [],
   "source": [
    "# M√©todo elbow para identificar el n√∫mero √≥ptimo de clusters\n",
    "# ==============================================================================\n",
    "range_n_clusters = range(1, 15)\n",
    "inertias = []\n",
    "\n",
    "for n_clusters in range_n_clusters:\n",
    "    modelo_kmeans = KMeans(\n",
    "        n_clusters   = n_clusters,\n",
    "        n_init       = 20,\n",
    "        random_state = 42\n",
    "    )\n",
    "    modelo_kmeans.fit(X_scaled)\n",
    "    inertias.append(modelo_kmeans.inertia_)\n",
    "\n",
    "fig, ax = plt.subplots(1, 1, figsize=(6, 3.84))\n",
    "ax.plot(range_n_clusters, inertias, marker='o')\n",
    "ax.set_title(\"Evoluci√≥n de la varianza intra-cluster total\")\n",
    "ax.set_xlabel('N√∫mero clusters')\n",
    "ax.set_ylabel('Intra-cluster (inertia)');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "id": "DtvdUdInC5D8"
   },
   "source": [
    "Vemos como el valor √≥ptimo es K=4, donde se encuentra el codo de la funci√≥n, como ya sab√≠amos."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "id": "lYuBd00eC5D8"
   },
   "source": [
    "### M√©todo Silhouette\n",
    "Algunas veces, con el m√©todo Elbow es dif√≠cil determinar en n√∫mero √≥ptimo de clusters. Otra aproximaci√≥n es el m√©todo Silhouette. Utiliza la distancia media intracl√∫ster y la distancia media cl√∫ster m√°s cercano para cada muestra. Cuanto mayor sea el valor de la puntuaci√≥n, mejor ser√° la estimaci√≥n. Normalmente, las puntuaciones de silhoutte suben y luego bajan hasta alcanzar un n√∫mero √≥ptimo de clusters. Los valores se sit√∫an entre -1,0 y 1,0."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "WJ-NIyVYC5D8"
   },
   "outputs": [],
   "source": [
    "# M√©todo silhouette para identificar el n√∫mero √≥ptimo de clusters\n",
    "# ==============================================================================\n",
    "range_n_clusters = range(2, 15)\n",
    "valores_medios_silhouette = []\n",
    "\n",
    "for n_clusters in range_n_clusters:\n",
    "    modelo_kmeans = KMeans(\n",
    "        n_clusters   = n_clusters,\n",
    "        n_init       = 20,\n",
    "        random_state = 42\n",
    "    )\n",
    "    cluster_labels = modelo_kmeans.fit_predict(X_scaled)\n",
    "    silhouette_avg = silhouette_score(X_scaled, cluster_labels)\n",
    "    valores_medios_silhouette.append(silhouette_avg)\n",
    "\n",
    "fig, ax = plt.subplots(1, 1, figsize=(6, 3.84))\n",
    "ax.plot(range_n_clusters, valores_medios_silhouette, marker='o')\n",
    "ax.set_title(\"Evoluci√≥n de media de los √≠ndices silhouette\")\n",
    "ax.set_xlabel('N√∫mero clusters')\n",
    "ax.set_ylabel('Media √≠ndices silhouette');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9eIsHfM1_BO4"
   },
   "source": [
    "Podemos tambi√©n dibujar los gr√°ficos de silhouette para un conjunto de clusters. Los gr√°ficos los haremos para 2, 3, 4, 5, y 6."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "OwgpVRET_Ay7"
   },
   "outputs": [],
   "source": [
    "# Gr√°ficos silhouette para identificar el n√∫mero √≥ptimo de clusters\n",
    "# ==============================================================================\n",
    "\n",
    "range_n_clusters = [2, 3, 4, 5, 6]\n",
    "\n",
    "for n_clusters in range_n_clusters:\n",
    "    # Crear un subplot con 1 fila y dos columnas\n",
    "    fig, (ax1, ax2) = plt.subplots(1, 2)\n",
    "    fig.set_size_inches(18, 7)\n",
    "\n",
    "    # El primer subplot es el gr√°fico silhouette\n",
    "    # El coeficiente de silhouette puede oscilar entre -1, 1 pero en este ejemplo\n",
    "    # todos se encuentran dentro de [-0.1, 1]\n",
    "    ax1.set_xlim([-0.1, 1])\n",
    "    # El (n_clusters+1)*10 sirve para insertar un espacio en blanco entre los\n",
    "    # gr√°ficos de silhouette de los clusters individuales, para delimitarlos claramente.\n",
    "    ax1.set_ylim([0, len(X_scaled) + (n_clusters + 1) * 10])\n",
    "\n",
    "    # Inicializar el clusterer con n_clusters valor y un generador aleatorio\n",
    "    # aleatorio de 42 para la reproducibilidad.\n",
    "    clusterer = KMeans(n_clusters=n_clusters, random_state=42)\n",
    "    cluster_labels = clusterer.fit_predict(X_scaled)\n",
    "\n",
    "    # El silhouette_score da el valor medio de todas las muestras. Esto da una\n",
    "    # perspectiva de la densidad y separaci√≥n de los clusters formados\n",
    "    silhouette_avg = silhouette_score(X_scaled, cluster_labels)\n",
    "    print(\n",
    "        \"Para n_clusters =\",\n",
    "        n_clusters,\n",
    "        \"La media de silhouette_score es :\",\n",
    "        silhouette_avg,\n",
    "    )\n",
    "\n",
    "    # Calcular las puntuaciones de silhouette de cada muestra\n",
    "    sample_silhouette_values = silhouette_samples(X_scaled, cluster_labels)\n",
    "\n",
    "    y_lower = 10\n",
    "    for i in range(n_clusters):\n",
    "        # Agregar las puntuaciones de silhouette de las muestras pertenecientes al\n",
    "        # cluster i, y ord√©nelas\n",
    "        ith_cluster_silhouette_values = sample_silhouette_values[cluster_labels == i]\n",
    "\n",
    "        ith_cluster_silhouette_values.sort()\n",
    "\n",
    "        size_cluster_i = ith_cluster_silhouette_values.shape[0]\n",
    "        y_upper = y_lower + size_cluster_i\n",
    "\n",
    "        color = cm.nipy_spectral(float(i) / n_clusters)\n",
    "        ax1.fill_betweenx(\n",
    "            np.arange(y_lower, y_upper),\n",
    "            0,\n",
    "            ith_cluster_silhouette_values,\n",
    "            facecolor=color,\n",
    "            edgecolor=color,\n",
    "            alpha=0.7,\n",
    "        )\n",
    "\n",
    "        # Etiquetar los gr√°ficos de silhouette con sus n√∫meros de cluster en el\n",
    "        # centro.\n",
    "        ax1.text(-0.05, y_lower + 0.5 * size_cluster_i, str(i))\n",
    "\n",
    "        # Calcular el nuevo y_lower para el siguiente gr√°fico\n",
    "        y_lower = y_upper + 10\n",
    "\n",
    "    ax1.set_title(\"El gr√°fico silhouette para varios clusters.\")\n",
    "    ax1.set_xlabel(\"Los valores del coeficiente de silueta\")\n",
    "    ax1.set_ylabel(\"Etiqueta del cluster\")\n",
    "\n",
    "    # La l√≠nea vertical para la puntuaci√≥n media de silhouette de todos los valores\n",
    "    ax1.axvline(x=silhouette_avg, color=\"red\", linestyle=\"--\")\n",
    "\n",
    "    ax1.set_yticks([])\n",
    "    ax1.set_xticks([-0.1, 0, 0.2, 0.4, 0.6, 0.8, 1])\n",
    "\n",
    "    # El segundo plot dibuja los clusters formados\n",
    "    colors = cm.nipy_spectral(cluster_labels.astype(float) / n_clusters)\n",
    "    ax2.scatter(\n",
    "        X[:, 0], X[:, 1], marker=\".\", s=30, lw=0, alpha=0.7, c=colors, edgecolor=\"k\"\n",
    "    )\n",
    "\n",
    "\n",
    "    ax2.set_title(\"Visualizaci√≥n de los datos clusterizados.\")\n",
    "    ax2.set_xlabel(\"Espacio de caracter√≠sticas para la 1¬™ caracter√≠stica\")\n",
    "    ax2.set_ylabel(\"Espacio de caracter√≠sticas para la 2¬™ caracter√≠stica\")\n",
    "\n",
    "    plt.suptitle(\n",
    "        \"An√°lisis silhouette para la agrupaci√≥n KMeans en datos de muestra con n_clusters = %d\"\n",
    "        % n_clusters,\n",
    "        fontsize=14,\n",
    "        fontweight=\"bold\",\n",
    "    )\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "VtH_v59KV2GW"
   },
   "source": [
    "En este ejemplo se utiliza el an√°lisis de silhouette para elegir un valor √≥ptimo para n_clusters. El gr√°fico de silhouette muestra que los valores n_clusters de 3, 5 y 6 son una mala elecci√≥n para los datos dados debido a la presencia de clusters con puntuaciones de silueta por debajo de la media y tambi√©n debido a las amplias fluctuaciones en el tama√±o de los gr√°ficos de silhouette. El an√°lisis de siluetas es m√°s ambivalente a la hora de decidir entre 2 y 4.\n",
    "\n",
    "Tambi√©n se puede visualizar el tama√±o del conglomerado a partir del grosor del gr√°fico de silhouette. El gr√°fico de silhouette del cluster 0 cuando n_clusters es igual a 2, es m√°s grande debido a la agrupaci√≥n de los 3 subclusters en un cluster grande. Sin embargo, cuando n_clusters es igual a 4, todos los gr√°ficos tienen m√°s o menos el mismo grosor y, por lo tanto, el mismo tama√±o, como puede comprobarse en el gr√°fico de dispersi√≥n etiquetado en t\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "id": "3QeUH5TlC5D9"
   },
   "source": [
    "# Cluster jer√°rquico\n",
    "\n",
    "Usaremos el dataset USArrests que contiene estad√≠sticas sobre arrestos por cada 100,000 residentes por asalto, asesinato y violaci√≥n en cada uno de los 50 estados de EE. UU. en 1973. Tambi√©n se da el porcentaje de la poblaci√≥n que vive en √°reas urbanas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "pQKkaZafC5D9"
   },
   "outputs": [],
   "source": [
    "# Tratamiento de datos\n",
    "# ==============================================================================\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import statsmodels.api as sm\n",
    "import seaborn as sns\n",
    "\n",
    "# Gr√°ficos\n",
    "# ==============================================================================\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib import style\n",
    "style.use('ggplot') or plt.style.use('ggplot')\n",
    "\n",
    "# Preprocesado y modelado\n",
    "# ==============================================================================\n",
    "from sklearn.cluster import AgglomerativeClustering\n",
    "from scipy.cluster.hierarchy import dendrogram\n",
    "from sklearn.preprocessing import scale\n",
    "from sklearn.metrics import silhouette_score\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "# Configuraci√≥n warnings\n",
    "# ==============================================================================\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "id": "EK2GyQm-C5D9"
   },
   "source": [
    "Datos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "6kAol_JAC5ED"
   },
   "outputs": [],
   "source": [
    "USArrests = sm.datasets.get_rdataset(\"USArrests\", \"datasets\")\n",
    "USArrests['StateAbbrv'] = [\"AL\", \"AK\", \"AZ\", \"AR\", \"CA\", \"CO\", \"CT\",\"DE\", \"FL\", \"GA\", \"HI\", \"ID\", \"IL\", \"IN\", \"IA\", \"KS\", \"KY\", \"LA\", \"ME\", \"MD\", \"MA\", \"MI\", \"MN\", \"MS\", \"MO\", \"MT\", \"NE\", \"NV\", \"NH\", \"NJ\",\"NM\", \"NY\", \"NC\", \"ND\", \"OH\", \"OK\", \"OR\", \"PA\", \"RI\", \"SC\",\"SD\", \"TN\", \"TX\", \"UT\", \"VT\", \"VA\", \"WA\", \"WV\", \"WI\", \"WY\"]\n",
    "\n",
    "datos = USArrests.data\n",
    "datos.head(4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "id": "ue9Ynij4C5ED"
   },
   "source": [
    "Los escalamos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Do9YGeeTC5ED"
   },
   "outputs": [],
   "source": [
    "# Escalado de las variables\n",
    "# ==============================================================================\n",
    "datos_scaled = scale(X=datos, axis=0, with_mean=True, with_std=True)\n",
    "datos_scaled = pd.DataFrame(datos_scaled, columns=datos.columns, index=datos.index)\n",
    "datos_scaled.head(4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "APHTcE9AC5ED"
   },
   "outputs": [],
   "source": [
    "def plot_dendrogram(model, **kwargs):\n",
    "    '''\n",
    "    Esta funci√≥n extrae la informaci√≥n de un modelo AgglomerativeClustering\n",
    "    y representa su dendograma con la funci√≥n dendogram de scipy.cluster.hierarchy\n",
    "    '''\n",
    "\n",
    "    counts = np.zeros(model.children_.shape[0])\n",
    "    n_samples = len(model.labels_)\n",
    "    for i, merge in enumerate(model.children_):\n",
    "        current_count = 0\n",
    "        for child_idx in merge:\n",
    "            if child_idx < n_samples:\n",
    "                current_count += 1  # leaf node\n",
    "            else:\n",
    "                current_count += counts[child_idx - n_samples]\n",
    "        counts[i] = current_count\n",
    "\n",
    "    linkage_matrix = np.column_stack([model.children_, model.distances_,\n",
    "                                      counts]).astype(float)\n",
    "\n",
    "    # Plot\n",
    "    dendrogram(linkage_matrix, **kwargs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "id": "DyWn04mmC5ED"
   },
   "source": [
    "Los par√°metros de la clase `sklearn.cluster.AgglomerativeClustering` que destacan:\n",
    "\n",
    "- `n_clusters`: determina el n√∫mero de clusters que se van a generar. En su lugar, su valor puede ser None si se quiere utilizar el criterio `distance_threshold` para crear los clusters o crecer todo el dendograma.\n",
    "- `distance_threshold`: distancia (altura del dendograma) a partir de la cual se dejan de unir los clusters. Indicar `distance_threshold=0` para crecer todo el √°rbol.\n",
    "- `compute_full_tree`: si se calcula la jerarqu√≠a completa de clusters. Debe ser `True` si `distance_threshold` es distinto de `None`.\n",
    "- `affinity`: m√©trica utilizada como distancia. Puede ser: ‚Äúeuclidean‚Äù, ‚Äúl1‚Äù, ‚Äúl2‚Äù, ‚Äúmanhattan‚Äù, ‚Äúcosine‚Äù, or ‚Äúprecomputed‚Äù. Si se utiliza `linkage=‚Äúward‚Äù`, solo se permite ‚Äúeuclidean‚Äù.\n",
    "- `linkage: tipo de linkage utilizado. Puede ser ‚Äúward‚Äù, ‚Äúcomplete‚Äù, ‚Äúaverage‚Äù o ‚Äúsingle‚Äù.\n",
    "\n",
    "Al aplicar un hierarchical clustering aglomerativo se tiene que escoger una medida de distancia y un tipo de linkage. El concepto de linkage contempla la forma que tenemos de medir la distancia entre pares de grupos. Tenemos varias formas:\n",
    "- `\"ward\"`. La elecci√≥n de los pares de clusters que se fusionan en cada etapa del *agglomerative hierarchical clustering* se fundamenta en el valor √≥ptimo de una funci√≥n objetivo, la cual puede ser cualquier funci√≥n determinada por el analista. El m√©todo de *Ward's minimum variance* es un ejemplo espec√≠fico en el cual el prop√≥sito es reducir al m√≠nimo la suma total de la varianza intra-cluster. Durante cada fase, se identifican aquellos 2 clusters cuya fusi√≥n resulta en el menor aumento de la varianza total intra-cluster. Este indicador es an√°logo a la m√©trica que se busca minimizar en el algoritmo K-means.\n",
    "- `\"complete\"`. Se calcula la distancia entre todos los posibles pares formados por una observaci√≥n del cluster A y una del cluster B. La mayor de todas ellas se selecciona como la distancia entre los dos clusters. Se trata de la medida m√°s conservadora\n",
    "- `\"average\"`. Se calcula la distancia entre todos los posibles pares formados por una observaci√≥n del cluster A y una del cluster B. El valor promedio de todas ellas se selecciona como la distancia entre los dos clusters.\n",
    "- `\"single\"`.Se calcula la distancia entre todos los posibles pares formados por una observaci√≥n del cluster A y una del cluster B. La menor de todas ellas se selecciona como la distancia entre los dos clusters. Se trata de la medida menos conservadora\n",
    "\n",
    "A continuaci√≥n, se comparan los resultados con los linkages complete, ward y average, utilizando la distancia eucl√≠dea como m√©trica de similitud."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "3jLitY4qC5ED"
   },
   "outputs": [],
   "source": [
    "# Modelos\n",
    "# ==============================================================================\n",
    "modelo_hclust_complete = AgglomerativeClustering(\n",
    "    linkage  = 'complete',\n",
    "    distance_threshold = 0,\n",
    "    n_clusters         = None\n",
    ")\n",
    "modelo_hclust_complete.fit(X=datos_scaled)\n",
    "\n",
    "modelo_hclust_average = AgglomerativeClustering(\n",
    "    linkage  = 'average',\n",
    "    distance_threshold = 0,\n",
    "    n_clusters         = None\n",
    ")\n",
    "modelo_hclust_average.fit(X=datos_scaled)\n",
    "\n",
    "modelo_hclust_ward = AgglomerativeClustering(\n",
    "    linkage  = 'ward',\n",
    "    distance_threshold = 0,\n",
    "    n_clusters         = None\n",
    ")\n",
    "modelo_hclust_ward.fit(X=datos_scaled)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "jlf-FKscC5ED"
   },
   "outputs": [],
   "source": [
    "# Dendrogramas\n",
    "# ==============================================================================\n",
    "fig, axs = plt.subplots(3, 1, figsize=(8, 8))\n",
    "plot_dendrogram(modelo_hclust_average, labels=datos_scaled.index, color_threshold=0, ax=axs[0])\n",
    "axs[0].set_title(\"Linkage average\")\n",
    "plot_dendrogram(modelo_hclust_complete, labels=datos_scaled.index, color_threshold=0, ax=axs[1])\n",
    "axs[1].set_title(\"Linkage complete\")\n",
    "plot_dendrogram(modelo_hclust_ward, labels=datos_scaled.index, color_threshold=0, ax=axs[2])\n",
    "axs[2].set_title(\"Linkage ward\")\n",
    "plt.tight_layout();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "id": "xEMhzuKvC5ED"
   },
   "source": [
    "## N√∫mero de clusters\n",
    "Una forma de decidir el n√∫mero de clusters, puede ser inspeccionando el dendograma y elegir la altura a la que se corta para generar los clusters. Por ejemplo, para los resultados de distancia euclidea y linkage ward, parece que una buena altura ser√≠a 5 y tendr√≠amos 4 clusters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "rVE98OxbC5EE"
   },
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(1, 1, figsize=(8, 4))\n",
    "altura_corte = 5\n",
    "plot_dendrogram(modelo_hclust_ward, labels=datos_scaled.index, color_threshold=altura_corte, ax=ax)\n",
    "ax.set_title(\"Linkage ward\")\n",
    "ax.axhline(y=altura_corte, c = 'black', linestyle='--', label='altura corte')\n",
    "ax.legend();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "id": "Q-hmsjhyC5EE"
   },
   "source": [
    "Otra forma de identificar potenciales valores √≥ptimos para el n√∫mero de clusters en modelos hierarchical clustering es mediante los √≠ndices silhouette."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "B13Mo1rzC5EE"
   },
   "outputs": [],
   "source": [
    "# M√©todo silhouette para identificar el n√∫mero √≥ptimo de clusters\n",
    "# ==============================================================================\n",
    "range_n_clusters = range(2, 15)\n",
    "valores_medios_silhouette = []\n",
    "\n",
    "for n_clusters in range_n_clusters:\n",
    "    modelo = AgglomerativeClustering(\n",
    "        linkage    = 'ward',\n",
    "        n_clusters = n_clusters\n",
    "    )\n",
    "\n",
    "    cluster_labels = modelo.fit_predict(X_scaled)\n",
    "    silhouette_avg = silhouette_score(X_scaled, cluster_labels)\n",
    "    valores_medios_silhouette.append(silhouette_avg)\n",
    "\n",
    "fig, ax = plt.subplots(1, 1, figsize=(6, 3.84))\n",
    "ax.plot(range_n_clusters, valores_medios_silhouette, marker='o')\n",
    "ax.set_title(\"Evoluci√≥n de media de los √≠ndices silhouette\")\n",
    "ax.set_xlabel('N√∫mero clusters')\n",
    "ax.set_ylabel('Media √≠ndices silhouette');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "YWxSITr7C5EE"
   },
   "outputs": [],
   "source": [
    "# Modelo final\n",
    "# ==============================================================================\n",
    "modelo_hclust_ward = AgglomerativeClustering(\n",
    "    linkage  = 'ward',\n",
    "    n_clusters = 4\n",
    ")\n",
    "modelo_hclust_ward.fit(X=X_scaled)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "r0IqC_isDh6n"
   },
   "source": [
    "# DBSCAN: Density-based spatial clustering of applications with noise\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "lByVf078C5EE"
   },
   "outputs": [],
   "source": [
    "# Tratamiento de datos\n",
    "# ==============================================================================\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.datasets import make_blobs\n",
    "\n",
    "# Gr√°ficos\n",
    "# ==============================================================================\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib import style\n",
    "style.use('ggplot') or plt.style.use('ggplot')\n",
    "\n",
    "# Preprocesado y modelado\n",
    "# ==============================================================================\n",
    "from sklearn.cluster import DBSCAN\n",
    "from sklearn.preprocessing import scale\n",
    "from sklearn.metrics import silhouette_score\n",
    "\n",
    "# Configuraci√≥n warnings\n",
    "# ==============================================================================\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ZnnDPhIIC5EF"
   },
   "outputs": [],
   "source": [
    "datos = pd.read_csv('DBSCAN.csv')\n",
    "datos.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "id": "W1rhwaCRC5EF"
   },
   "source": [
    "Con la clase `sklearn.cluster.DBSCAN` de Scikit-Learn se pueden entrenar modelos de clustering utilizando el algoritmo DBSCAN. Entre sus par√°metros destacan:\n",
    "- `eps`: Distancia m√°xima entre dos muestras para que una se considere vecina de la otra. Define el *ùúñ-neighborhood*\n",
    "- `min_samples`: El n√∫mero de muestras (o peso total) en un vecindario para que un punto se considere un *core point*. Esto incluye el propio punto. Si `min_samples` se establece en un valor m√°s alto, DBSCAN encontrar√° conglomerados m√°s densos, mientras que si se establece en un valor m√°s bajo, los conglomerados encontrados ser√°n m√°s dispersos.\n",
    "- `metric``: m√©trica utilizada como distancia. Puede ser: ‚Äúeuclidean‚Äù, ‚Äúl1‚Äù, ‚Äúl2‚Äù, ‚Äúmanhattan‚Äù, ‚Äúcosine‚Äù, or ‚Äúprecomputed‚Äù. Por defecto es ‚Äúeuclidean‚Äù."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "9tHrTXGQC5EF"
   },
   "outputs": [],
   "source": [
    "# Escalado de datos\n",
    "# ==============================================================================\n",
    "X = datos.drop(columns='shape').to_numpy()\n",
    "X_scaled = scale(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "OQQvukFHF-iH"
   },
   "outputs": [],
   "source": [
    "# Visualizado de datos\n",
    "# ==============================================================================\n",
    "\n",
    "fig, ax = plt.subplots(1, 1, figsize=(4.5, 4.5))\n",
    "\n",
    "ax.scatter(\n",
    "    x = X[:, 0],\n",
    "    y = X[:, 1],\n",
    "    c = 'black',\n",
    "    marker    = 'o',\n",
    "    edgecolor = 'black'\n",
    ")\n",
    "#ax.legend()\n",
    "ax.set_title('Nube de puntos iniciales');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "InfpLf4AC5EF"
   },
   "outputs": [],
   "source": [
    "# Modelo\n",
    "# ==============================================================================\n",
    "modelo_dbscan = DBSCAN(\n",
    "    eps          = 0.2,\n",
    "    min_samples  = 5,\n",
    "    metric       = 'euclidean',\n",
    ")\n",
    "\n",
    "modelo_dbscan.fit(X=X_scaled)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "msu_B9hiC5EF"
   },
   "outputs": [],
   "source": [
    "# Clasificaci√≥n\n",
    "# ==============================================================================\n",
    "labels = modelo_dbscan.labels_\n",
    "\n",
    "fig, ax = plt.subplots(1, 1, figsize=(4.5, 4.5))\n",
    "\n",
    "ax.scatter(\n",
    "    x = X[:, 0],\n",
    "    y = X[:, 1],\n",
    "    c = labels,\n",
    "    marker    = 'o',\n",
    "    edgecolor = 'black'\n",
    ")\n",
    "\n",
    "# Los outliers se identifican con el label -1\n",
    "ax.scatter(\n",
    "    x = X[labels == -1, 0],\n",
    "    y = X[labels == -1, 1],\n",
    "    c = 'red',\n",
    "    marker    = 'o',\n",
    "    edgecolor = 'black',\n",
    "    label = 'outliers'\n",
    ")\n",
    "\n",
    "ax.legend()\n",
    "ax.set_title('Clusterings generados por DBSCAN');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "7YQveIDLC5EF"
   },
   "outputs": [],
   "source": [
    "# N√∫mero de clusters y observaciones \"outliers\"\n",
    "# ==============================================================================\n",
    "n_clusters = len(set(labels)) - (1 if -1 in labels else 0)\n",
    "n_noise    = list(labels).count(-1)\n",
    "\n",
    "print(f'N√∫mero de clusters encontrados: {n_clusters}')\n",
    "print(f'N√∫mero de outliers encontrados: {n_noise}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2ySZkl8yRS44"
   },
   "source": [
    "# An√°lisis de Componentes Principales y Clustering\n",
    "Volvemos con la base de datos de USArrests.\n",
    "\n",
    "Como hemos visto, para tener una visualizaci√≥n de las agrupaciones, tendr√≠amos que reducir nuestro problema a dos o tres variables de entrada.\n",
    "\n",
    "En problemas en los que las variables de entrada est√©n muy correladas, podemos pensar en reducir el n√∫mero de variables utilizando PCA (An√°lisis de Componentes Principales)\n",
    "\n",
    "Vamos a hacer un an√°lisis exploratorio inicial viendo las correlaciones entre variables. esto nos dar√° pie a ver si podemos usar PCA m√°s adelante."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "7A6EB9Wf8gi_"
   },
   "outputs": [],
   "source": [
    "USArrests = sm.datasets.get_rdataset(\"USArrests\", \"datasets\")\n",
    "USArrests['StateAbbrv'] = [\"AL\", \"AK\", \"AZ\", \"AR\", \"CA\", \"CO\", \"CT\",\"DE\", \"FL\", \"GA\", \"HI\", \"ID\", \"IL\", \"IN\", \"IA\", \"KS\", \"KY\", \"LA\", \"ME\", \"MD\", \"MA\", \"MI\", \"MN\", \"MS\", \"MO\", \"MT\", \"NE\", \"NV\", \"NH\", \"NJ\",\"NM\", \"NY\", \"NC\", \"ND\", \"OH\", \"OK\", \"OR\", \"PA\", \"RI\", \"SC\",\"SD\", \"TN\", \"TX\", \"UT\", \"VT\", \"VA\", \"WA\", \"WV\", \"WI\", \"WY\"]\n",
    "\n",
    "datos = USArrests.data\n",
    "datos.head(4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "zNZneJ1l9WFw"
   },
   "outputs": [],
   "source": [
    "# Escalado de las variables\n",
    "# ==============================================================================\n",
    "datos_scaled = scale(X=datos, axis=0, with_mean=True, with_std=True)\n",
    "datos_scaled_df = pd.DataFrame(datos_scaled, columns=datos.columns, index=datos.index)\n",
    "datos_scaled_df.head(4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "w4BKxf8wC5D6"
   },
   "outputs": [],
   "source": [
    "estados = datos.index\n",
    "corr_df = datos.corr()\n",
    "etiquetas = corr_df.columns\n",
    "\n",
    "mask_ut=np.triu(np.ones(corr_df.shape)).astype(bool)\n",
    "sns.heatmap(corr_df, mask=mask_ut, annot=True, cmap=\"coolwarm\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "YBXEhvPUgCFi"
   },
   "source": [
    "Por lo que se v√©, existe una correlaci√≥n significativa entre algunas variables."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "id": "PxnOXaEdC5EE"
   },
   "source": [
    "## PCA y visualizaci√≥n\n",
    "En este problema tenemos cuatro variables de entrada. Para poder representar gr√°ficamente estos datos, necesitamos que sean dos o tres variables. Por este motivo vamos a transformar los datos de entrada mediante PCA de dos dimensiones."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "tyFyyE5NC5EE"
   },
   "outputs": [],
   "source": [
    "np.set_printoptions(precision=2)\n",
    "\n",
    "X = datos_scaled_df.values.squeeze()\n",
    "\n",
    "pca = PCA(n_components=2)\n",
    "\n",
    "X_trans = pca.fit_transform(X)\n",
    "\n",
    "df_pca = pd.DataFrame(X_trans, columns=['PC1','PC2'])\n",
    "df_pca.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "1HwN9gmTC5EE"
   },
   "outputs": [],
   "source": [
    "std = df_pca.describe().transpose()[\"std\"]\n",
    "print(f\"Proporci√≥n de varianza explicada: {pca.explained_variance_ratio_}\")\n",
    "print(f\"Proporci√≥n acumulada: {np.cumsum(pca.explained_variance_ratio_)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "uK8nylsWgqvC"
   },
   "source": [
    "Como podemos observar, con dos variables, podemos contemplar el 87% de la varianza explicada del problema. Esto nos permite dibujar las muestras en las nuevas coordenadas PC1 y PC2 como aparece a continuaci√≥n."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "bttsoKFKe1Bx"
   },
   "outputs": [],
   "source": [
    "sns.scatterplot(data=df_pca, x=\"PC1\", y=\"PC2\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qbxjIrfqjIv-"
   },
   "source": [
    "Cuando se utiliza PCA, el gr√°fico biplot nos da informaci√≥n de c√≥mo se ha realizado la transformaci√≥n. Nos muestra las muestras en el espacio transformado bidimensional, junto con unos vectores en los que se representan las variables de entrada. De los vectores nos fijamos en:\n",
    "* **√Ångulo**: cuanto m√°s paralelo es un vector al eje de una componente, m√°s ha contribuido a la creaci√≥n de la misma. Con ello obtienes informaci√≥n sobre qu√© variable(s) ha sido m√°s determinante para crear cada componente, y si entre las variables (y cuales) hay correlaciones. √Ångulos peque√±os entre vectores representa alta correlaci√≥n entre las variables implicadas (observaciones con valores altos en una de esas variables tendr√° valores altos en la variable o variables correlacionadas); √°ngulos rectos representan falta de correlaci√≥n, y √°ngulos opuestos representan correlaci√≥n negativa (una observaci√≥n con valores altos en una de las variables ir√° acompa√±ado de valores bajos en la otra).\n",
    "* **Longitud**: cuanto mayor la longitud de un vector relacionado con x variable, mayor variabilidad de dicha variable est√° contenida en la representaci√≥n de las dos componentes del biplot, es decir, mejor est√° representada su informaci√≥n en el gr√°fico."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "II8MKPzhC5EE"
   },
   "outputs": [],
   "source": [
    "# Funci√≥n para dibujar un gr√°fico biplot\n",
    "# ==============================================================================\n",
    "\n",
    "def biplot(scaled_data, fitted_pca, original_dim_labels, point_labels):\n",
    "\n",
    "    pca_results = fitted_pca.transform(scaled_data)\n",
    "    pca1_scores = pca_results[:,0]\n",
    "    pca2_scores = pca_results[:,1]\n",
    "\n",
    "    # plot each point in 2D post-PCA space\n",
    "    plt.scatter(pca1_scores,pca2_scores)\n",
    "\n",
    "    # label each point\n",
    "    for i in range(len(pca1_scores)):\n",
    "        plt.text(pca1_scores[i],pca2_scores[i], point_labels[i])\n",
    "\n",
    "    #for each original dimension, plot what an increase of 1 in that dimension means in this space\n",
    "    for i in range(fitted_pca.components_.shape[1]):\n",
    "        raw_dims_delta_on_pca1 = fitted_pca.components_[0,i]\n",
    "        raw_dims_delta_on_pca2 = fitted_pca.components_[1,i]\n",
    "        plt.arrow(0, 0, raw_dims_delta_on_pca1, raw_dims_delta_on_pca2 ,color = 'r',alpha = 1)\n",
    "        plt.text(raw_dims_delta_on_pca1*1.1, raw_dims_delta_on_pca2*1.1, original_dim_labels[i], color = 'g', ha = 'center', va = 'center')\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "mSzJaJbVkTAY"
   },
   "source": [
    "Dibujamos el gr√°fico biplot."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "r_rYqWWYAo-p"
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(8.5,8.5))\n",
    "plt.xlim(-3.5,3.5)\n",
    "plt.ylim(-3.5,3.5)\n",
    "plt.xlabel(\"PC{}\".format(1))\n",
    "plt.ylabel(\"PC{}\".format(2))\n",
    "plt.title(\"Gr√°fico biplot para PCA\")\n",
    "plt.grid()\n",
    "biplot(datos_scaled, PCA().fit(datos_scaled),\n",
    "       original_dim_labels=datos.columns,\n",
    "       point_labels=USArrests['StateAbbrv'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "NGC-0z8ckdbY"
   },
   "source": [
    "Vemos como las variables `UrbanPop` y `Murder`no tienen pr√°cticamente correlaci√≥n. Igualmente `Murder`y `Assault`s√≠ que est√°n correladas."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "M9TrQcOqhNho"
   },
   "source": [
    "Visualmente es complicado determinar cuantas agrupaciones tenemos, usaremos el m√©todo silhouette que identifica el n√∫mero √≥ptimo de clusters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ZEcGlsNme1Bx"
   },
   "outputs": [],
   "source": [
    "# M√©todo silhouette para identificar el n√∫mero √≥ptimo de clusters (ahora con los coeficientes de la PCA)\n",
    "# ==============================================================================\n",
    "range_n_clusters = range(2, 15)\n",
    "valores_medios_silhouette = []\n",
    "\n",
    "for n_clusters in range_n_clusters:\n",
    "    modelo_kmeans = KMeans(\n",
    "        n_clusters   = n_clusters,\n",
    "        n_init       = 20,\n",
    "        random_state = 42\n",
    "    )\n",
    "    cluster_labels = modelo_kmeans.fit_predict(X_trans)\n",
    "    silhouette_avg = silhouette_score(X_trans, cluster_labels)\n",
    "    valores_medios_silhouette.append(silhouette_avg)\n",
    "\n",
    "fig, ax = plt.subplots(1, 1, figsize=(6, 3.84))\n",
    "ax.plot(range_n_clusters, valores_medios_silhouette, marker='o')\n",
    "ax.set_title(\"Evoluci√≥n de media de los √≠ndices silhouette (PCA)\")\n",
    "ax.set_xlabel('N√∫mero clusters')\n",
    "ax.set_ylabel('Media √≠ndices silhouette');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "j-eJ2hG5e1Bx"
   },
   "source": [
    "Parece que el n√∫mero ideal de clusters es de 2, pero 4 tampoco es un mal resultado. Elegimos 4 para que nos d√© m√°s juego a la hora del an√°lisis final. Vamos a ver el resultado con un n√∫mero de clusters de 4 y los m√©todos KMeans y DBSCAN."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "QUPau2J3e1By"
   },
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(1, 2, figsize=(10, 4))\n",
    "\n",
    "# Clustering Kmeans\n",
    "\n",
    "y_predict_Kmeans = KMeans(n_clusters=4, n_init=25, random_state=42).fit_predict(X=X_trans)\n",
    "\n",
    "for i in np.unique(y_predict_Kmeans):\n",
    "    ax[0].scatter(\n",
    "        x = X_trans[y_predict_Kmeans == i, 0],\n",
    "        y = X_trans[y_predict_Kmeans == i, 1],\n",
    "        c = plt.rcParams['axes.prop_cycle'].by_key()['color'][i],\n",
    "        marker    = 'o',\n",
    "        edgecolor = 'black',\n",
    "        label= f\"Cluster {i}\"\n",
    "    )\n",
    "ax[0].set_title('Clusters encontrados con K-Means')\n",
    "ax[0].legend();\n",
    "\n",
    "# Clustering DBSCAN\n",
    "\n",
    "modelo_dbscan = DBSCAN(eps = 0.6, min_samples  = 5, metric = 'euclidean')\n",
    "y_predict = modelo_dbscan.fit(X=X_trans)\n",
    "\n",
    "labels = modelo_dbscan.labels_\n",
    "ax[1].scatter(\n",
    "    x = X_trans[:, 0],\n",
    "    y = X_trans[:, 1],\n",
    "    c = labels,\n",
    "    marker    = 'o',\n",
    "    edgecolor = 'black'\n",
    ")\n",
    "\n",
    "\n",
    "# Los outliers se identifican con el label -1\n",
    "ax[1].scatter(\n",
    "    x = X_trans[labels == -1, 0],\n",
    "    y = X_trans[labels == -1, 1],\n",
    "    c = 'red',\n",
    "    marker    = 'o',\n",
    "    edgecolor = 'black',\n",
    "    label = 'outliers'\n",
    ")\n",
    "\n",
    "ax[1].set_title(f'DBSCAN (clusters = {len(set(labels)) - (1 if -1 in labels else 0)})')\n",
    "ax[1].legend();\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "41H5jUZ6l3Kl"
   },
   "source": [
    "Ahora, nuestro inter√©s es caracterizar estos cuatros grupos que nos han salido usando alguna estad√≠sticas descriptiva."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "iFkxMHaIBvaO"
   },
   "outputs": [],
   "source": [
    "datos2 = datos.copy()\n",
    "datos2[\"Cluster\"]=y_predict_Kmeans\n",
    "\n",
    "aux=datos2.columns.tolist()\n",
    "aux[0:len(aux)-1]\n",
    "\n",
    "for col in aux[0:len(aux)-1]:\n",
    "    sns.boxplot(data=datos2, x=\"Cluster\", y=col, fill=False, gap=.1)\n",
    "    plt.title(f\"Boxplot de la variable {col}\")\n",
    "    plt.show()\n",
    "#    grid.map(plt.hist, cluster,color='red')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ebUNg618mCrb"
   },
   "source": [
    "¬øPodemos sacar conclusiones?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1uGTfB7ZLLm2"
   },
   "source": [
    "# GMMs: Gaussian mixture models\n",
    "\n",
    "Un Gaussian Mixture model es un modelo probabil√≠stico en el que se considera que las observaciones siguen una distribuci√≥n probabil√≠stica formada por la combinaci√≥n de m√∫ltiples distribuciones normales (componentes). En su aplicaci√≥n al clustering, puede entenderse como una generalizaci√≥n de K-means con la que, en lugar de asignar cada observaci√≥n a un √∫nico cluster, se obtiene una probabilidad de pertenencia a cada uno.\n",
    "\n",
    "Para estimar los par√°metros que definen la funci√≥n de distribuci√≥n de cada cluster (media y matriz de covarianza) se recurre al algoritmo de *Expectation-Maximization* (EM). Una vez aprendidos los par√°metros, se puede calcular la probabilidad que tiene cada observaci√≥n de pertenecer a cada cluster y asignarla a aquel con mayor probabilidad."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "IPyqkHA2Loa-"
   },
   "outputs": [],
   "source": [
    "# Tratamiento de datos\n",
    "# ==============================================================================\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.datasets import make_blobs\n",
    "\n",
    "# Gr√°ficos\n",
    "# ==============================================================================\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib as mpl\n",
    "from matplotlib.patches import Ellipse\n",
    "from matplotlib import style\n",
    "style.use('ggplot') or plt.style.use('ggplot')\n",
    "\n",
    "# Preprocesado y modelado\n",
    "# ==============================================================================\n",
    "from sklearn.mixture import GaussianMixture\n",
    "from sklearn.preprocessing import scale\n",
    "from sklearn.metrics import silhouette_score\n",
    "\n",
    "# Configuraci√≥n warnings\n",
    "# ==============================================================================\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "DGQhFAtuLxGN"
   },
   "outputs": [],
   "source": [
    "# Simulaci√≥n de datos\n",
    "# ==============================================================================\n",
    "X, y = make_blobs(\n",
    "        n_samples    = 300,\n",
    "        n_features   = 2,\n",
    "        centers      = 4,\n",
    "        cluster_std  = 0.60,\n",
    "        shuffle      = True,\n",
    "        random_state = 0\n",
    "       )\n",
    "\n",
    "fig, ax = plt.subplots(1, 1, figsize=(6, 3.84))\n",
    "ax.scatter(\n",
    "    x = X[:, 0],\n",
    "    y = X[:, 1],\n",
    "    c = 'white',\n",
    "    marker    = 'o',\n",
    "    edgecolor = 'black',\n",
    ")\n",
    "ax.set_title('Datos simulados');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "y6Ih_0OtL_6Y"
   },
   "source": [
    "Con la clase `sklearn.mixture.GaussianMixture` de Scikit-Learn se pueden entrenar modelos de GMMs utilizando el algoritmo expectation-maximization (EM) . Entre sus par√°metros destacan:\n",
    "\n",
    "- `n_components`: n√∫mero de componentes (en este caso clusters) que forman el modelo.\n",
    "\n",
    "- `covariance_type`: tipo de matriz de covarianza (‚Äòfull‚Äô (default), ‚Äòtied‚Äô, ‚Äòdiag‚Äô, ‚Äòspherical‚Äô).\n",
    "\n",
    "- `max_iter`: n√∫mero m√°ximo de iteraciones permitidas.\n",
    "\n",
    "- `random_state`: semilla para garantizar la reproducibilidad de los resultados."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "KeHLVPRYMxFj"
   },
   "outputs": [],
   "source": [
    "# Modelo\n",
    "# ==============================================================================\n",
    "modelo_gmm = GaussianMixture(n_components=4, covariance_type='full', random_state=42)\n",
    "modelo_gmm.fit(X=X)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9PEztIKgM8A0"
   },
   "source": [
    "El objeto devuelto por GaussianMixture contiene entre otros datos: el peso de cada componente (cluster) en el modelo (`weights_`), su media (`means_`) y matriz de covarianza (`covariances_`).La estructura de esta √∫ltima depende del tipo de matriz empleada en el ajuste del modelo (`covariance_type`)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "jnXA8_99NZbc"
   },
   "outputs": [],
   "source": [
    "# Media de cada componente\n",
    "modelo_gmm.means_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "QbGUPPGkNeQ-"
   },
   "outputs": [],
   "source": [
    "# Matriz de covarianza de cada componente\n",
    "modelo_gmm.covariances_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "OAlg-CYFN-fI"
   },
   "source": [
    "Una vez entrenado el modelo GMMs, se puede predecir la probabilidad que tiene cada observaci√≥n de pertenecer a cada una de las componentes (clusters). Para obtener la clasificaci√≥n final, se asigna a la componente con mayor probabilidad."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "M-Y7KCwYN_lO"
   },
   "outputs": [],
   "source": [
    "# Probabilidades\n",
    "# ==============================================================================\n",
    "# Cada fila es una observaci√≥n y cada columna la probabilidad de pertenecer a\n",
    "# cada una de las componentes.\n",
    "probabilidades = modelo_gmm.predict_proba(X)\n",
    "probabilidades"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "tZBAajtSOGpa"
   },
   "outputs": [],
   "source": [
    "# Clasificaci√≥n (asignaci√≥n a la componente de mayor probabilidad)\n",
    "# ==============================================================================\n",
    "# Cada fila es una observaci√≥n y cada columna la probabilidad de pertenecer a\n",
    "# cada una de las componentes.\n",
    "clasificacion = modelo_gmm.predict(X)\n",
    "clasificacion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "7-bjj_BqORVz"
   },
   "outputs": [],
   "source": [
    "# Representaci√≥n gr√°fica\n",
    "# ==============================================================================\n",
    "# Codigo obtenido de:\n",
    "# https://github.com/amueller/COMS4995-s20/tree/master/slides/aml-14-clustering-mixture-models\n",
    "def make_ellipses(gmm, ax):\n",
    "    for n in range(gmm.n_components):\n",
    "        if gmm.covariance_type == 'full':\n",
    "            covariances = gmm.covariances_[n]\n",
    "        elif gmm.covariance_type == 'tied':\n",
    "            covariances = gmm.covariances_\n",
    "        elif gmm.covariance_type == 'diag':\n",
    "            covariances = np.diag(gmm.covariances_[n])\n",
    "        elif gmm.covariance_type == 'spherical':\n",
    "            covariances = np.eye(gmm.means_.shape[1]) * gmm.covariances_[n]\n",
    "        v, w = np.linalg.eigh(covariances)\n",
    "        u = w[0] / np.linalg.norm(w[0])\n",
    "        angle = np.arctan2(u[1], u[0])\n",
    "        angle = 180 * angle / np.pi  # convert to degrees\n",
    "        v = 2. * np.sqrt(2.) * np.sqrt(v)\n",
    "\n",
    "        for i in range(1,3):\n",
    "            ell = mpl.patches.Ellipse(gmm.means_[n], i*v[0], i*v[1],\n",
    "                                      angle=180 + angle, color=\"blue\")\n",
    "            ell.set_clip_box(ax.bbox)\n",
    "            ell.set_alpha(0.1)\n",
    "            ax.add_artist(ell)\n",
    "\n",
    "\n",
    "fig, axs = plt.subplots(1, 2, figsize=(12, 3.84))\n",
    "\n",
    "# Distribuci√≥n de probabilidad de cada componente\n",
    "for i in np.unique(clasificacion):\n",
    "    axs[0].scatter(\n",
    "        x = X[clasificacion == i, 0],\n",
    "        y = X[clasificacion == i, 1],\n",
    "        c = plt.rcParams['axes.prop_cycle'].by_key()['color'][i],\n",
    "        marker    = 'o',\n",
    "        edgecolor = 'black',\n",
    "        label= f\"Cluster {i}\"\n",
    "    )\n",
    "\n",
    "make_ellipses(modelo_gmm, ax = axs[0])\n",
    "axs[0].set_title('Distribuci√≥n de prob. de cada componente')\n",
    "axs[0].legend()\n",
    "\n",
    "# Distribuci√≥n de probabilidad del modelo completo\n",
    "xs = np.linspace(min(X[:, 0]), max(X[:, 0]), 1000)\n",
    "ys = np.linspace(min(X[:, 1]), max(X[:, 1]), 1000)\n",
    "xx, yy = np.meshgrid(xs, ys)\n",
    "scores = modelo_gmm.score_samples(np.c_[xx.ravel(), yy.ravel()], )\n",
    "axs[1].scatter(X[:, 0], X[:, 1], s=5, alpha=.6, c=plt.cm.tab10(clasificacion))\n",
    "scores = np.exp(scores) # Las probabilidades est√°n en log\n",
    "axs[1].contour(\n",
    "    xx, yy, scores.reshape(xx.shape),\n",
    "    levels=np.percentile(scores, np.linspace(0, 100, 10))[1:-1]\n",
    ")\n",
    "axs[1].set_title('Distribuci√≥n de prob. del modelo completo');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "wy2hk91bOeCx"
   },
   "source": [
    "## N√∫mero de clusters\n",
    "Dado que los modelos GMM son modelos probabil√≠sticos, se puede recurrir a m√©tricas como el Akaike information criterion (AIC) o Bayesian information criterion (BIC) para identificar c√≥mo de bien se ajustan los datos observados a modelo creado."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "TSgxN9idOoTA"
   },
   "outputs": [],
   "source": [
    "n_components = range(1, 21)\n",
    "valores_bic = []\n",
    "valores_aic = []\n",
    "\n",
    "for i in n_components:\n",
    "    modelo = GaussianMixture(n_components=i, covariance_type=\"full\")\n",
    "    modelo = modelo.fit(X)\n",
    "    valores_bic.append(modelo.bic(X))\n",
    "    valores_aic.append(modelo.aic(X))\n",
    "\n",
    "fig, ax = plt.subplots(1, 1, figsize=(6, 3.84))\n",
    "ax.plot(n_components, valores_bic, label='BIC')\n",
    "ax.plot(n_components, valores_aic, label='AIC')\n",
    "ax.set_title(\"Valores BIC y AIC\")\n",
    "ax.set_xlabel(\"N√∫mero componentes\")\n",
    "ax.legend();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "_0J6Mu5HOxNj"
   },
   "outputs": [],
   "source": [
    "print(f\"N√∫mero √≥ptimo acorde al BIC: {range(1, 21)[np.argmin(valores_bic)]}\")\n",
    "print(f\"N√∫mero √≥ptimo acorde al AIC: {range(1, 21)[np.argmin(valores_aic)]}\")"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
