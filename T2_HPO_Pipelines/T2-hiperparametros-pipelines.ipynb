{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "E0d1NdTBxjrd"
   },
   "source": [
    "# Manejo con pipelines\n",
    "\n",
    "En este tutorial aprenderemos a trabajar con pipelines y lo mezclaremos con estrategias de optimización de parámetros.\n",
    "\n",
    "Usaremos el dataset del Titanic donde nos encontraremos con los siguientes temas:\n",
    "   - Habrá variables nulas\n",
    "   - Tendremos variables categóricas y numéricas\n",
    "   - El dataset se encuentra desbalanceado\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "pJJr_V0rgGRx"
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "#######\n",
    "# Importamos todas las dependencias\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.datasets import fetch_openml\n",
    "from sklearn.pipeline import Pipeline, make_pipeline\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.impute import SimpleImputer, KNNImputer\n",
    "from sklearn.preprocessing import RobustScaler, OneHotEncoder, OrdinalEncoder\n",
    "#from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn import tree\n",
    "from sklearn.model_selection import train_test_split, cross_val_score, RandomizedSearchCV\n",
    "from sklearn import metrics\n",
    "\n",
    "np.random.seed(42)\n",
    "\n",
    "X, y = fetch_openml(\"titanic\", version=1, as_frame=True, return_X_y=True)\n",
    "X.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "7xztMyaLK_RV"
   },
   "outputs": [],
   "source": [
    "y.value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "id": "bBuQfsrhK_RW"
   },
   "source": [
    "Como vemos este dataset se encuentra algo desbalanceado. Lo tendremos que tener en cuenta en los siguientes momentos:\n",
    "   - Al dividir en muestras de aprendizaje y test\n",
    "   - Cuando usemos Cross Validation\n",
    "   - En el entrenamiento de los modelos. Casi todos los métodos en sklearn admiten un parámetro `class_weight='balanced'`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "fOajKItXK_RW"
   },
   "outputs": [],
   "source": [
    "# Borramos columnas que no nos interesan\n",
    "X.drop(['boat', 'body', 'home.dest', 'ticket', 'name'], axis=1, inplace=True)\n",
    "# Seleccionados train/test en modo stratify, ya que el dataset se encuentra desbalanceado\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, stratify=y, test_size=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "971JPXiuhSZS"
   },
   "outputs": [],
   "source": [
    "X.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3i24oFW7O2jC"
   },
   "source": [
    "## Tratamiento de valores nulos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "FEqLv3HohRi_"
   },
   "outputs": [],
   "source": [
    "X_train.isnull().any()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "rkwrzzkVhqaJ"
   },
   "outputs": [],
   "source": [
    "X_train.isnull().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "OWop5l_7PGyR"
   },
   "source": [
    "Vamos a dibujar los valores que son nulos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Elk-mLoDhxyq"
   },
   "outputs": [],
   "source": [
    "import missingno as msno\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "%config InlineBackend.figure_format = 'png'\n",
    "\n",
    "msno.matrix(X_train)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Tgb5bJGoh7kr"
   },
   "outputs": [],
   "source": [
    "X_train.isnull().sum() / len(X_train) * 100"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Q2PAd5S_Pdkc"
   },
   "source": [
    "El atributo 'cabin' tiene demasiados valores nulos. En este caso no tiene sentido aplicar ningún método de imputación, por lo que vamos a eliminar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "BTjHdvf8iDLi"
   },
   "outputs": [],
   "source": [
    "X_train.drop(['cabin'], axis=1, inplace=True)\n",
    "X_test.drop(['cabin'], axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "fMpNwwfzP-GG"
   },
   "source": [
    "Vamos a calcular las correlaciones de los valores numéricos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Z_Vm4bCciI1z"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "%config InlineBackend.figure_format = 'svg'\n",
    "\n",
    "X_comb = pd.concat([X_train, y_train.astype(float)], axis=1)\n",
    "g = sns.heatmap(X_comb[['pclass', 'age', 'sibsp', 'parch', 'fare', 'survived']].corr(),\n",
    "                annot=True,\n",
    "                cmap = \"coolwarm\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "mZYy3NN_qXuK"
   },
   "outputs": [],
   "source": [
    "# Si fuera tipo objeto evaluar \"OrdinalEncoder\"\n",
    "X['pclass'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Bp-iTRmzRo6l"
   },
   "source": [
    "## Codificamos la salida\n",
    "Aunque la variable de salida es 0 y 1, pasamos a codificarla con `LabelEncoder` ya que para algunas funciones como el cálculo de las curvas ROC necesitamos esta codificación."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Has35auCRTT7"
   },
   "outputs": [],
   "source": [
    "# Es necesario\n",
    "from sklearn import preprocessing\n",
    "le = preprocessing.LabelEncoder()\n",
    "le.fit(y)\n",
    "\n",
    "y_test = le.transform(y_test)\n",
    "y_train = le.transform (y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "W3O--NolMH0m"
   },
   "source": [
    "## Analizar tipos de variables\n",
    "Vamos a analizar las variables categóricas que tenemos en el dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "_Q1FfeSrMTZj"
   },
   "outputs": [],
   "source": [
    "X_train.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Y1E33baKM6a4"
   },
   "outputs": [],
   "source": [
    "cat_cols = X_train.select_dtypes(include=\"category\").columns\n",
    "num_cols = X_train.select_dtypes(exclude=\"category\").columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "IJ6rANfKHjvJ"
   },
   "outputs": [],
   "source": [
    "cat_cols"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Sb_hq9qHOQMh"
   },
   "source": [
    "Para las variables categóticas vamos a crear un pipeline donde:\n",
    "1. Si hubiese una muestra con valores nulos, le vamos a imputar el valor del más frecuente.\n",
    "2. Codificaremos con la técnicas One_Hot_Encoder.\n",
    "3. Aplicaremos sobre ellos un PCA, para extraer características más discriminantes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "kSoYVcK9OPH-"
   },
   "outputs": [],
   "source": [
    "cat_transformer = Pipeline(steps=[\n",
    "    ('imputer', SimpleImputer(strategy='most_frequent')),\n",
    "    ('onehot', OneHotEncoder(handle_unknown='ignore', sparse_output=False)),\n",
    "    ('pca', PCA(n_components=5))\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Vuhr22iKkfmp"
   },
   "outputs": [],
   "source": [
    "# Si queremos acceder al primer paso del pipeline. De la misma forma podríamos acceder a los distintos pasos del pipeline\n",
    "cat_transformer[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "uw9PqZd8PkU2"
   },
   "source": [
    "Ahora nos metemos con las variables numéricas. En este caso vamos a realizar el siguiente preproceso:\n",
    "1. Si hay valores nulos, vamos a asignarles la media de sus K=5 vecinos más cercanos.\n",
    "2. Realizaremos un escalado"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "xWufY9ZVko5q"
   },
   "outputs": [],
   "source": [
    "from sklearn.impute import KNNImputer\n",
    "from sklearn.preprocessing import RobustScaler\n",
    "\n",
    "num_transformer = Pipeline(steps=[\n",
    "    ('imputer', KNNImputer(n_neighbors=5)),\n",
    "    ('scaler', RobustScaler())\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "kYV8YA2SQY46"
   },
   "source": [
    "En el siguiente código vamos a utilizar la función `ColumnTransformer` para indicar cómo vamos a preprocesar a las variables categóricas y numéricas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "lMSjq_zflPTJ"
   },
   "outputs": [],
   "source": [
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        ('num', num_transformer, num_cols),\n",
    "        ('cat', cat_transformer, cat_cols)\n",
    "    ])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "PcANmNg0n6xo"
   },
   "outputs": [],
   "source": [
    "preprocessor\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_5fnpkS-QqeY"
   },
   "source": [
    "## Creamos el clasificador\n",
    "Por último, vamos a crear un `Pipeline` para unir el paso del preproceso, con el paso de entrenamiento del modelo. Fijaros que en el entrenamiento del modelo estoy indicando `class_weight='balanced'`, ya que el dataset se encuentra desbalanceado."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "GG_27e0Ulcpx"
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import cross_val_score\n",
    "clf = Pipeline(steps=[('preprocessor', preprocessor),\n",
    "                      ('classifier', tree.DecisionTreeClassifier(class_weight='balanced'))])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7mWd0PHbSZv0"
   },
   "source": [
    "Si el valor del parámetro es `cv` asignado a un número entero y la variable de salida es binaria o multiclase, entonces utiliza `StratifiedKFold`. Directamente utiliza el valor de `shuffle=False`.\n",
    "En este caso, lleva a cabo un `StratifiedKFold` con K=5, y obtenemos el valor medio del `accuracy`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "bqS6zIyjoCto"
   },
   "outputs": [],
   "source": [
    "clf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "urFPio77HbaN"
   },
   "source": [
    "Como el dataset está desbalanceado usamos como mejor resultado \"balanced_accuracy\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "FP6hyBo-RmU1"
   },
   "outputs": [],
   "source": [
    "cross_val_score(clf, X_train, y_train, cv=5, scoring=\"balanced_accuracy\").mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "yg_qfaXqHBSn"
   },
   "source": [
    "Ahora vamos a buscar los mejores hiperparámetros"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "kYJaAPBfl0uW"
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "\n",
    "num_transformer_dist = {'preprocessor__num__imputer__n_neighbors': list(range(2, 15)),\n",
    "                        'preprocessor__num__imputer__add_indicator': [True, False]}\n",
    "\n",
    "cat_transformer_dist = {'preprocessor__cat__imputer__strategy': ['most_frequent', 'constant'],\n",
    "                        'preprocessor__cat__imputer__add_indicator': [True, False],\n",
    "                        'preprocessor__cat__pca__n_components': list(range(2, 5))}\n",
    "\n",
    "random_forest_dist = {'classifier__max_depth': list(range(2, 20)),\n",
    "                      'classifier__min_samples_split': list(range(20, 200))}\n",
    "\n",
    "param_dist = {**num_transformer_dist, **cat_transformer_dist, **random_forest_dist}\n",
    "\n",
    "random_search = RandomizedSearchCV(clf,\n",
    "                                   param_distributions=param_dist,\n",
    "                                   n_iter=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "1OAJzeDGmHUZ"
   },
   "outputs": [],
   "source": [
    "random_search.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "VE0x64a3mTVO"
   },
   "outputs": [],
   "source": [
    "random_search.best_score_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "igX9CT-8nu5y"
   },
   "outputs": [],
   "source": [
    "random_search.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "035RnFOnn17o"
   },
   "outputs": [],
   "source": [
    "y_pred = random_search.predict(X_test)\n",
    "y_pred[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "os1939zRn8aB"
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "\n",
    "print(classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "STUbZIRxvBY4"
   },
   "source": [
    "La precisión balanceada se usa en problemas de clasificación binaria y multiclase para tratar conjuntos de datos desbalanceados. Se define como la media del recall obtenido en cada clase."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "fqePw5ewoDa2"
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import balanced_accuracy_score\n",
    "\n",
    "print(f\"El valor de balanced accuracy: {balanced_accuracy_score(y_test, y_pred)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Da3zOsj9oI2i"
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import ConfusionMatrixDisplay, confusion_matrix\n",
    "\n",
    "cm = confusion_matrix(y_test, y_pred, labels=random_search.classes_)\n",
    "disp= ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=random_search.classes_)\n",
    "disp.plot()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "v0p09VIRON_n"
   },
   "outputs": [],
   "source": [
    "#Obtenemos las curva ROC y el área bajo la curva (AUC)\n",
    "\n",
    "probs = random_search.predict_proba(X_test)[:, 1]\n",
    "\n",
    "auc = metrics.roc_auc_score(y_test, probs)\n",
    "fpr, tpr, thresholds = metrics.roc_curve(y_test, probs)\n",
    "\n",
    "plt.figure(figsize=(8, 5))\n",
    "plt.plot(fpr, tpr, label=f'AUC  = {auc:.2f}')\n",
    "plt.plot([0, 1], [0, 1], color='blue', linestyle='--', label='Baseline')\n",
    "plt.title('Curva ROC', size=20)\n",
    "plt.xlabel('Falsos Positivos', size=14)\n",
    "plt.ylabel('Verdaderos Positivos', size=14)\n",
    "plt.legend();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "DAa7z4-4PFfi"
   },
   "outputs": [],
   "source": [
    "# Modelo final\n",
    "modelo_final = random_search.best_estimator_\n",
    "_ = modelo_final.fit (X,y)\n"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
