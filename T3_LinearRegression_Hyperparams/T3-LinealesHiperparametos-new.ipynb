{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"toc_visible":true,"authorship_tag":"ABX9TyNz3MJRHZkgGtnsJyGpP6GW"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","source":["# Búsqueda de hiperparámetros en regresión lineal\n","\n","En este tutorial trabajaremos con los modelos de regresión lineal. En concreto construiremos modelos lineales aproximados por mínimos cuadrados, así como sus versiones en las que se incorporan las regularizaciones Ridge, Lasso y Elastic Net.\n","\n","Como último veremos una regresión polinómica"],"metadata":{"id":"s_jEsE8W6HbZ"}},{"cell_type":"code","source":["# Tratamiento de datos\n","# ==============================================================================\n","import pandas as pd\n","import numpy as np\n","\n","# Gráficos\n","# ==============================================================================\n","import matplotlib.pyplot as plt\n","from matplotlib import style\n","import seaborn as sns\n","\n","# Preprocesado y modelado\n","# ==============================================================================\n","from scipy.stats import pearsonr\n","import scipy as sp\n","from sklearn.model_selection import train_test_split\n","from sklearn.metrics import mean_squared_error\n","from sklearn.linear_model import LinearRegression\n","from sklearn.linear_model import Ridge\n","from sklearn.linear_model import Lasso\n","from sklearn.linear_model import ElasticNet\n","from sklearn.linear_model import RidgeCV\n","from sklearn.linear_model import Ridge\n","from sklearn.linear_model import LassoCV\n","from sklearn.linear_model import ElasticNetCV\n","from sklearn.preprocessing import StandardScaler\n","from sklearn.pipeline import Pipeline\n","from sklearn.pipeline import make_pipeline\n","from sklearn import metrics\n","from sklearn.preprocessing import MinMaxScaler\n","from sklearn.preprocessing import OneHotEncoder\n","from sklearn.compose import ColumnTransformer\n","from sklearn.compose import make_column_selector as selector\n","from sklearn.compose import make_column_transformer\n","from sklearn.compose import TransformedTargetRegressor\n","\n","\n","\n","# Configuración matplotlib\n","# ==============================================================================\n","plt.rcParams['image.cmap'] = \"bwr\"\n","plt.rcParams['savefig.bbox'] = \"tight\"\n","style.use('ggplot') or plt.style.use('ggplot')\n","\n","\n","# Configuración warnings\n","# ==============================================================================\n","import warnings\n","warnings.filterwarnings('ignore')"],"metadata":{"id":"8W2YIMIi7SyF"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["def rmse(y_test, y_test_pred):\n","  \"\"\" Este es mi cálculo del error cuadrático medio \"\"\"\n","  return np.sqrt(metrics.mean_squared_error(y_test, y_test_pred))"],"metadata":{"id":"skKrA10C-h_e"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["def pintaResultados (reg, n):\n","  plt.subplots(figsize=(30, 5))\n","  x = np.arange(y_test[:n].size)\n","  pred = reg.predict(X=X_test)\n","  plt.plot(x, y_test[:n], 'b.', label='Estimado')\n","  plt.plot(x, pred[:n], 'g^', label='Deseado')\n","  plt.legend(loc='upper right')\n","  plt.show()"],"metadata":{"id":"N_n7pkh3XwQX"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Cargamos los datos"],"metadata":{"id":"T5bni26R7hn3"}},{"cell_type":"code","source":["from sklearn.datasets import fetch_openml\n","\n","survey = fetch_openml(data_id=534, as_frame=True)"],"metadata":{"id":"ZL6u47Le7jNh"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["print(survey.DESCR)"],"metadata":{"id":"hJQzFXQErj1V"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["X = survey.data[survey.feature_names]\n","X.describe(include=\"all\")"],"metadata":{"id":"yQAjKuvjOK1G"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["X.head()"],"metadata":{"id":"6hji-747OdwN"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["y = survey.target.values.ravel()\n","survey.target.head()"],"metadata":{"id":"EQPEtJIhOlJ4"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Holdout para la evaluación del modelo. 33% de los datos disponibles para test\n","X_train, X_test, y_train, y_test = train_test_split(X, y,test_size=0.33, random_state=42)"],"metadata":{"id":"j7PQK5_08EYF"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Analizamos las variables numéricas\n","# ==============================================================================\n","\n","train_dataset = X_train.copy()\n","train_dataset.insert(0, \"WAGE\", y_train)\n","_ = sns.pairplot(train_dataset, kind=\"reg\", diag_kind=\"kde\")\n"],"metadata":{"id":"1NqM54g9O3iy"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["survey.data.info()"],"metadata":{"id":"7fnoacdSP96s"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Definimos el preproceso de las columnas\n","# ==============================================================================\n","\n","categorical_columns = X_train.select_dtypes(include=\"category\").columns\n","numerical_columns = X_train.select_dtypes(exclude=\"category\").columns\n","\n","# Otra forma\n","# categorical_columns = [\"RACE\", \"OCCUPATION\", \"SECTOR\", \"MARR\", \"UNION\", \"SEX\", \"SOUTH\"]\n","# numerical_columns = [\"EDUCATION\", \"EXPERIENCE\", \"AGE\"]\n","\n","preprocessor = make_column_transformer(\n","    (OneHotEncoder(drop=\"if_binary\"), categorical_columns),\n","    (StandardScaler(), numerical_columns),\n",")\n","\n","# Otra forma\n","# preprocesor = ColumnTransformer(\n","#    [('categoricos', OneHotEncoder(drop=\"if_binary\"),categorical_columns),\n","#     ('numericos', StandardScaler(), numerical_columns)]\n","#    )"],"metadata":{"id":"I5C06vYySSpo"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## Regresión lineal por mínimos cuadrados"],"metadata":{"id":"PpkoVv3l-GjJ"}},{"cell_type":"code","source":["# Creación y entrenamiento del modelo lineal\n","# ==============================================================================\n","\n","pipe_regr = Pipeline([\n","        ('preproceso', preprocessor),\n","        ('regresor', LinearRegression())\n","])\n","\n","np.random.seed(42)\n","pipe_regr.fit(X_train, y_train)"],"metadata":{"id":"44yLUauEU0K_"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["print(f\"RMSE de regresión lineal: {rmse(y_test, pipe_regr.predict(X=X_test)):.3f} $/hora\")\n","\n","pintaResultados(pipe_regr,100)"],"metadata":{"id":"hU38mMU3Vl85"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["feature_names = pipe_regr[:-1].get_feature_names_out()\n","\n","coefs = pd.DataFrame(\n","    pipe_regr['regresor'].coef_,\n","    columns=[\"Importancia de coeficientes\"],\n","    index=feature_names,\n",")\n","coefs.plot.barh(figsize=(9, 7))\n","plt.title(\"Modelo lineal\")\n","plt.xlabel(\"Coeficientes\")\n","plt.axvline(x=0, color=\".5\")\n","plt.subplots_adjust(left=0.3)"],"metadata":{"id":"9LXiroLLWP8B"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## Ridge\n"],"metadata":{"id":"2rWfPbouDS3k"}},{"cell_type":"code","source":["# Creación y entrenamiento del modelo (con búsqueda por CV del valor óptimo alpha)\n","# ==============================================================================\n","# Por defecto RidgeCV utiliza el mean squared error\n","\n","\n","pipe_regrRidge = Pipeline([\n","        ('preproceso', preprocessor),\n","        ('regresor', RidgeCV(\n","            alphas          = np.logspace(-3, 8, 200),\n","            fit_intercept   = True,\n","            store_cv_values = True\n","         ))\n","])\n","\n","np.random.seed(42)\n","pipe_regrRidge.fit(X_train, y_train)\n"],"metadata":{"id":"beTgTzLGDVfr"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["print(f\"RMSE de regresión lineal RIDGE: {rmse(y_test, pipe_regrRidge.predict(X=X_test)):.3f} $/hora\")\n","print(f\"Mejor aplpha: {pipe_regrRidge['regresor'].alpha_}\")\n","\n","pintaResultados(pipe_regrRidge,50)"],"metadata":{"id":"-35Kln8-mdSm"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Evolución de los coeficientes en función de alpha\n","# ==============================================================================\n","alphas = pipe_regrRidge['regresor'].alphas\n","\n","coefs = []\n","\n","for alpha in alphas:\n","    modelo_temp = Pipeline([\n","        ('preproceso', preprocessor),\n","        ('regresor', Ridge(alpha=alpha, fit_intercept=False))\n","    ])\n","    modelo_temp.fit(X_train, y_train)\n","    coefs.append(modelo_temp['regresor'].coef_.flatten())\n","\n","fig, ax = plt.subplots(figsize=(7, 3.84))\n","ax.plot(alphas, coefs)\n","ax.set_xscale('log')\n","ax.set_xlabel('alpha')\n","ax.set_ylabel('coeficientes')\n","ax.set_title('Coeficientes del modelo en función de la regularización');\n","plt.axis('tight')\n","plt.show()"],"metadata":{"id":"MWlY0HMHFIBo"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Como vemos, a medida que aumenta el valor de alpha, la regularización es mayor y el valor de los coeficientes se reduce"],"metadata":{"id":"kl8787AIF0_h"}},{"cell_type":"code","source":["# Evolución del error en función de alpha\n","# ==============================================================================\n","# modelo.cv_values almacena el mse de cv para cada valor de alpha. Tiene\n","# dimensiones (n_samples, n_targets, n_alphas)\n","mse_cv = pipe_regrRidge['regresor'].cv_values_.reshape((-1, 200)).mean(axis=0)\n","mse_sd = pipe_regrRidge['regresor'].cv_values_.reshape((-1, 200)).std(axis=0)\n","\n","# Se aplica la raíz cuadrada para pasar de mse a rmse\n","rmse_cv = np.sqrt(mse_cv)\n","rmse_sd = np.sqrt(mse_sd)\n","\n","# Se identifica el óptimo y el óptimo + 1std\n","min_rmse     = np.min(rmse_cv)\n","sd_min_rmse  = rmse_sd[np.argmin(rmse_cv)]\n","min_rsme_1sd = np.max(rmse_cv[rmse_cv <= min_rmse + sd_min_rmse])\n","optimo       = pipe_regrRidge['regresor'].alphas[np.argmin(rmse_cv)]\n","optimo_1sd   = pipe_regrRidge['regresor'].alphas[rmse_cv == min_rsme_1sd]\n","\n","\n","# Gráfico del error +- 1 desviación estándar\n","fig, ax = plt.subplots(figsize=(7, 3.84))\n","ax.plot(pipe_regrRidge['regresor'].alphas, rmse_cv)\n","ax.fill_between(\n","    pipe_regrRidge['regresor'].alphas,\n","    rmse_cv + rmse_sd,\n","    rmse_cv - rmse_sd,\n","    alpha=0.2\n",")\n","\n","\n","ax.axvline(\n","    x         = optimo,\n","    c         = \"gray\",\n","    linestyle = '--',\n","    label     = 'óptimo'\n",")\n","\n","ax.axvline(\n","    x         = optimo_1sd,\n","    c         = \"blue\",\n","    linestyle = '--',\n","    label     = 'óptimo_1sd'\n",")\n","ax.set_xscale('log')\n","ax.set_ylim([0,None])\n","ax.set_title('Evolución del error CV en función de la regularización')\n","ax.set_xlabel('alpha')\n","ax.set_ylabel('RMSE')\n","plt.legend();"],"metadata":{"id":"wYk2ZtPJF7Q5"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Mejor valor alpha encontrado\n","# ==============================================================================\n","print(f\"Mejor valor de alpha encontrado: {pipe_regrRidge['regresor'].alpha_}\")"],"metadata":{"id":"5TBg1A5kGne1"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Coeficientes del modelo\n","# ==============================================================================\n","coefs = pd.DataFrame(\n","    pipe_regrRidge['regresor'].coef_,\n","    columns=[\"Importancia de coeficientes\"],\n","    index=feature_names,\n",")\n","coefs.plot.barh(figsize=(9, 7))\n","plt.title(\"Modelo Ridge\")\n","plt.xlabel(\"Coeficientes\")\n","plt.axvline(x=0, color=\".5\")\n","plt.subplots_adjust(left=0.3)"],"metadata":{"id":"G4P-gLQvh0_m"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["En comparación al modelo por mínimos cuadrados ordinarios, con ridge, el orden de magnitud de los coeficientes es menor."],"metadata":{"id":"VFshaxFfHFRr"}},{"cell_type":"markdown","source":["## Lasso"],"metadata":{"id":"BFzwibQtP6UJ"}},{"cell_type":"code","source":["# Creación y entrenamiento del modelo (con búsqueda por CV del valor óptimo alpha)\n","# ==============================================================================\n","# Por defecto LassoCV utiliza el mean squared error\n","pipe_regrLasso = Pipeline([\n","        ('preproceso', preprocessor),\n","        ('regresor', LassoCV(\n","            alphas          = np.logspace(-9, 3, 200),\n","            cv              = 3\n","         ))\n","])\n","\n","np.random.seed(42)\n","pipe_regrLasso.fit(X = X_train, y = y_train)\n"],"metadata":{"id":"xn6jNFCPP8hL"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["print(f\"RMSE de regresión lineal Lasso: {rmse(y_test, pipe_regrLasso.predict(X=X_test)):.3f} $/hora\")\n","print(f\"Mejor aplpha: {pipe_regrLasso['regresor'].alpha_}\")\n","\n","pintaResultados(pipe_regrRidge,50)"],"metadata":{"id":"-eaClfJ6mt2g"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Evolución de los coeficientes en función de alpha\n","# ==============================================================================\n","alphas = pipe_regrLasso['regresor'].alphas_\n","coefs = []\n","\n","for alpha in alphas:\n","    modelo_temp = Pipeline([\n","        ('preproceso', preprocessor),\n","        ('regresor', Lasso(alpha=alpha, fit_intercept=False))\n","    ])\n","    modelo_temp.fit(X_train, y_train)\n","    coefs.append(modelo_temp['regresor'].coef_.flatten())\n","\n","fig, ax = plt.subplots(figsize=(7, 3.84))\n","ax.plot(alphas, coefs)\n","ax.set_xscale('log')\n","ax.set_ylim([-2.5,None])\n","ax.set_xlabel('alpha')\n","ax.set_ylabel('coeficientes')\n","ax.set_title('Coeficientes del modelo en función de la regularización');"],"metadata":{"id":"HW_CR-3lQ_dm"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Puede ver se como, a medida que aumenta el valor de alpha, la regularización es mayor y más predictores quedan excluidos (su coeficiente es 0)."],"metadata":{"id":"VGmK6pp-ROa4"}},{"cell_type":"code","source":["# Número de predictores incluidos (coeficiente !=0) en función de alpha\n","# ==============================================================================\n","alphas = pipe_regrLasso['regresor'].alphas_\n","n_predictores = []\n","\n","for alpha in alphas:\n","    modelo_temp = Pipeline([\n","        ('preproceso', preprocessor),\n","        ('regresor', Lasso(alpha=alpha, fit_intercept=False))\n","    ])\n","    modelo_temp.fit(X_train, y_train)\n","    coef_no_cero = np.sum(modelo_temp['regresor'].coef_.flatten() != 0)\n","    n_predictores.append(coef_no_cero)\n","\n","\n","fig, ax = plt.subplots(figsize=(7, 3.84))\n","ax.plot(alphas, n_predictores)\n","ax.set_xscale('log')\n","ax.set_ylim([-1,None])\n","ax.set_xlabel('alpha')\n","ax.set_ylabel('nº predictores')\n","ax.set_title('Predictores incluidos en función de la regularización');"],"metadata":{"id":"yu05PlMURODL"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Evolución del error en función de alpha\n","# ==============================================================================\n","# regrLasso.mse_path_ almacena el mse de cv para cada valor de alpha. Tiene\n","# dimensiones (n_alphas, n_folds)\n","mse_cv = pipe_regrLasso['regresor'].mse_path_.mean(axis=1)\n","mse_sd = pipe_regrLasso['regresor'].mse_path_.std(axis=1)\n","\n","# Se aplica la raíz cuadrada para pasar de mse a rmse\n","rmse_cv = np.sqrt(mse_cv)\n","rmse_sd = np.sqrt(mse_sd)\n","\n","# Se identifica el óptimo y el óptimo + 1std\n","min_rmse     = np.min(rmse_cv)\n","sd_min_rmse  = rmse_sd[np.argmin(rmse_cv)]\n","min_rsme_1sd = np.max(rmse_cv[rmse_cv <= min_rmse + sd_min_rmse])\n","optimo       = pipe_regrLasso['regresor'].alphas_[np.argmin(rmse_cv)]\n","optimo_1sd   = pipe_regrLasso['regresor'].alphas_[rmse_cv == min_rsme_1sd]\n","\n","# Gráfico del error +- 1 desviación estándar\n","fig, ax = plt.subplots(figsize=(7, 3.84))\n","ax.plot(pipe_regrLasso['regresor'].alphas_, rmse_cv)\n","ax.fill_between(\n","    pipe_regrLasso['regresor'].alphas_,\n","    rmse_cv + rmse_sd,\n","    rmse_cv - rmse_sd,\n","    alpha=0.2\n",")\n","\n","ax.axvline(\n","    x         = optimo,\n","    c         = \"gray\",\n","    linestyle = '--',\n","    label     = 'óptimo'\n",")\n","\n","#ax.axvline(\n","#    x         = optimo_1sd,\n","#    c         = \"blue\",\n","#    linestyle = '--',\n","#    label     = 'óptimo_1sd'\n","#)\n","\n","ax.set_xscale('log')\n","ax.set_ylim([0,None])\n","ax.set_title('Evolución del error CV en función de la regularización')\n","ax.set_xlabel('alpha')\n","ax.set_ylabel('RMSE')\n","plt.legend();\n"],"metadata":{"id":"dnZjfsReRyiJ"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Mejor valor alpha encontrado\n","# ==============================================================================\n","print(f\"Mejor valor de alpha encontrado: {pipe_regrLasso['regresor'].alpha_}\")"],"metadata":{"id":"UpHM3-RoSYp2"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Coeficientes del modelo\n","# ==============================================================================\n","coefs = pd.DataFrame(\n","    pipe_regrLasso['regresor'].coef_,\n","    columns=[\"Importancia de coeficientes\"],\n","    index=feature_names,\n",")\n","coefs.plot.barh(figsize=(9, 7))\n","plt.title(\"Modelo Lasso\")\n","plt.xlabel(\"Coeficientes\")\n","plt.axvline(x=0, color=\".5\")\n","plt.subplots_adjust(left=0.3)"],"metadata":{"id":"yeTJza4sTLda"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Comprobar como disminuye el número de predictores."],"metadata":{"id":"Q-r6uOGfTrgk"}},{"cell_type":"markdown","source":["## Elastic Net"],"metadata":{"id":"zhOip6WPT5Jq"}},{"cell_type":"code","source":["# ==============================================================================\n","# Por defecto ElasticNetCV utiliza el mean squared error\n","pipe_regrElastic = Pipeline([\n","        ('preproceso', preprocessor),\n","        ('regresor', ElasticNetCV(\n","            l1_ratio        = [0, 0.1, 0.5, 0.7, 0.9, 0.95, 0.99],\n","            alphas          = np.logspace(-10, 3, 200),\n","            cv              = 3)\n","        )\n","])\n","\n","np.random.seed(42)\n","pipe_regrElastic.fit(X = X_train, y = y_train)\n","print(f\"RMSE de regresión lineal - Elastic Net: {rmse(y_test, pipe_regrElastic.predict(X=X_test)):.3f} $/hora\")\n","print(f\"Mejor aplpha: {pipe_regrElastic['regresor'].alpha_}\")\n","print(f\"L1_ratio: {pipe_regrElastic['regresor'].l1_ratio_}\")\n","\n","pintaResultados(pipe_regrElastic,50)"],"metadata":{"id":"11Ia_q_xUAU5"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Evolución del error en función de alpha y l1_ratio\n","# ==============================================================================\n","# regrElastic.mse_path_ almacena el mse de cv para cada valor de alpha y l1_ratio.\n","# Tiene dimensiones (n_l1_ratio, n_alpha, n_folds)\n","\n","# Error medio de las 10 particiones por cada valor de alpha y l1_ratio\n","mean_error_cv = pipe_regrElastic['regresor'].mse_path_.mean(axis =2)\n","\n","# El resultado es un array de dimensiones (n_l1_ratio, n_alpha)\n","# Se convierte en un dataframe\n","df_resultados_cv = pd.DataFrame(\n","                        data   = mean_error_cv.flatten(),\n","                        index  = pd.MultiIndex.from_product(\n","                                    iterables = [pipe_regrElastic['regresor'].l1_ratio, pipe_regrElastic['regresor'].alphas_],\n","                                    names     = ['l1_ratio', 'modelo.alphas_']\n","                                 ),\n","                        columns = [\"mse_cv\"]\n","                    )\n","\n","df_resultados_cv['rmse_cv'] = np.sqrt(df_resultados_cv['mse_cv'])\n","df_resultados_cv = df_resultados_cv.reset_index().sort_values('mse_cv', ascending = True)\n","df_resultados_cv"],"metadata":{"id":"Zn473D7jUXXT"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Mejor valor encontrado para cada l1_ratio\n","fig, ax = plt.subplots(figsize=(7, 3.84))\n","df_resultados_cv.groupby('l1_ratio')['rmse_cv'].min().plot(ax = ax)\n","ax.set_title('Evolución del error CV en función de la l1_ratio')\n","ax.set_xlabel('l1_ratio')\n","ax.set_ylabel('rmse_cv');"],"metadata":{"id":"5ag5UrIoUsfd"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Mejor valor alpha y l1_ratio_ encontrado\n","# ==============================================================================\n","print(f\"Mejor valor de alpha encontrado: {pipe_regrElastic['regresor'].alpha_}\")\n","print(f\"Mejor valor de l1_ratio encontrado: {pipe_regrElastic['regresor'].l1_ratio_}\")"],"metadata":{"id":"bdC7DxOeU34r"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Coeficientes del modelo\n","# ==============================================================================\n","coefs = pd.DataFrame(\n","    pipe_regrElastic['regresor'].coef_,\n","    columns=[\"Importancia de coeficientes\"],\n","    index=feature_names,\n",")\n","coefs.plot.barh(figsize=(9, 7))\n","plt.title(\"Modelo Elastic Net\")\n","plt.xlabel(\"Coeficientes\")\n","plt.axvline(x=0, color=\".5\")\n","plt.subplots_adjust(left=0.3)"],"metadata":{"id":"MuV7vl6vVCRR"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## Comparamos"],"metadata":{"id":"a28AuPepVpXn"}},{"cell_type":"code","source":["rmse_lineal = rmse(y_test, pipe_regr.predict(X=X_test))\n","rmse_ridge = rmse(y_test, pipe_regrRidge.predict(X=X_test))\n","rmse_lasso = rmse(y_test, pipe_regrLasso.predict(X=X_test))\n","rmse_elastic = rmse(y_test, pipe_regrElastic.predict(X=X_test))\n","df_comparacion = pd.DataFrame({\n","                    'modelo': ['Lineal', 'Ridge', 'Lasso', 'Elastic-net'],\n","                    'test rmse': [rmse_lineal, rmse_ridge, rmse_lasso, rmse_elastic]\n","                 })\n","\n","fig, ax = plt.subplots(figsize=(7, 3.84))\n","df_comparacion.set_index('modelo').plot(kind='barh', ax=ax)\n","ax.set_xlabel('rmse')\n","ax.set_ylabel('modelo')\n","ax.set_title('Comparación de modelos');"],"metadata":{"id":"Za4vsD40VrT5"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["En este problema no hay mucha diferencia entre el modelo lineal y los modelos regularizados en cuanto al rmse. Sin embargo, como hemos visto, los valores de los coeficientes son menores."],"metadata":{"id":"55YhF5O5Wnpo"}},{"cell_type":"markdown","source":["# Regresión polinómica\n","Pasemos ahora a intentar una regresión polinómica. Haremos una búsqueda de los mejores hiperparámetros:\n","- 'grades': Grado del polinomio [2,3,4,5]"],"metadata":{"id":"OA7HWqL5kXg2"}},{"cell_type":"code","source":["from sklearn.linear_model import LinearRegression\n","from sklearn.preprocessing import PolynomialFeatures\n","from sklearn.model_selection import cross_val_score\n","\n","degrees = [1, 2, 3, 4] # Change degree \"hyperparameter\" here\n","best_score = 0.0\n","best_degree = 0\n","\n","for degree in degrees:\n","    poly_features = Pipeline([\n","        ('preproceso', preprocessor),\n","        ('regresor', PolynomialFeatures(degree = degree, include_bias=False))\n","    ])\n","    X_train_poly = poly_features.fit_transform(X_train)\n","    polynomial_regressor = LinearRegression()\n","    polynomial_regressor.fit(X_train_poly, y_train)\n","    scores = cross_val_score(polynomial_regressor, X_train_poly, y_train, cv=3) # Change k-fold cv value here\n","    print(f\"Procesado {degree} con mejor valor R2: {max(scores)} \")\n","    if max(scores) > best_score:\n","      best_score = max(scores)\n","      best_degree = degree\n"],"metadata":{"id":"gUDwQIE3kYjG"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["print(f\"Mejor valor (R2): {best_score}\")\n","print(f\"Mejor valor de grado encontrado: {best_degree}\")"],"metadata":{"id":"M8NDhsyHkkNf"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Creamos el modelo polinomial con los mejores parámetros:"],"metadata":{"id":"FuARagPzkmLp"}},{"cell_type":"code","source":["poly_features = Pipeline([\n","   ('preproceso', preprocessor),\n","   ('regresor', PolynomialFeatures(degree = best_degree))\n","])\n","X_train_poly = poly_features.fit_transform(X_train)\n","best_polynomial_regressor = LinearRegression()\n","best_polynomial_regressor.fit(X_train_poly, y_train)\n","X_test_poly = poly_features.fit_transform(X_test)\n","print(f\"RMSE de regresión polinómica: {rmse(y_test, best_polynomial_regressor.predict(X=X_test_poly))}\")\n"],"metadata":{"id":"lLpP7hrZko70"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["print(best_polynomial_regressor.coef_)"],"metadata":{"id":"hfGfwHiFfOZ1"},"execution_count":null,"outputs":[]}]}