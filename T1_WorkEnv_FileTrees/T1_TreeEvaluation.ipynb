{"cells":[{"cell_type":"markdown","metadata":{"id":"c0QivjbPXPWq"},"source":["# Estructura de la sesión:\n","- Introducción. Entorno de trabajo\n","- Entrenando un árbol de decisión (clasificación)\n","  - Holdout\n","  - Validación cruzada\n","  - Modificación de hiperparámetros\n","  - Variables categóricas\n","- Árboles de regresión\n"]},{"cell_type":"markdown","metadata":{"id":"8FNDY9Rfw2Ps"},"source":["# Introducción. Entono de trabajo\n","Opción 1\n","   - GitHub + Google Colab (nube de Google)\n","    https://colab.research.google.com/\n","\n","Opción 2\n","   - GitHub + tu editor favorito (DataSpell de JetBrains, Visual Studio, ...)\n","\n","Opción 3\n","   - GitHub + Jupyter Notebook (local)\n","   - Instalación:\n","        - **pip install notebook**\n","   - Ejecución\n","        - **jupyter notebook**\n","\n","  \n","\n","---\n","\n","\n","\n","\n","\n","\n","\n","\n"]},{"cell_type":"markdown","metadata":{"id":"cWAOeLZ94XOz"},"source":["## Usando Python.\n","- Python básico (python sin librerías extra): listas, diccionarios, conjuntos, ...\n","- numpy: vectores y matrices con números\n","- pandas: Data frames"]},{"cell_type":"markdown","metadata":{"id":"MS7sU2en7dVC"},"source":["Esto es una lista (base Python)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"FfuwVwX65nqi"},"outputs":[],"source":["a = [1, 2, 3]\n","print(a)"]},{"cell_type":"markdown","metadata":{"id":"ZAEx9NdA7jby"},"source":["Esto es un vector y una matriz con numpy. Sólo pueden contener números"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"9kybjyy77jra"},"outputs":[],"source":["import numpy as np\n","\n","print('This is a vector:')\n","\n","a = np.array([1, 2, 3])\n","print(a)\n","\n","print()\n","\n","print('Ths is a matrix:')\n","b = np.array([[1,2,3],\n","              [4,5,6]])\n","print(b)"]},{"cell_type":"markdown","metadata":{"id":"m1NceMWE8FDD"},"source":["Estos son pandas data frames. Contienen diferentes columnas, algunas de ellas son números, pero otras contienen valores categóricos"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"mt5NzVA18FM6"},"outputs":[],"source":["import pandas as pd\n","pd.DataFrame({'a':[1,2,3], 'b':['a', 'b', 'c']})"]},{"cell_type":"markdown","metadata":{"id":"E2iDVRYc9fFi"},"source":["Parece que pandas dataframes son una estructura de datos apropiada para representar datos. Sin embargo, scikit-learn sólo puede utilizar matrices numpy. Por lo tanto, los valores categóricos deben ser codificados como números. El flujo de trabajo típico cuando se trabaja con scikit-learn es:\n","1. Cargar los datos como Pandas dataframe\n","2. Llevar a cabo un análisis exploratorio de los datos para comprenderlos. Se conoce como  EDA (Exploratory Data Analysis)\n","3. Codificar los Pandas dataframe como matrices numpy (procesar los valores categóricos y los valores inexistentes - missing values)\n","4. Diseñar los modelos de aprendizaje automático con la librería scikit-learn"]},{"cell_type":"markdown","metadata":{"id":"J9dwWGrb4QXi"},"source":["## Scikit-learn (sklearn):\n","- Colección de algoritmos y herramientas de aprendizaje automático en Python.\n","- [http://scikit-learn.org/stable/](SCIKIT-LEARN)\n","\n","** Otros paquetes de aprendizaje automático en Python: **\n","- Pylearn2\n","- PyBrain\n","- ...\n","\n","---"]},{"cell_type":"markdown","metadata":{"id":"DL5_Er0v4QXl"},"source":["\n","\n","\n","\n","## Datos de entrada para sklearn (matrices numéricas)"]},{"cell_type":"markdown","metadata":{"id":"m-3u_OGV4QXm"},"source":["- Los conjuntos de datos para sklearn son matrices numpy **numéricas**:\n","- Esto implica que los atributos/características categóricas deben ser representadas como:\n","    - Enteros\n","    - One-hot-encoding / variables dummy\n","\n","- Sin embargo, hay una tendencia para integrar Pandas dataframes con scikit learn\n","- Los valores inexistentes se representan como np.nan\n"]},{"cell_type":"markdown","metadata":{"id":"iMtbmEyl4QXm"},"source":["- Un ejemplo sencillo de conjunto de datos es el conjunto de datos del iris, que se incluye dentro del propio sklearn.\n","- De lo contrario, tendríamos que cargarlo desde un archivo"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"SwhF9rEr4QXm"},"outputs":[],"source":["import numpy as np\n","from sklearn.datasets import load_iris\n","iris_meta = load_iris()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"i4XLUUY-4QXn"},"outputs":[],"source":["print(iris_meta.feature_names)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"BraxjcIT4QXo","tags":[]},"outputs":[],"source":["print(iris_meta.target_names)"]},{"cell_type":"markdown","metadata":{"id":"IUWlrFzX4QXp"},"source":["Aquí tenemos las variables/características/atributos de entrada y la variable objetivo/salida"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"wETZD6q64QXp"},"outputs":[],"source":["X = iris_meta.data\n","y = iris_meta.target"]},{"cell_type":"markdown","metadata":{"id":"eTfFws3S4QXq"},"source":["- X (la característica de entrada) es un numpy array de 2 dimensiones\n","- y (variable objetivo) es un numpy vector"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"tTf1ylam4QXq"},"outputs":[],"source":["print(type(X))\n","print(X.shape) # 150 instancias and 4 características de entrada\n","print(X.dtype) # Lo valores son números reales (float)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"xQb5GqPa4QXq"},"outputs":[],"source":["print(type(y))\n","print(y.shape) # 150 valores de la variable objetivo\n","print(y.dtype) # Los valores son enteros"]},{"cell_type":"markdown","metadata":{"id":"I1MSNYt04QXr"},"source":["Podemos visualizar las 10 primeras estancias"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"-UOb_pJx4QXr"},"outputs":[],"source":["print(X[0:10,0:4])"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Oa8xhgG34QXr"},"outputs":[],"source":["print(y[:])"]},{"cell_type":"markdown","metadata":{"id":"GL8zQzuh4QXr"},"source":["A continuación, visualizamos toda la tabla, siendo la variable de respuesta la última columna. No es necesario hacer esto cuando se trabaja con sklearn, es sólo para visualizarlo"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"LASHXWc74QXs"},"outputs":[],"source":["np.concatenate((X,y[:,np.newaxis]), axis=1)[0:10]"]},{"cell_type":"markdown","metadata":{"id":"rke_0WUy4QXs"},"source":["---\n","Podemos dibujar el conjunto de datos del dataset iris para ver cómo se distribuye."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"SUtaFKIH4QXs"},"outputs":[],"source":["import matplotlib.pyplot as plt\n","plt.scatter(X[:, 1], X[:, 3], c=y, cmap=plt.cm.Paired)\n","plt.xlabel('Sepal length')\n","plt.ylabel('Sepal width')\n","plt.show()"]},{"cell_type":"markdown","source":["Podemos ver como se integran los pandas con matplotlib"],"metadata":{"id":"G28AybH9qvKy"}},{"cell_type":"code","source":["import pandas as pd\n","\n","df_X = pd.DataFrame(X)\n","\n","df_X.columns = [\"sepal_length\", \"sepal_width\", \"petal_length\", \"petal_width\"]\n","df_X[\"Iris\"] = y\n","\n","df_X.plot.scatter(x=\"sepal_length\", y=\"sepal_width\", c=y, cmap=plt.cm.Set3)\n","plt.show()\n"],"metadata":{"id":"YpmIDYJck8n2"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Dibujar un histograma de una de las variables"],"metadata":{"id":"irUdGTPmkVCA"}},{"cell_type":"code","source":["\n","df_X.petal_width.plot.hist()\n","# Probar mas configuraciones\n","#df_X.petal_width.plot.hist(bins=50, xlim=(0,3))\n","plt.show()\n","\n","# Otras opciones\n","# df_X[\"petal_width\"].hist()"],"metadata":{"id":"irF0SZjhrHzt"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["plt.hist(X[0:49,2], label='setosa')\n","plt.hist(X[50:99,2], label='versicolor')\n","plt.hist(X[100:149,2], label='virginica')\n","plt.legend(loc='upper right')\n","plt.show()"],"metadata":{"id":"m_qnBHhWvEnF"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## Ejercicio\n","Mediante el histograma, podemos ver la distribucción de las variables de entrada teniendo en cuenta la clase. ¿Cuál es la mejor variable de entrada que nos permite separar las distintas clases?"],"metadata":{"id":"hF0w7zAY8iKN"}},{"cell_type":"markdown","metadata":{"id":"Ko8Y_aes9UD7"},"source":["Hay una librería de visualización (**seaborn**) que posee un conjunto de funciones estadísticas gráficas. Trabaja sobre **matplotlib**"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"VcgCzgkUxz5-"},"outputs":[],"source":["#Si no está instalado\n","# !pip install seaborn\n","import seaborn as sns\n","import pandas as pd\n","\n","my_column_names=iris_meta.feature_names.copy()\n","my_column_names.append('species')\n","\n","iris=np.concatenate((X,y[:,np.newaxis]), axis=1)\n","\n","#Creamos el dataframe\n","df_iris = pd.DataFrame(data=iris, columns=my_column_names)\n","\n","#Cambiamos el valor de la variable de salida\n","df_iris.loc[df_iris['species'] == 0.0, 'species'] = 'setosa'\n","df_iris.loc[df_iris['species'] == 1.0, 'species'] = 'versicolor'\n","df_iris.loc[df_iris['species'] == 2.0, 'species'] = 'virginica'\n","\n","\n","g = sns.pairplot(df_iris, hue='species')"]},{"cell_type":"markdown","source":["---\n","## Ejercicio\n","Dibujar un gráfica de barras para comprobar si el problema se encuentra balenceado (pista: usar **barplot**)"],"metadata":{"id":"Ds_PK5Q22L64"}},{"cell_type":"code","source":["# Tu código AQUÍ\n","lista=[np.count_nonzero(y == 0), np.count_nonzero(y == 1), np.count_nonzero(y == 2)]\n","sns.barplot(x = ['setosa','versicolor','virginica'], y = lista)\n","plt.show()\n","\n","\n"],"metadata":{"id":"H5GYn34D0mn8"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"C1HKbQlo4QXs"},"source":["# Entrenando un árbol de decisión\n","---\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"9zidraxu4QXt"},"outputs":[],"source":["from sklearn import tree\n","# Aquí definimos el tipo de método de entrenamiento (todavía no pasa nada)\n","clf = tree.DecisionTreeClassifier()\n","# Ahora, entrenamos (*fit*) el método en el conjunto de datos (X,y)\n","clf.fit(X, y)\n","# clf **ha sido modificado** y ahora contiene el modelo entrenado"]},{"cell_type":"markdown","metadata":{"id":"QgtYgDGH4QXt"},"source":["Podemos visualizar el árbol como texto"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"HXdfMAUC4QXt"},"outputs":[],"source":["text_representation = tree.export_text(clf, feature_names=iris_meta.feature_names)\n","print(text_representation)"]},{"cell_type":"markdown","metadata":{"id":"mvjUKYTd4QXt"},"source":["También podemos visualizar el árbol gráficamente"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"LPEoeoGc4QXu"},"outputs":[],"source":["import matplotlib.pyplot as plt\n","fig = plt.figure(figsize=(25,20))\n","_ = tree.plot_tree(clf,\n","                   feature_names = iris_meta.feature_names,\n","                   class_names=iris_meta.target_names,\n","                   filled=True)"]},{"cell_type":"markdown","metadata":{"id":"bDexRXHP4QXu"},"source":["#HOLDOUT. Entrenamiento y evaluación de un árbol de decisión con un conjunto de pruebas (holdout)\n","---"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"O7uNOyNv4QXu"},"outputs":[],"source":["from sklearn.model_selection import train_test_split"]},{"cell_type":"markdown","metadata":{"id":"f-4APKeP4QXu"},"source":["- Creamos las particiones de entrenamiento (X_train, y_train) y de prueba (X_test, y_test): 2/3 para entrenamiento, 1/3 para pruebas\n","- Fíjate en el **random_state=42** para la reproducibilidad (¡importante!)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"HSfsVFxH4QXu"},"outputs":[],"source":["X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.33, random_state=42)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"7OQ_uQ0CelIl"},"outputs":[],"source":["X_train[0:5]"]},{"cell_type":"markdown","metadata":{"id":"L1OxaH3-4QXv"},"source":["Configuración de las particiones de entrenamiento y de prueba"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"sLayO_Ir4QXv"},"outputs":[],"source":["print(X_train.shape, y_train.shape) # 100 instancias para entrenamiento\n","print(X_test.shape, y_test.shape)   # 50 instances para test"]},{"cell_type":"markdown","metadata":{"id":"Fxjyn5es4QXv"},"source":["Imprimamos las cinco primeras instancias de entrenamiento"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"jS3dEvOK4QXv"},"outputs":[],"source":["print(\"INPUT FEATURES:\")\n","print(X_train[:5,:])\n","print(\"RESPONSE:\")\n","print(y_train[:5])"]},{"cell_type":"markdown","metadata":{"id":"M-hCi0G-4QXv"},"source":["Si volvemos a crear la partición, será igual que antes si utilizamos el mismo *random_state*\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"EMUbPInn4QXv"},"outputs":[],"source":["X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.33, random_state=42)\n","print(\"INPUT FEATURES:\")\n","print(X_train[:5,:])"]},{"cell_type":"markdown","metadata":{"id":"PJwTUDiv4QXv"},"source":["Pero será diferente si cambiamos *random_state*"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"C9iUH1c14QXw"},"outputs":[],"source":["X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.33, random_state=24)\n","print(\"INPUT FEATURES:\")\n","print(X_train[:5,:])"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"GwiXO5WR4QXw"},"outputs":[],"source":["# Mantengamos la partición original (con random_state=42)\n","X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.33, random_state=42)"]},{"cell_type":"markdown","metadata":{"id":"2W36mNPh4QXw"},"source":["- Entrenamos el árbol con .fit\n","- Fíjate que usamos np.random.seed(42) para que el entrenamiento del árbol también sea reproducible (en caso de que el entrenamiento del árbol sea no determinista)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"pGUABN5a4QXw"},"outputs":[],"source":["from sklearn import tree\n","\n","# Definimos el modelo\n","clf = tree.DecisionTreeClassifier()\n","# Permitir que los resultados sean reproducibles\n","np.random.seed(42)\n","# Entrenamos el modelo\n","clf.fit(X_train, y_train)\n","# We can see that the tree is inside\n","print(tree.export_text(clf))"]},{"cell_type":"markdown","metadata":{"id":"tsF1kKuQ4QXw"},"source":["Por cierto, podemos obtener ayuda de cualquier función (como .fit)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"my616HwN4QXw"},"outputs":[],"source":["?clf.fit"]},{"cell_type":"markdown","metadata":{"id":"2jqPY0dT4QXw"},"source":["Evaluamos el árbol, calculando las predicciones sobre el conjunto de pruebas"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"eCGTo5oO4QXx"},"outputs":[],"source":["y_test_pred = clf.predict(X_test)"]},{"cell_type":"markdown","metadata":{"id":"bfu7AeHg4QXx"},"source":["Podemos comprobar las predicciones para las instancias de prueba"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"VEzdzi7_4QXx"},"outputs":[],"source":["print(y_test_pred)"]},{"cell_type":"markdown","metadata":{"id":"1qgkCQxm4QXx"},"source":["A efectos de visualización, podemos comparar las predicciones y los valores reales. Podemos ver que para los 5 primeros casos coinciden."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"-K2BLDm24QXx"},"outputs":[],"source":["print(np.hstack((y_test_pred[:5,np.newaxis], y_test[:5,np.newaxis])))"]},{"cell_type":"markdown","metadata":{"id":"NCpZW27s4QXx"},"source":["- Pero, para evaluar el modelo en la partición de prueba, podemos calcular una métrica (precisión (accuracy) de la clasificación en este caso)\n","- Usamos más métricas\n","- Y la matriz de confusión\n","- Es muy alta (98%)\n","---"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"uspUaRJe4QXx"},"outputs":[],"source":["from sklearn import metrics\n","accuracy_tree = metrics.accuracy_score(y_test, y_test_pred)\n","print(accuracy_tree)\n","result1 = metrics.classification_report(y_test, y_test_pred)\n","print(\"Classification Report:\",)\n","print (result1)\n","\n","# We compute recall and other metrics\n","precision,recall,fscore,support = metrics.precision_recall_fscore_support(y_test, y_test_pred)\n","print(\"Precision of classes: {0}\".format(precision))\n","print(\"Recall of classes: {0}\".format(recall))\n","print(\"Fscore of classes: {0}\".format(fscore))\n","print(\"Support of classes: {0}\".format(support))\n","\n","# Creates a confusion matrix\n","cm = metrics.confusion_matrix(y_test, y_test_pred)\n","\n","# Transform to df for easier plotting\n","cm_df = pd.DataFrame(cm,\n","                     index = ['setosa','versicolor','virginica'],\n","                     columns = ['setosa','versicolor','virginica'])\n","# Transform to df for easier plotting\n","cm_df = pd.DataFrame(cm,\n","                     index = ['setosa','versicolor','virginica'],\n","                     columns = ['setosa','versicolor','virginica'])\n","\n","plt.figure(figsize=(5.5,4))\n","sns.heatmap(cm_df, annot=True)\n","plt.title('Accuracy:{0:.3f}'.format(accuracy_tree))\n","plt.ylabel('True label')\n","plt.xlabel('Predicted label')\n","plt.show()"]},{"cell_type":"markdown","metadata":{"id":"pTqJfBWJY8W-"},"source":["Una forma de ver el rendimiento de un clasificador es utilizar las curvas ROC. Estas curvas se usan para clasificaciones binarias. Vamos a construir un clasificador que permita clasificar muestras de la clase \"virginia\" del resto.\n","\n","---\n"]},{"cell_type":"code","execution_count":null,"metadata":{"jupyter":{"outputs_hidden":false},"id":"FrSchn83Y8W_"},"outputs":[],"source":["y_train_int = y_train.copy()\n","y_train_int[y_train_int == 1] = 0\n","y_train_int[y_train_int == 2] = 1\n","# Ahora tenemos una clase 0 (\"setosa\" y \"versicolor\") y 1 (\"virginia\")\n","\n","# Igual con test\n","y_test_int = y_test.copy()\n","y_test_int[y_test_int == 1] = 0\n","y_test_int[y_test_int == 2] = 1\n","\n","# Jugamos con \"max_depth\"\n","clf_int = tree.DecisionTreeClassifier(max_depth=1)\n","# Para hacerlo reproducible\n","np.random.seed(42)\n","# Entrenamos\n","clf_int.fit(X_train, y_train_int)\n","\n","y_test_pred_int = clf_int.predict(X_test)\n","result1 = metrics.classification_report(y_test_int, y_test_pred_int)\n","print(\"Classification Report:\",)\n","print (result1)\n","\n","print(tree.export_text(clf_int))\n","\n","iris=np.concatenate((X_train,y_train_int[:,np.newaxis]), axis=1)\n","\n","#Creamos el dataframe\n","df_iris = pd.DataFrame(data=iris, columns=my_column_names)\n","\n","#Cambiamos el valor de la variable de salida\n","df_iris.loc[df_iris['species'] == 0.0, 'species'] = 'otras'\n","df_iris.loc[df_iris['species'] == 1.0, 'species'] = 'virginia'\n","\n","g = sns.pairplot(df_iris, hue='species')\n","\n","probs = clf_int.predict_proba(X_test)[:, 1]\n","\n","auc = metrics.roc_auc_score(y_test_int, probs)\n","fpr, tpr, thresholds = metrics.roc_curve(y_test_int, probs)\n","\n","plt.figure(figsize=(8, 5))\n","plt.plot(fpr, tpr, label=f'AUC  = {auc:.2f}')\n","plt.plot([0, 1], [0, 1], color='blue', linestyle='--', label='Baseline')\n","plt.title('Curva ROC', size=20)\n","plt.xlabel('Falsos Positivos', size=14)\n","plt.ylabel('Verdaderos Positivos', size=14)\n","plt.legend();"]},{"cell_type":"markdown","metadata":{"id":"CjgSbJvq4QXx"},"source":["Sin embargo, la precisión de 0,98 es la evaluación del modelo (estimación del rendimiento). Todavía tenemos que calcular el modelo final (el que enviará y utilizará la empresa) **utilizando todos los datos disponibles**."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"VN0n0ZU94QXy"},"outputs":[],"source":["final_clf = tree.DecisionTreeClassifier()\n","# Para que los resultados sean reproducibles\n","np.random.seed(42)\n","# Entrenamos (CON TODAS LAS MUESTRAS)\n","final_clf.fit(X, y)\n","# final_clf Tiene el modelo que usará la empresa\n","# La estimación de su accuracy la hemos obtenido antes (95%)"]},{"cell_type":"markdown","metadata":{"id":"8Bzm7znT4QXy"},"source":["Por cierto, podemos almacenar (y cargar) este modelo en un fichero. Esto se llama \"persistencia del modelo\""]},{"cell_type":"code","execution_count":null,"metadata":{"id":"V_ukcKMF4QXy"},"outputs":[],"source":["from joblib import dump, load\n","# Save the final model to a file\n","dump(final_clf, 'final_tree.joblib')\n","# Load the tree from the file\n","final_clf_reloaded = load('final_tree.joblib')"]},{"cell_type":"markdown","metadata":{"id":"ObJu3av94QXy"},"source":["A continuación, tienes el código completo"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Fv2Zgzb94QXy"},"outputs":[],"source":["from sklearn.model_selection import train_test_split\n","from sklearn import metrics\n","from sklearn import tree\n","\n","# Train/test split\n","X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.33, random_state=42)\n","\n","# Here, we set our model to classification tree\n","clf = tree.DecisionTreeClassifier()\n","np.random.seed(42) # reproducibility\n","# We train it\n","clf.fit(X_train, y_train)\n","# We obtain predictions on the test set\n","y_test_pred = clf.predict(X_test)\n","# We compute accuracy\n","accuracy_tree = metrics.accuracy_score(y_test, y_test_pred)\n","print(f\"Accuracy of the tree: {accuracy_tree} \")\n","\n","\n","\n","# We finally compute the final model with all available data\n","\n","final_clf = tree.DecisionTreeClassifier()\n","np.random.seed(42)  # reproducibility\n","final_clf.fit(X, y)"]},{"cell_type":"markdown","source":["---"],"metadata":{"id":"xe3925CTjfCT"}},{"cell_type":"markdown","metadata":{"id":"f7lsOqmTBnzt"},"source":["## Ejercicio: hacer el aprendizaje, la predicción, la evaluación del modelo y la construcción del modelo final con KNN\n","\n","- Para la clasificación:\n","\n","from sklearn.neighbors import KNeighborsClassifier\n","\n","- Para la regresión:\n","\n","from sklearn.neighbors import KNeighborsRegressor\n","\n","Ayuda sobre KNN en sklearn:\n","\n","from sklearn.neighbors import KNeighborsClassifier\n","\n","Lo definimos como:\n","\n","clf = KNeighborsClassifier()\n","\n","(y lo entrenamos con .fit).\n","\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"VSb9cSjfNafr"},"outputs":[],"source":["# Tu código AQUÍ\n"]},{"cell_type":"markdown","source":["---"],"metadata":{"id":"cRfJa03SjttT"}},{"cell_type":"markdown","metadata":{"id":"hX9_a4E94QXy"},"source":["# VALIDACIÓN CRUZADA. Entrenamiento y evaluación de un árbol de decisión con validación cruzada"]},{"cell_type":"markdown","metadata":{"id":"HiMzev7a4QX0"},"source":["- Primero, vamos a hacer la validación cruzada con un bucle, para que entendamos mejor el proceso\n","- Sin embargo (!!), es mejor hacer la validación cruzada con la función *cross_val_score*, como haremos más adelante"]},{"cell_type":"markdown","metadata":{"id":"r121gQdC4QX0"},"source":["- *KFold* crea los grupos de validación cruzada de entrenamiento/test.\n","    - *shuffle* mezcla aleatoriamente los datos antes de dividir los grupos. Siempre debemos hacer esto, a menos que tengamos buenas razones para lo contrario.\n","    - *random_state* hace que la mezcla sea reproducible"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"ECsSw2by4QX0"},"outputs":[],"source":["from sklearn.model_selection import KFold\n","cv = KFold(n_splits=5, shuffle=True, random_state=42)\n"]},{"cell_type":"markdown","metadata":{"id":"9bOHa6wQ4QX0"},"source":["- Vamos a realizar la validación cruzada utilizando 5 agrupaciones.\n","- En cada iteración:\n","    - Entrenamos un modelo en las agrupaciones de entrenamiento\n","    - Calculamos predicciones en las agrupaciones de test\n","    - Calculamos la métrica (precisión - accuracy) y la almacenamos.\n","- Cuando el bucle de validación cruzada termina, calculamos la media (y std)   "]},{"cell_type":"code","execution_count":null,"metadata":{"id":"wI_3sDaF4QX0"},"outputs":[],"source":["np.random.seed(42) # reproducibility\n","\n","# This variable will contain the 5 crossvalidation accuracies, one per iteration\n","scores = []\n","\n","for train_index, test_index in cv.split(X):\n","    print(f\"TRAIN: {train_index[:5]} ...\", f\"TEST: {test_index[:5]} ...\")\n","    # Getting the actual training and testing partitions out of the indices\n","    X_train, X_test = X[train_index,:], X[test_index,:]\n","    y_train, y_test = y[train_index], y[test_index]\n","\n","    # Training the model for this particular crossvalidation iteration\n","    clf = tree.DecisionTreeClassifier()\n","    clf.fit(X_train, y_train)\n","    y_test_pred = clf.predict(X_test)\n","    accuracy_tree = metrics.accuracy_score(y_test, y_test_pred)\n","\n","    print(f\"The accuracy for this crossval iteration is: {accuracy_tree}\")\n","    print()\n","    # We add this accuracy to the list\n","    scores.append(accuracy_tree)\n","\n","# Transforming scores from list to numpy array (this is just a technicallity)\n","scores = np.array(scores)\n","print(f\"All the accuracies are: {scores}\")\n","print(f\"And the average crossvalidation accuracy is: {scores.mean():.2f} +- {scores.std():.2f}\")\n","\n","# Trace the accuracy of all k values\n","plt.figure(figsize=(12, 6))\n","plt.plot([1, 2, 3, 4, 5], scores,marker='o', markerfacecolor='blue', markersize=10)\n","plt.title('Accuracy Rate Crossvalidation')\n","plt.xlabel('Crossval iter')\n","plt.ylabel('Accuracy')"]},{"cell_type":"markdown","source":["Si estuvieramos en un problema desbalanceado, podríamos usar *Stratified k-fold cross-validation*. No es el caso, pero para practicar, podemos escribir la función de validación usando *StratifiedKFold*"],"metadata":{"id":"sL65kKCBF_Pp"}},{"cell_type":"markdown","metadata":{"id":"v7dfsh8t4QX0"},"source":["- Para entender lo que hace la validación cruzada, hemos programado el bucle explícitamente\n","- Pero la validación cruzada se hace típicamente por medio de la función *cross_val_score*, como a continuación"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"ejuZVow54QX1"},"outputs":[],"source":["from sklearn.model_selection import cross_val_score, KFold\n","\n","# create a k-fold crossvalidation iterator of k=5 folds\n","# shuffle = True randomly rearranges the dataframe\n","# random_state = 42 is for making the folds reproducible\n","cv = KFold(n_splits=5, shuffle=True, random_state=42)\n","clf = tree.DecisionTreeClassifier()\n","\n","# Making model training reproducible\n","np.random.seed(42)\n","\n","scores = cross_val_score(clf, X, y, scoring='accuracy', cv = cv)\n","\n","print(f\"All the accuracies are: {scores}\")\n","print(f\"And the average crossvalidation accuracy is: {scores.mean():.2f} +- {scores.std():.2f}\")"]},{"cell_type":"markdown","source":["Si estuvieramos en un problema desbalanceado, podríamos usar *Stratified k-fold cross-validation*. No es el caso, pero para practicar, podemos escribir la función de validación usando *StratifiedKFold*"],"metadata":{"id":"c4gC6BiMHi95"}},{"cell_type":"code","source":["# Tu código AQUÍ\n","\n"],"metadata":{"id":"iS0d-OP6HMMc"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"INNQMn8n4QX1"},"source":["- Vemos que para nuestro caso no hay diferencia.\n","- 0.95 es la evaluación del modelo (estimación del rendimiento).\n","- Pero el modelo final debe entrenarse con todos los datos disponibles."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"yNXty5034QX1"},"outputs":[],"source":["final_clf = tree.DecisionTreeClassifier()\n","# Making results reproducible, in case training a tree contains random decisions\n","np.random.seed(42)\n","# Now, we train it\n","final_clf.fit(X, y)\n","# final_clf contains the model that would be used by the company\n","# Its estimated accuracy is what we computed before"]},{"cell_type":"markdown","metadata":{"id":"prQCnIMt4QX1"},"source":["A continuación, tienes el código completo para la evaluación de validación cruzada (y también la obtención del modelo final al final)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"ZT8P6pJz4QX1"},"outputs":[],"source":["from sklearn import metrics\n","from sklearn import tree\n","from sklearn.model_selection import KFold, cross_val_score\n","\n","# create a k-fold crossvalidation iterator of k=5 folds\n","# shuffle = True randomly rearranges the dataframe\n","# random_state = 42 is for making the folds reproducible\n","cv = KFold(n_splits=5, shuffle=True, random_state=42)\n","clf = tree.DecisionTreeClassifier()\n","\n","# Making model training reproducible\n","np.random.seed(42)\n","\n","scores = cross_val_score(clf, X, y, scoring='accuracy', cv = cv)\n","\n","# print(f\"All the accuracies are: {scores}\")\n","print(f\"The average crossvalidation accuracy is: {scores.mean():.2f} +- {scores.std():.2f}\")\n","\n","final_clf = tree.DecisionTreeClassifier()\n","# Making results reproducible, in case training a tree contains random decisions\n","np.random.seed(42)\n","# Now, we train it\n","final_clf.fit(X, y)\n","# final_clf contains the model that would be used by the company"]},{"cell_type":"markdown","metadata":{"id":"q7FP2q4V4QX1"},"source":["# Modificación de los hiperparámetros de un árbol de decisión\n","\n","---"]},{"cell_type":"markdown","metadata":{"id":"ewZT0QkW4QX1"},"source":["https://scikit-learn.org/stable/modules/generated/sklearn.tree.DecisionTreeClassifier.html  "]},{"cell_type":"code","execution_count":null,"metadata":{"id":"7Q_jHJgb4QX1"},"outputs":[],"source":["from sklearn.model_selection import train_test_split\n","from sklearn import metrics\n","from sklearn import tree\n","\n","X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.33, random_state=42)"]},{"cell_type":"markdown","metadata":{"id":"C0eoV0e54QX2"},"source":["Veamos el efecto de cambiar de gini a entropía. Aquí utilizamos holdout. Parece que los resultados son exactamente los mismos."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"eN-4gAJg4QX2"},"outputs":[],"source":["# This loop checks what happens with the two criterions\n","for criterion in [\"gini\", \"entropy\"]:\n","    clf = tree.DecisionTreeClassifier(criterion=criterion)\n","    np.random.seed(42)\n","    clf.fit(X_train,y_train)\n","    y_test_pred = clf.predict(X_test)\n","    accuracy_tree = metrics.accuracy_score(y_test, y_test_pred)\n","    print(f\"With {criterion}: {accuracy_tree:.2f}\")"]},{"cell_type":"markdown","metadata":{"id":"KgKDmuV84QX2"},"source":["Veamos los efectos de *maximum_depth*. \"None\" representa la máxima profundidad posible."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"UHjavMZk4QX2"},"outputs":[],"source":["for max_depth in [1,2,3,None]:\n","    clf = tree.DecisionTreeClassifier(max_depth=max_depth)\n","    np.random.seed(42)\n","    clf.fit(X_train,y_train)\n","    y_test_pred = clf.predict(X_test)\n","    accuracy_tree = metrics.accuracy_score(y_test, y_test_pred)\n","    print(f\"With max_depth {max_depth}: {accuracy_tree:.2f}\")"]},{"cell_type":"markdown","metadata":{"id":"qZ1FfR1l4QX2"},"source":["Parece que max_depth=2 es suficiente. Visualicemos un árbol con profundidad máxima de 2"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"2ipRiYjF4QX2"},"outputs":[],"source":["import matplotlib.pyplot as plt\n","\n","fig = plt.figure(figsize=(10,8))\n","clf = tree.DecisionTreeClassifier(max_depth=2)\n","np.random.seed(42)\n","clf.fit(X_train,y_train)\n","\n","_ = tree.plot_tree(clf,\n","                   feature_names = iris_meta.feature_names,\n","                   class_names=iris_meta.target_names,\n","                   filled=True)"]},{"cell_type":"markdown","metadata":{"id":"5oy55XCGY8XB"},"source":["Podemos comprobar si al limitar la profundidad del árbol hemos perdido precisión"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"YLQSZ1hVY8XB"},"outputs":[],"source":["import pandas as pd\n","import seaborn as sns\n","\n","result1 = metrics.classification_report(y_test, y_test_pred)\n","print(\"Classification Report (max_depth=2):\",)\n","print (result1)\n","\n","# Creates a confusion matrix\n","cm = metrics.confusion_matrix(y_test, y_test_pred)\n","\n","# Transform to df for easier plotting\n","cm_df = pd.DataFrame(cm,\n","                     index = ['setosa','versicolor','virginica'],\n","                     columns = ['setosa','versicolor','virginica'])\n","# Transform to df for easier plotting\n","cm_df = pd.DataFrame(cm,\n","                     index = ['setosa','versicolor','virginica'],\n","                     columns = ['setosa','versicolor','virginica'])\n","\n","plt.figure(figsize=(5.5,4))\n","sns.heatmap(cm_df, annot=True)\n","plt.title('Accuracy (max_depth=2): {0:.3f}'.format(accuracy_tree))\n","plt.ylabel('True label')\n","plt.xlabel('Predicted label')\n","plt.show()"]},{"cell_type":"markdown","metadata":{"id":"4-Vh6got4QX2"},"source":["Veamos los efectos de *min_samples_split*. El valor por defecto es 2."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"e1BeviS94QX2"},"outputs":[],"source":["for min_samples in [2,10,20,30,100]:\n","    clf = tree.DecisionTreeClassifier(min_samples_split=min_samples)\n","    np.random.seed(42)\n","    clf.fit(X_train,y_train)\n","    y_test_pred = clf.predict(X_test)\n","    accuracy_tree = metrics.accuracy_score(y_test, y_test_pred)\n","    print(f\"With min_samples_split {min_samples}: {accuracy_tree:.2f}\")\n"]},{"cell_type":"markdown","metadata":{"id":"ZkXAUuStFKid"},"source":["Por último, comprobemos otro hiperparámetro llamado *min_impurity_decrease*: significa que sólo se crea un nuevo nivel del árbol si la ganancia de información (es decir, la disminución de la entropía o gini) es mayor que el valor de *min_impurity_decrease*. Es otra forma de controlar la profundidad del árbol."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Su3NPiA94QX3"},"outputs":[],"source":["for min_impurity_decrease in np.linspace(0,2,num=10):\n","    clf = tree.DecisionTreeClassifier(min_impurity_decrease=min_impurity_decrease)\n","    np.random.seed(42)\n","    clf.fit(X_train,y_train)\n","    y_test_pred = clf.predict(X_test)\n","    accuracy_tree = metrics.accuracy_score(y_test, y_test_pred)\n","    print(f\"With min_impurity_decrease {min_impurity_decrease}: {accuracy_tree:.2f}\")\n","\n","import matplotlib.pyplot as plt\n","fig = plt.figure(figsize=(10,8))\n","\n","clf = tree.DecisionTreeClassifier(min_impurity_decrease=0.2)\n","np.random.seed(42)\n","clf.fit(X_train,y_train)\n","\n","_ = tree.plot_tree(clf,\n","                   feature_names = iris_meta.feature_names,\n","                   class_names=iris_meta.target_names,\n","                   filled=True)"]},{"cell_type":"markdown","metadata":{"id":"RAJYiJr1OYmz"},"source":["##Ejercicio: comprobar el efecto de cambiar el hiperparámetro número de vecinos de KNN.\n","\n","Ayuda sobre KNN en sklearn:\n","\n","from sklearn.neighbors import KNeighborsClassifier\n","\n","Lo definimos como:\n","\n","clf = KNeighborsClassifier()\n","\n","(y lo entrenamos con .fit).\n","\n","Más ayuda del clasificador KNN de sklearn:\n","\n","https://scikit-learn.org/stable/modules/generated/sklearn.neighbors.KNeighborsClassifier.html\n","\n","Cambiamos el parámetro *n_neighbors* con valores de 1 a 100 tomados de cinco en cinco (*range(1,100,5)*), por ejemplo.\n","\n","\n","Puedes comprobar lo que ocurre al cambiar el hiperparámetro *weights*: :\n","\n","- ‘uniform’ : Todos los puntos de cada vecindario se ponderan por igual.\n","\n","- ‘distance’ : Ponderar los puntos por la inversa de su distancia. Es decir, los vecinos más cercanos de un nuevo punto de consulta tendrán una mayor influencia que los vecinos más alejados.\n","\n","---\n","\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"BnCg6h_MCPM4"},"outputs":[],"source":["# Tu código AQUÍ\n","\n"]},{"cell_type":"markdown","metadata":{"id":"ZFONncXU4QX3"},"source":["# Tratamiento de variables categóricas en DecisionTreeClassifier\n","---\n","- La implementación de árboles de Sklearn **NO PUEDE** tratar con variables categóricas (en la mayoría de los casos).\n","- Deben convertirse en variables ficticias (one-hot-encoding).\n","- Los árboles Sklearn tampoco pueden tratar con valores faltantes (missing values)\n","\n","El flujo de trabajo típico cuando se trabaja con scikit-learn es:\n","\n","1. Cargar los datos como un Pandas dataframe\n","\n","2. LLevar a cabo un proceso EDA (Exploratory Data Analysis) para entender/comprender sus datos. Y esto significa:\n","  - Cuántas instancias y atributos hay\n","  - Qué tipo de atributos hay (numéricos o categóricos). Esto se hace para comprobar si hay características categóricas que deban ser codificadas (como dummies / one-hot-encoding)\n","  - Qué atributos tienen valores faltantes, y cuántos\n","  - Si se trata de un problema de clasificación o de regresión (variable de salida es una clase o un valor continuo), y en caso de clasificación, si la clase está desequilibrada.\n","\n","3. Codificar el Pandas dataframe como una matriz numpy (transformar los valores categóricos y los valores faltantes)\n","\n","4. Diseñar modelos de aprendizaje automático con scikit-learn"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"y5_UHlRVJqtw"},"outputs":[],"source":["# This is for uploading tennis.txt from your hard drive into Colab\n","\n","#from google.colab import files\n","#import io\n","#uploaded = files.upload()\n","#tennis_tmp = io.BytesIO(uploaded['tennis.txt'])"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"ueoMs8uHk2vw"},"outputs":[],"source":["# There are other ways of accessing files from google colab\n","# https://neptune.ai/blog/google-colab-dealing-with-files\n","# Code below allows to mount your google drive and load data directly from GD\n","from google.colab import drive\n","drive.mount('/content/gdrive')"]},{"cell_type":"code","source":["import os\n","# Cambia el directorio donde tengas los tutoriales con el fichero \"tennis.txt\"\n","os.chdir(\"/content/gdrive/MyDrive/Docencia/AprendizajeAutomatico/2023-2024/Practicas/Tutoriales\")"],"metadata":{"id":"VucReN_mNRib"},"execution_count":null,"outputs":[]},{"cell_type":"code","execution_count":null,"metadata":{"id":"vGwW0YWM4QX3"},"outputs":[],"source":["import pandas as pd\n","tennis_df = pd.read_csv(\"tennis.txt\", sep=\",\")"]},{"cell_type":"markdown","metadata":{"id":"8-jAr-k5KQfe"},"source":["Podemos comprobar el valos de las primeras instancias del dataframe"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"C6fyz7ae4QX3"},"outputs":[],"source":["tennis_df.head()"]},{"cell_type":"markdown","metadata":{"id":"_mdqmMcdKYcl"},"source":["El dataset es pequeño y podremos visualizarlo por completo"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"pGckFPDlKdc2"},"outputs":[],"source":["tennis_df"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Vw_EJnkwLD9d"},"outputs":[],"source":["print('The shape of the data table is:')\n","print('===============================')\n","print(tennis_df.shape)\n","print()\n","\n","print('The types of the attributes are:')\n","print('================================')\n","tennis_df.info()\n","\n","print()\n","\n","print('How many missing values per attribute:')\n","print('======================================')\n","print(tennis_df.isnull().sum())\n","\n","print()\n","\n","print('Fraction of missing values per attribute:')\n","print('======================================')\n","print(tennis_df.isnull().mean())\n"]},{"cell_type":"markdown","metadata":{"id":"_WpY6Yu8PPFm"},"source":["Por último, comprobamos si el dataset se encuentra desequilibrado. Vemos que no está demasiado desequilibrado."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"rG4XjXvQPJ61"},"outputs":[],"source":["print(tennis_df.Play.value_counts())\n","\n","print()\n","\n","print(tennis_df['Play'].value_counts())\n","\n","print()\n","\n","print(tennis_df['Play'].value_counts()/tennis_df['Play'].count())"]},{"cell_type":"markdown","metadata":{"id":"6T8D5USxa34S"},"source":["Ahora vamos a codificar:\n","- Nuestras características categóricas (Sky y Windy)\n","- La variable de salida (la clase: Play)\n","\n","Pero primero, vamos a separar la tabla de datos en entradas (X) y salidas (y)\n","\n","Vamos a utilizar un ColumnTransformer, que permite procesar sólo algunas columnas en particular, y deja a los demás como están\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"6XLHZORsa0yz"},"outputs":[],"source":["y_df = tennis_df['Play']\n","print(y_df)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"6SKomsD9bgQU"},"outputs":[],"source":["X_df = tennis_df.drop('Play', axis=1)\n","print(X_df)"]},{"cell_type":"markdown","metadata":{"id":"QzuEqVg3a1ea"},"source":["Utilizaremos la función ColumnTransformer, que permite procesar sólo algunas columnas en particular, y deja las demás intactas (*passthrough*). En este caso, procesaremos sólo las categóricas.\n","\n","La salida de esta transformación es una matriz numpy (!!)\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"wwdnSKoRA9CO"},"outputs":[],"source":["from sklearn.preprocessing import OneHotEncoder, MinMaxScaler\n","from sklearn.compose import ColumnTransformer\n","\n","numeric_features = ['Temperature', 'Humidity']\n","categorical_features = ['Sky', 'Windy']\n","\"\"\"\n","preprocessor = ColumnTransformer(\n","    transformers = [\n","                    ('categorical', OneHotEncoder(handle_unknown='ignore'),\n","                                    categorical_features)\n","                    ],\n","                    remainder='passthrough'\n",")\n","\"\"\"\n","preprocessor = ColumnTransformer(\n","    transformers = [\n","                    ('categorical', OneHotEncoder(handle_unknown='ignore'),\n","                                    categorical_features),\n","                    ('scaler', MinMaxScaler(),\n","                                    numeric_features)\n","                    ],\n","                    remainder='passthrough'\n",")\n","\n","preprocessor.fit(X_df)\n","X = preprocessor.transform(X_df)\n","\n","# Notice that now we have 7 columnos\n","print(X.shape)\n","print()\n","\n","# Notice that now the type of the data matrix is numpy, which can already be used by sklearn\n","print(type(X))\n","print()\n","\n","# The first three columns are the dummies for Sky, the second two columns are the dummies for Windy\n","# The last two columns are Temperature and Humidity, untouched\n","# Please, notice that the order of columns has changed (not important, in principle), but the last versions of sklearn\n","# return the names of variables, which is useful to understand where the variables come from\n","\n","print(X)\n","\n","from pprint import pprint\n","pprint(list(preprocessor.get_feature_names_out()))"]},{"cell_type":"markdown","metadata":{"id":"OETfim-Jw2P2"},"source":["Nos puede interesar que para las variables booleanas sólo codifique un variable."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"2WGtsLHX4QX4"},"outputs":[],"source":["from sklearn.preprocessing import OneHotEncoder\n","from sklearn.compose import ColumnTransformer\n","\n","# numeric_features = ['Temperature', 'Humidity']\n","categorical_features = ['Sky', 'Windy']\n","\n","preprocessor = ColumnTransformer(\n","    transformers = [\n","                    ('categorical', OneHotEncoder(drop='if_binary',\n","                                                  handle_unknown='ignore'),\n","                                    categorical_features)\n","                    ],\n","                    remainder='passthrough'\n",")\n","\n","preprocessor.fit(X_df)\n","X = preprocessor.transform(X_df)\n","\n","# Notice that now we have 6 columns\n","print(X.shape)\n","print()\n","\n","# Notice that now the type of the data matrix is numpy, which can already be used by sklearn\n","print(type(X))\n","print()\n","\n","# The first three columns are the dummies for Sky, the second column is the dummy for Windy\n","# The last two columns are Temperature and Humidity, untouched\n","# Please, notice that the order of columns has changed, but the last versions of sklearn\n","# return the names of variables, which is useful to understand where the variables come from\n","\n","print(X)\n","\n","from pprint import pprint\n","pprint(list(preprocessor.get_feature_names_out()))"]},{"cell_type":"markdown","metadata":{"id":"bQ7O7ElKoyDp"},"source":["A veces lleva mucho tiempo enumerar todas las columnas categóricas. Podemos utilizar seleccionar los tipos de columnas que queremos transformar."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"U4ATnGozohJZ"},"outputs":[],"source":["from sklearn.preprocessing import OneHotEncoder\n","from sklearn.compose import ColumnTransformer\n","from sklearn.compose import make_column_selector as selector\n","\n","# numeric_features = ['Temperature', 'Humidity']\n","categorical_features = ['Sky', 'Windy']\n","\n","preprocessor = ColumnTransformer(\n","    transformers = [\n","                    ('categorical', OneHotEncoder(drop='if_binary',\n","                                                  handle_unknown='ignore'),\n","                                    selector(dtype_include=[\"object\",\"bool\"]))\n","                    ],\n","                    remainder='passthrough'\n",")\n","\n","preprocessor.fit(X_df)\n","X = preprocessor.transform(X_df)\n","\n","# Notice that now we have 6 columns\n","print(X.shape)\n","print()\n","\n","# Notice that now the type of the data matrix is numpy, which can already be used by sklearn\n","print(type(X))\n","print()\n","\n","# The first three columns are the dummies for Sky, the second two columns are the dummies for Windy\n","# The last two columns are Temperature and Humidity, untouched\n","# Please, notice that the order of columns has changed (not important, in principle)\n","\n","print(X)"]},{"cell_type":"markdown","metadata":{"id":"wDXkuInIduqR"},"source":["Ahora, necesitamos codificar la clase (variable de salida) con enteros. Lo hacemos con *LabelEncoder*."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"vfihfozAyP-N"},"outputs":[],"source":["from sklearn import preprocessing\n","le = preprocessing.LabelEncoder()\n","le.fit(y_df)\n","\n","y = le.transform(y_df)\n","print(y)\n","\n","# Actually, it seems that sklearn can use the original y_df, so we could have done y = y_df"]},{"cell_type":"markdown","metadata":{"id":"n7w5y9ASdgVF"},"source":["Por último, tenemos el código para evaluar los árboles de decisión y construir el modelo final.\n","Lo aplicamos a nuestro modelo (X,y)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"B9sDNvM74QX4"},"outputs":[],"source":["from sklearn.model_selection import train_test_split\n","from sklearn import metrics\n","from sklearn import tree\n","import numpy as np\n","\n","# Train/test split\n","X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.33, random_state=42)\n","\n","# Here, we set our model to classification tree\n","clf = tree.DecisionTreeClassifier()\n","np.random.seed(42) # reproducibility\n","# We train it\n","clf.fit(X_train, y_train)\n","# We obtain predictions on the test set\n","y_test_pred = clf.predict(X_test)\n","# We compute accuracy\n","accuracy_tree = metrics.accuracy_score(y_test, y_test_pred)\n","print(f\"Accuracy of the tree: {accuracy_tree} \")\n","result1 = metrics.classification_report(y_test, y_test_pred)\n","print(\"Classification Report:\",)\n","print (result1)\n","\n","# We finally compute the final model with all available data\n","\n","final_clf = tree.DecisionTreeClassifier()\n","np.random.seed(42)  # reproducibility\n","final_clf.fit(X, y)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"rXrsYLX1assM"},"outputs":[],"source":["import matplotlib.pyplot as plt\n","fig = plt.figure(figsize=(10,8))\n","\n","_ = tree.plot_tree(final_clf,\n","                   feature_names = list(sorted(X_df['Sky'].unique())) + list(sorted(X_df['Windy'].unique())) + ['Temperature', 'Humidity'],\n","                   class_names= list(sorted(y_df.unique())),\n","                   filled=True)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"_3dHiNA53rdr"},"outputs":[],"source":["!pip install statsmodels"]},{"cell_type":"markdown","metadata":{"id":"Yvrx1JqN4yZm"},"source":["Dado que se trata de un problema de clasificación de 2 clases, podemos construir un intervalo de confianza para la precisión.\n","Podemos ver que es muy inexacta ..."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"JHaewbCxiBQ6"},"outputs":[],"source":["from statsmodels.stats.proportion import proportion_confint\n","print(f\"Only {len(y_test)} instances on the testing partition\")\n","print(f\"Tree accuracy: {accuracy_tree} \")\n","proportion_confint(len(y_test)* accuracy_tree, len(y_test), method=\"wilson\" )"]},{"cell_type":"markdown","source":["Una clasificación bastante desastrosa, lo podemos comprobar viendos sus valores AUC\n","## Ejercicio\n","Dibujar la curva ROC con el valor AUC\n","\n","---"],"metadata":{"id":"SeyLcRt1XXdI"}},{"cell_type":"code","source":["# Tu código AQUÍ\n","\n","\n"],"metadata":{"id":"heM79lJ7W-GL"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"ADfIN64L4QX4"},"source":["# Árboles de regresión con evaluación holdout"]},{"cell_type":"markdown","metadata":{"id":"L2r3ejoO4QX4"},"source":["Carguemos el dataset de Boston y comprobemos su descripción. Sus datos sobre precios de la vivienda en función de las características de la zona"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"jRUioZm04QX4"},"outputs":[],"source":["import numpy as np\n","import matplotlib.pyplot as plt\n","\n","import pandas as pd\n","import seaborn as sns\n","\n","from sklearn.datasets import fetch_california_housing\n","housing_meta = fetch_california_housing()\n","\n","X = housing_meta.data\n","y = housing_meta.target"]},{"cell_type":"code","source":["print(housing_meta.DESCR)"],"metadata":{"id":"Vsf9UdUHw7M-"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"YPR-pKr4xdTs"},"execution_count":null,"outputs":[]},{"cell_type":"code","execution_count":null,"metadata":{"id":"4R6sLJoI4QX4"},"outputs":[],"source":["print(housing_meta.keys())\n","print(X.shape, y.shape)\n"]},{"cell_type":"markdown","metadata":{"id":"ONchO8X1Y8XE"},"source":["Comencemos con un pre-proceso y comprobar si hay valores nulos"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"k7B8XZxmY8XE"},"outputs":[],"source":["# Lo pasamos a un data frame para comprobar su contenido\n","housing = pd.DataFrame(housing_meta.data, columns=housing_meta.feature_names)\n","# Añadimos la variable objetivo a este data frame\n","housing['MEDV'] = housing_meta.target\n","housing.head()"]},{"cell_type":"markdown","metadata":{"id":"KYIjmBZiY8XE"},"source":["Comprobamos si tienen valores nulos las columnas"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"LgzQ1lDgY8XE"},"outputs":[],"source":["housing.isnull().sum()"]},{"cell_type":"markdown","metadata":{"id":"CB7h0-0nY8XE"},"source":["Pintamos la distribución de los valores de la variable objetivo"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"gQE4f6BDY8XF"},"outputs":[],"source":["sns.set(rc={'figure.figsize':(11.7,8.27)})\n","sns.distplot(housing['MEDV'], bins=30)\n","plt.show()"]},{"cell_type":"markdown","metadata":{"id":"iIACv-ATY8XF"},"source":["Parece que sigue una distribución normal con algunos outliers (podríamos pensar en eliminarlos). Pasemos a analizar la correlación de las variables."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"vZkG6GHrY8XF"},"outputs":[],"source":["correlation_matrix = housing.corr().round(2)\n","sns.heatmap(data=correlation_matrix, annot=True)"]},{"cell_type":"markdown","metadata":{"id":"o1zVU-WGY8XF"},"source":["Podemos ver ciertas cosas para más adelante:\n","- Las variables `'Longitude'` y `'Latitude'` tienen una alta correlación (-0.92). ¿Es necesario tener las dos en el modelo que voy a construir?\n","- La variable objetivo `'MDEV'` está bastante correlada con la variable `'MedInc'` (0.69). ¿Sería suficiente generar un modelo con sólo esta variable? Vamos a dibujar la distribucción de los puntos de estas dos variables"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"A-UzmYdlY8XF"},"outputs":[],"source":["plt.figure(figsize=(20, 5))\n","\n","features = ['MedInc']\n","target = housing['MEDV']\n","\n","for i, col in enumerate(features):\n","    plt.subplot(1, len(features) , i+1)\n","    x = housing[col]\n","    y = target\n","    plt.scatter(x, y, marker='o')\n","    plt.title(col)\n","    plt.xlabel(col)\n","    plt.ylabel('MEDV')"]},{"cell_type":"markdown","metadata":{"id":"APMXvjj14QX4"},"source":["El principal cambio es que utilizamos un DecisionTreeRegressor y la métrica es ahora RMSE"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"TZ21Wukc4QX5"},"outputs":[],"source":["from sklearn.model_selection import train_test_split\n","from sklearn import metrics\n","from sklearn import tree\n","import numpy as np\n","\n","# Train/test split\n","X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.33, random_state=42)\n","\n","# Here, we set our model to classification tree\n","regr = tree.DecisionTreeRegressor()\n","np.random.seed(42) # reproducibility\n","# We train it\n","regr.fit(X_train, y_train)\n","# We obtain predictions on the test set\n","y_test_pred = regr.predict(X_test)\n","# We compute accuracy\n","rmse_tree = np.sqrt(metrics.mean_squared_error(y_test, y_test_pred))\n","r2_tree = metrics.r2_score(y_test, y_test_pred)\n","print(f\"RMSE of the tree: {rmse_tree}\")\n","print(f\"R2 of the tree: {r2_tree}\")\n","\n","# We would have to compute the final model with all available data\n","# Not done here, only interested on test RMSE"]},{"cell_type":"markdown","metadata":{"id":"JZnnKRiQyNOH"},"source":["¿Es mejor que un regresor trivial?"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"RQiGRsbpyN0z"},"outputs":[],"source":["from sklearn.dummy import DummyRegressor\n","regr_mean = DummyRegressor(strategy=\"mean\")\n","regr_mean.fit(X_train, y_train)\n","rmse_mean = np.sqrt(metrics.mean_squared_error(y_test, regr_mean.predict(X_test)))\n","r2_mean = metrics.r2_score(y_test, regr_mean.predict(X_test))\n","\n","print(f\"RMSE of the tree: {rmse_tree}\")\n","print(f\"RMSE of dummy(mean): {rmse_mean}\")\n","print(f\"R2 of the tree: {r2_tree}\")\n","print(f\"R2 of dummy(mean): {r2_mean}\")\n","print(f\"RMSE ratio tree/dummy(mean): {rmse_tree/rmse_mean}\")\n","print(f\"R2 ratio tree/dummy(mean): {r2_tree/r2_mean}\")"]},{"cell_type":"markdown","metadata":{"id":"qERBOOPQ0yzI"},"source":["¿Y MAE?"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"wJVUu1JO0xiJ"},"outputs":[],"source":["mae_tree = metrics.mean_absolute_error(y_test, y_test_pred)\n","\n","from sklearn.dummy import DummyRegressor\n","regr_median = DummyRegressor(strategy=\"median\")\n","regr_median.fit(X_train, y_train)\n","mae_median = metrics.mean_absolute_error(y_test, regr_median.predict(X_test))\n","\n","print(f\"MAE of the tree: {mae_tree}\")\n","print(f\"MAE of dummy(median): {mae_median}\")\n","print(f\"MAE ratio tree/dummy(median): {mae_tree/mae_median}\")"]},{"cell_type":"markdown","metadata":{"id":"DQ0bJSuz4QX5"},"source":["El árbol tiene más de 10 niveles y es muy difícil de visualizar. Visualicemos uno con sólo cuatro niveles"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"V1-moMyM4QX5"},"outputs":[],"source":["regr = tree.DecisionTreeRegressor(max_depth=4)\n","np.random.seed(42) # reproducibility\n","# We train it\n","regr.fit(X_train, y_train)\n","\n","import matplotlib.pyplot as plt\n","fig = plt.figure(figsize=(25,20))\n","\n","_ = tree.plot_tree(regr,\n","                   feature_names = housing_meta.feature_names,\n","                   filled=True)"]},{"cell_type":"markdown","metadata":{"id":"6in65Vuu4QX5"},"source":["# **Entrenaremos con Model tree (usados para la regresión)**"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"GBI37quR4QX5"},"outputs":[],"source":["# More info about this implementation of model trees\n","# https://towardsdatascience.com/linear-tree-the-perfect-mix-of-linear-model-and-decision-tree-2eaed21936b7\n","# https://pypi.org/project/linear-tree/\n","\n","# IMPORTANT: This implementation of Model Trees is able to deal with Categorical Features (whose values are encoded as integers 0,1,2, ...)\n","# in order to use categorical features, the parameter categorical_features must be used."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"rMTJR8N61dsr"},"outputs":[],"source":["!pip install --upgrade linear-tree\n","#!pip install pydot\n","#!pip install graphviz"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"-rHXrM9B4QX5"},"outputs":[],"source":["from sklearn.model_selection import train_test_split\n","from sklearn import metrics\n","from sklearn import tree\n","\n","from sklearn.linear_model import LinearRegression\n","from lineartree import LinearTreeRegressor\n","\n","# Train/test split\n","X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.33, random_state=42)\n","\n","regr = LinearTreeRegressor(base_estimator=LinearRegression())\n","np.random.seed(42) # reproducibility\n","# We train it\n","regr.fit(X_train, y_train)\n","# We obtain predictions on the test set\n","y_test_pred = regr.predict(X_test)\n","# We compute accuracy\n","rmse_tree = np.sqrt(metrics.mean_squared_error(y_test, y_test_pred))\n","r2_tree = metrics.r2_score(y_test, y_test_pred)\n","print(f\"RMSE of the tree: {rmse_tree}\")\n","print(f\"R2 of the tree: {r2_tree}\")\n","\n","# We would have to compute the final model with all available data\n","# Not done here, only interested on test RMSE"]},{"cell_type":"markdown","metadata":{"id":"AZMaKH1q86_U"},"source":["Pintamos el árbol resultante"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"xmXWQ1SD4QX5"},"outputs":[],"source":["regr.plot_model(feature_names=housing_meta.feature_names)"]},{"cell_type":"markdown","metadata":{"id":"OeGJtMBM9Djz"},"source":["Para comprobar los modelos lineales en las hojas, tenemos que ir hoja a hoja. A continuación se muestra los coeficientes del modelo lineal en el nodo 7"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"OgM4K5gS4Ac4"},"outputs":[],"source":["leaves = regr.summary(feature_names=housing_meta.feature_names, only_leaves=True, max_depth=None)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Ika31dVu4xgB"},"outputs":[],"source":["leaves"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"8jpFdkSA6C-H"},"outputs":[],"source":["from pprint import pprint\n","model_7_coefs = leaves[7]['models'].coef_\n","model_7_intercept = leaves[7]['models'].intercept_\n","pprint(list(zip(housing_meta.feature_names, model_7_coefs)))\n","pprint(f'intercept: {model_7_intercept}')"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"4As4OQmq8FFE"},"outputs":[],"source":["?LinearTreeRegressor"]},{"cell_type":"markdown","source":["## Ejercicio.\n","Obtener un modelo de árbolo lineal que sólo utilice la variable `'MedInc'` para estimar `'MEDV''.\n","Dibujar el árbol resultante y obtener los coeficiente de algún nodo hoja."],"metadata":{"collapsed":false,"id":"T65a_QZlY8XG"}},{"cell_type":"code","execution_count":null,"outputs":[],"source":["boston1 = housing[['MedInc', 'MEDV']].copy()\n","\n","y = boston1['MEDV']\n","X = boston1.drop('MEDV', axis = 1)\n","\n","X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.33, random_state=42)\n","\n","# Tu código aquí\n","regr = LinearTreeRegressor(base_estimator=LinearRegression())\n","np.random.seed(42) # reproducibility\n","# We train it\n","regr.fit(X_train, y_train)\n","# We obtain predictions on the test set\n","y_test_pred = regr.predict(X_test)\n","# We compute accuracy\n","rmse_tree = np.sqrt(metrics.mean_squared_error(y_test, y_test_pred))\n","print(f\"RMSE of the tree: {rmse_tree}\")\n"],"metadata":{"id":"fbm8c5AeY8XG"}},{"cell_type":"code","source":["# obtenemos información de los nodos hojas del árbol\n","# Tu código aquí\n","leaves = regr.summary(feature_names=['MedInc'], only_leaves=True, max_depth=None)\n","leaves\n","\n","\n","\n"],"metadata":{"id":"gBLsFUHVFCiJ"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Obtenemos los coeficientes para el nodo 12 (u otro cualquiera)\n","model_12_coefs = leaves[12]['models'].coef_\n","model_12_intercept = leaves[12]['models'].intercept_\n","pprint(list(zip(['MedInc'], model_12_coefs)))\n","pprint(f'intercept: {model_12_intercept}')\n"],"metadata":{"id":"zEPeQCjcEcM_"},"execution_count":null,"outputs":[]}],"metadata":{"colab":{"provenance":[]},"interpreter":{"hash":"b3ba2566441a7c06988d0923437866b63cedc61552a5af99d1f4fb67d367b25f"},"kernelspec":{"display_name":"Python 3 (ipykernel)","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.10.5"}},"nbformat":4,"nbformat_minor":0}