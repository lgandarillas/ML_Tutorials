{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "s_jEsE8W6HbZ"
   },
   "source": [
    "# Random Forest\n",
    "\n",
    "En este tutorial trabajaremos con Random Forest y su uso en un problema de regresión."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 3523,
     "status": "ok",
     "timestamp": 1710781862265,
     "user": {
      "displayName": "MIGUEL ANGEL PATRICIO GUISADO",
      "userId": "13328848534964446801"
     },
     "user_tz": -60
    },
    "id": "8W2YIMIi7SyF"
   },
   "outputs": [],
   "source": [
    "# Tratamiento de datos\n",
    "# ==============================================================================\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# Gráficos\n",
    "# ==============================================================================\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Preprocesado y modelado\n",
    "# ==============================================================================\n",
    "from sklearn.datasets import fetch_california_housing\n",
    "from sklearn.ensemble import RandomForestRegressor, ExtraTreesRegressor\n",
    "from sklearn import metrics\n",
    "from sklearn.model_selection import RandomizedSearchCV, KFold, GridSearchCV\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.feature_selection import SelectKBest, f_regression\n",
    "from sklearn.pipeline import Pipeline\n",
    "#from sklearn.model_selection import RepeatedKFold\n",
    "#from sklearn.model_selection import GridSearchCV\n",
    "#from sklearn.model_selection import ParameterGrid\n",
    "from sklearn.inspection import permutation_importance\n",
    "#import multiprocessing\n",
    "\n",
    "from scipy.stats import randint as sp_randint\n",
    "\n",
    "# Configuración warnings\n",
    "# ==============================================================================\n",
    "import warnings\n",
    "warnings.filterwarnings('once')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "skKrA10C-h_e"
   },
   "outputs": [],
   "source": [
    "def rmse(y_test, y_test_pred):\n",
    "  \"\"\" Este es mi cálculo del error cuadrático medio \"\"\"\n",
    "  return np.sqrt(metrics.mean_squared_error(y_test, y_test_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "T5bni26R7hn3"
   },
   "source": [
    "Utilizaremos el dataset de Housing de California"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ZL6u47Le7jNh"
   },
   "outputs": [],
   "source": [
    "housing = fetch_california_housing()\n",
    "\n",
    "X = housing.data\n",
    "y = housing.target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 227,
     "status": "ok",
     "timestamp": 1709575565091,
     "user": {
      "displayName": "MIGUEL ANGEL PATRICIO GUISADO",
      "userId": "13328848534964446801"
     },
     "user_tz": -60
    },
    "id": "hJQzFXQErj1V",
    "outputId": "f3ef5602-9758-467a-fe6e-7e989dddceab"
   },
   "outputs": [],
   "source": [
    "print(housing.DESCR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "j7PQK5_08EYF"
   },
   "outputs": [],
   "source": [
    "# Holdout para la evaluación del modelo. 33% de los datos disponibles para test\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y,test_size=0.33, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "N_n7pkh3XwQX"
   },
   "outputs": [],
   "source": [
    "# Para dibujar las predicciones y los valores reales\n",
    "def pintaResultados (reg, n):\n",
    "  plt.subplots(figsize=(30, 5))\n",
    "  x = np.arange(y_test[:n].size)\n",
    "  pred = reg.predict(X=X_test)\n",
    "  plt.plot(x, y_test[:n], 'b.', label='Verdaderas')\n",
    "  plt.plot(x, pred[:n], 'g^', label='Predichas')\n",
    "  plt.legend();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "PpkoVv3l-GjJ"
   },
   "source": [
    "Los parámetros más importantes de RF son:\n",
    "- `n_estimators`; número de árboles incluidos en el modelo.\n",
    "- `max_depth`: profundidad máxima que pueden alcanzar los árboles.\n",
    "- `min_samples_split`: número mínimo de observaciones que debe de tener un nodo para que pueda dividirse. Si es un valor decimal se interpreta como fracción del total de observaciones de entrenamiento ceil(`min_samples_split` * `n_samples`).\n",
    "- `min_samples_leaf`: número mínimo de observaciones que debe de tener cada uno de los nodos hijos para que se produzca la división. Si es un valor decimal se interpreta como fracción del total de observaciones de entrenamiento ceil(`min_samples_split` * `n_samples`).\n",
    "- `max_features`: número de predictores considerados a en cada división. Puede ser:\n",
    "  - Un valor entero\n",
    "  - Una fracción del total de predictores. Se calcula como `max(1, int(max_features * n_features_in_))`. Si su valor es `1.0` tiene en cuenta todos los predictores\n",
    "  - `“sqrt”`, raiz cuadrada del número total de predictores.\n",
    "  - `“log2”`, log2 del número total de predictores.\n",
    "  - `None`, utiliza todos los predictores (igual que `1.0`)\n",
    "- `oob_score`: Si se calcula o no el out-of-bag R^2. Por defecto es `False` ya que aumenta el tiempo de entrenamiento."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 207
    },
    "executionInfo": {
     "elapsed": 1333,
     "status": "ok",
     "timestamp": 1709575585594,
     "user": {
      "displayName": "MIGUEL ANGEL PATRICIO GUISADO",
      "userId": "13328848534964446801"
     },
     "user_tz": -60
    },
    "id": "XeXx3Bsf-LAF",
    "outputId": "ad96e78c-e997-4acd-ccc2-9084f1f82c6c"
   },
   "outputs": [],
   "source": [
    "# Creación del modelo\n",
    "# ==============================================================================\n",
    "regr_rf = RandomForestRegressor(\n",
    "            n_estimators = 10,\n",
    "            max_depth    = None,\n",
    "            max_features = 1.0,\n",
    "            oob_score    = False,\n",
    "            n_jobs       = -1,\n",
    "            random_state = 42\n",
    "         )\n",
    "\n",
    "# Entrenamiento del modelo\n",
    "# ==============================================================================\n",
    "regr_rf.fit(X_train, y_train)\n",
    "print(f\"RMSE de RF: {rmse(y_test, regr_rf.predict(X=X_test))}\")\n",
    "pintaResultados(regr_rf,50)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7-BXQb4NEgX3"
   },
   "source": [
    "## Optimización de parámetros\n",
    "### Número de árboles `n_estimators`\n",
    "Este no es un parámetro crítico. Podemos ir creciendo el número de árboles ya que no supone ningún problema de overfitting. Sin embargo, llegado a un valor, no se van a obtener mejores resultados. Por lo tanto, conviene coger el número de árboles justo.\n",
    "\n",
    "En los modelos RF se cuenta con el valor de Out_of_Bag, que refleja las instancias que no han sido utilizadas en el entrenamiento de un determinado árbol. Las podemos usar para calcular la eficiencia de los modelos sin tener que recurrir a la validación cruzada. El valor de `oob_score` es R^2."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 470
    },
    "executionInfo": {
     "elapsed": 73406,
     "status": "ok",
     "timestamp": 1709575677404,
     "user": {
      "displayName": "MIGUEL ANGEL PATRICIO GUISADO",
      "userId": "13328848534964446801"
     },
     "user_tz": -60
    },
    "id": "2dT9e5kQFre4",
    "outputId": "c35f1d19-c591-474c-c24c-fd37905be3fe"
   },
   "outputs": [],
   "source": [
    "# Validación empleando el Out-of-Bag error\n",
    "# ==============================================================================\n",
    "train_scores = []\n",
    "oob_scores   = []\n",
    "\n",
    "# Valores evaluados\n",
    "estimator_range = range(1, 150, 20)\n",
    "\n",
    "# Bucle para entrenar un modelo con cada valor de n_estimators y extraer su error\n",
    "# de entrenamiento y de Out-of-Bag.\n",
    "for n_estimators in estimator_range:\n",
    "    modelo = RandomForestRegressor(\n",
    "                n_estimators = n_estimators,\n",
    "                max_depth    = None,\n",
    "                max_features = 1.0,\n",
    "                oob_score    = True,\n",
    "                n_jobs       = -1,\n",
    "                random_state = 42\n",
    "             )\n",
    "    modelo.fit(X_train, y_train)\n",
    "    train_scores.append(modelo.score(X_train, y_train))\n",
    "    oob_scores.append(modelo.oob_score_)\n",
    "\n",
    "# Gráfico con la evolución de los errores\n",
    "fig, ax = plt.subplots(figsize=(6, 3.84))\n",
    "ax.plot(estimator_range, train_scores, label=\"train scores\")\n",
    "ax.plot(estimator_range, oob_scores, label=\"out-of-bag scores\")\n",
    "ax.plot(estimator_range[np.argmax(oob_scores)], max(oob_scores),\n",
    "        marker='o', color = \"red\", label=\"max score\")\n",
    "ax.set_ylabel(\"R^2\")\n",
    "ax.set_xlabel(\"n_estimators\")\n",
    "ax.set_title(\"Evolución del out-of-bag-error vs número árboles\")\n",
    "ax.set_ylim([0.7,None])\n",
    "plt.legend();\n",
    "print(f\"Valor óptimo de n_estimators: {estimator_range[np.argmax(oob_scores)]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "pV0vx72BOntO"
   },
   "source": [
    "Podemos comprobar que si ampliamos el valor de `n_estimators` el valor de R^2 crece, pero con unos aumentos muy pequeños. Por la gráfica, nos podemos quedar con un valor de 40"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "6579ilBuJm_0",
    "outputId": "48bfd849-f434-42be-b6a1-397ea0f0fbb0"
   },
   "outputs": [],
   "source": [
    "# Validación empleando k-cross-validation y neg_root_mean_squared_error\n",
    "# ==============================================================================\n",
    "train_scores = []\n",
    "cv_scores    = []\n",
    "\n",
    "# Valores evaluados\n",
    "estimator_range = range(1, 150, 20)\n",
    "\n",
    "# Bucle para entrenar un modelo con cada valor de n_estimators y extraer su error\n",
    "# de entrenamiento y de k-cross-validation.\n",
    "for n_estimators in estimator_range:\n",
    "\n",
    "    modelo = RandomForestRegressor(\n",
    "                n_estimators = n_estimators,\n",
    "                max_depth    = None,\n",
    "                max_features = 1.0,\n",
    "                oob_score    = False,\n",
    "                n_jobs       = -1,\n",
    "                random_state = 42\n",
    "             )\n",
    "\n",
    "    # Error de train\n",
    "    modelo.fit(X_train, y_train)\n",
    "    train_scores.append(rmse(y_test, modelo.predict(X=X_test)))\n",
    "\n",
    "    # Error de validación cruzada\n",
    "    scores = cross_val_score(\n",
    "                estimator = modelo,\n",
    "                X         = X_train,\n",
    "                y         = y_train,\n",
    "                scoring   = 'neg_root_mean_squared_error',\n",
    "                cv        = 3\n",
    "             )\n",
    "    # Se agregan los scores de cross_val_score() y se pasa a positivo\n",
    "    cv_scores.append(-1*scores.mean())\n",
    "\n",
    "# Gráfico con la evolución de los errores\n",
    "fig, ax = plt.subplots(figsize=(6, 3.84))\n",
    "ax.plot(estimator_range, train_scores, label=\"train scores\")\n",
    "ax.plot(estimator_range, cv_scores, label=\"cv scores\")\n",
    "ax.plot(estimator_range[np.argmin(cv_scores)], min(cv_scores),\n",
    "        marker='o', color = \"red\", label=\"min score\")\n",
    "ax.set_ylabel(\"root_mean_squared_error\")\n",
    "ax.set_xlabel(\"n_estimators\")\n",
    "ax.set_title(\"Evolución del cv-error vs número árboles\")\n",
    "ax.set_ylim([None,0.6])\n",
    "plt.legend();\n",
    "print(f\"Valor óptimo de n_estimators: {estimator_range[np.argmin(cv_scores)]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "o67A031mQwKG"
   },
   "source": [
    "Igualmente, se comprueba que con 40 árboles el error se estabiliza. Por lo que tomamos un valor de `n_estimators` de 40"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "lTtD9CRAPnHd"
   },
   "source": [
    "### Máximo número de atributos\n",
    "El valor de `max_features` es uno de los hiperparámetros más importantes de RF, ya que es el que permite controlar cuánto se decorrelacionan los árboles entre sí.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "-3k7qbGyQGA3",
    "outputId": "e6fc2cf7-1e55-4fa5-dc39-b118dfff34b2"
   },
   "outputs": [],
   "source": [
    "# Validación empleando el Out-of-Bag error\n",
    "# ==============================================================================\n",
    "train_scores = []\n",
    "oob_scores   = []\n",
    "\n",
    "# Valores evaluados\n",
    "max_features_range = range(1, X_train.shape[1] + 1, 1)\n",
    "\n",
    "# Bucle para entrenar un modelo con cada valor de max_features y extraer su error\n",
    "# de entrenamiento y de Out-of-Bag.\n",
    "for max_features in max_features_range:\n",
    "    modelo = RandomForestRegressor(\n",
    "                n_estimators = 40,\n",
    "                max_depth    = None,\n",
    "                max_features = max_features,\n",
    "                oob_score    = True,\n",
    "                n_jobs       = -1,\n",
    "                random_state = 42\n",
    "             )\n",
    "    modelo.fit(X_train, y_train)\n",
    "    train_scores.append(modelo.score(X_train, y_train))\n",
    "    oob_scores.append(modelo.oob_score_)\n",
    "\n",
    "# Gráfico con la evolución de los errores\n",
    "fig, ax = plt.subplots(figsize=(6, 3.84))\n",
    "ax.plot(max_features_range, train_scores, label=\"train scores\")\n",
    "ax.plot(max_features_range, oob_scores, label=\"out-of-bag scores\")\n",
    "ax.plot(max_features_range[np.argmax(oob_scores)], max(oob_scores),\n",
    "        marker='o', color = \"red\")\n",
    "ax.set_ylabel(\"R^2\")\n",
    "ax.set_xlabel(\"max_features\")\n",
    "ax.set_title(\"Evolución del out-of-bag-error vs número de predictores\")\n",
    "plt.legend();\n",
    "print(f\"Valor óptimo de max_features: {max_features_range[np.argmax(oob_scores)]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "pkdIxZwQRSvX",
    "outputId": "6a414fac-aa41-41ca-8820-2092481baac5"
   },
   "outputs": [],
   "source": [
    "# Validación empleando k-cross-validation y neg_root_mean_squared_error\n",
    "# ==============================================================================\n",
    "train_scores = []\n",
    "cv_scores    = []\n",
    "\n",
    "# Valores evaluados\n",
    "max_features_range = range(1, X_train.shape[1] + 1, 1)\n",
    "\n",
    "# Bucle para entrenar un modelo con cada valor de max_features y extraer su error\n",
    "# de entrenamiento y de k-cross-validation.\n",
    "for max_features in max_features_range:\n",
    "\n",
    "    modelo = RandomForestRegressor(\n",
    "                n_estimators = 100,\n",
    "                max_depth    = None,\n",
    "                max_features = max_features,\n",
    "                oob_score    = False,\n",
    "                n_jobs       = -1,\n",
    "                random_state = 42\n",
    "             )\n",
    "\n",
    "    # Error de train\n",
    "    modelo.fit(X_train, y_train)\n",
    "    train_scores.append(rmse(y_test, modelo.predict(X=X_test)))\n",
    "\n",
    "    # Error de validación cruzada\n",
    "    scores = cross_val_score(\n",
    "                estimator = modelo,\n",
    "                X         = X_train,\n",
    "                y         = y_train,\n",
    "                scoring   = 'neg_root_mean_squared_error',\n",
    "                cv        = 3\n",
    "             )\n",
    "    # Se agregan los scores de cross_val_score() y se pasa a positivo\n",
    "    cv_scores.append(-1*scores.mean())\n",
    "\n",
    "# Gráfico con la evolución de los errores\n",
    "fig, ax = plt.subplots(figsize=(6, 3.84))\n",
    "ax.plot(max_features_range, train_scores, label=\"train scores\")\n",
    "ax.plot(max_features_range, cv_scores, label=\"cv scores\")\n",
    "ax.plot(max_features_range[np.argmin(cv_scores)], min(cv_scores),\n",
    "        marker='o', color = \"red\", label=\"min score\")\n",
    "ax.set_ylabel(\"root_mean_squared_error\")\n",
    "ax.set_xlabel(\"max_features\")\n",
    "ax.set_title(\"Evolución del cv-error vs número de predictores\")\n",
    "plt.legend();\n",
    "print(f\"Valor óptimo de max_features: {max_features_range[np.argmin(cv_scores)]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "SHXQtAh9TLGm"
   },
   "source": [
    "Analizando los dos resultados, entendemos que el mejor parámetro para `max_features` está para un valor de 3."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zzTsNGzbUxkg"
   },
   "source": [
    "## Búsqueda de parámetros con Random Search\n",
    "La búsqueda de parámetros no la podemos hacer de forma secuencial porque unos parámetros pueden influir en otros. Vamos a realizar una búsqueda usando Random Search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Zy4hijlzVLJg",
    "outputId": "640c8afd-88a2-41fd-b498-a21134d5f82a"
   },
   "outputs": [],
   "source": [
    "# Quitar comentario si se quiere evaluar el modelo\n",
    "# outer = KFold(n_splits=5, shuffle=True, random_state=42)\n",
    "\n",
    "param_grid = {'n_estimators': sp_randint(10,200),\n",
    "              'max_features': sp_randint(1,X.shape[1])}\n",
    "\n",
    "inner = KFold(n_splits=3, shuffle=True, random_state=42)\n",
    "\n",
    "budget = 20\n",
    "# Cross-validation (3-fold) para la búsqueda de hiper-parámetros\n",
    "regr = RandomizedSearchCV(RandomForestRegressor(),\n",
    "                         param_grid,\n",
    "                         scoring='neg_mean_squared_error',\n",
    "                         cv=inner,\n",
    "                         refit=True,\n",
    "                         n_jobs=-1, verbose=1,\n",
    "                         n_iter=budget\n",
    "                        )\n",
    "\n",
    "np.random.seed(42)\n",
    "regr.fit(X=X_train, y=y_train)\n",
    "\n",
    "# Quitar comentario si se quiere evaluar el modelo\n",
    "\"\"\"\n",
    "scores = -cross_val_score(regr,\n",
    "                            X, y,\n",
    "                            scoring='neg_mean_squared_error',\n",
    "                            cv = outer)\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "uMgzgH1ExqtL",
    "outputId": "c2992066-aea5-466b-94b0-3f4fc586d584"
   },
   "outputs": [],
   "source": [
    "print(f\"RMSE de RF con búsqueda de hiperparámetros: {rmse(y_test, regr.predict(X=X_test))}\")\n",
    "pintaResultados(regr,50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "V3CLrwwfokga",
    "outputId": "ab5080f4-3ab3-4a02-ee5f-74a0bdf56562"
   },
   "outputs": [],
   "source": [
    "regr.best_params_, -regr.best_score_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5G0QkdL5ou9J"
   },
   "source": [
    "Los mejores parámetros son un número de árboles de 97 y un número máximo de características de 3."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "rQtqNxBUmpCQ",
    "outputId": "c51ec8f4-48ad-464b-bc9a-59ffde4c1e3a"
   },
   "outputs": [],
   "source": [
    "# Descomentar si vamos a estimar el error de mi modelo\n",
    "\"\"\"\n",
    "print(scores)\n",
    "# Nos devuelve MSE, calculo RMSE\n",
    "scores = np.sqrt(scores)\n",
    "# La media de los 5-fold con cross validation. Es la estimación del error de mi modelo.\n",
    "print(f\"{scores.mean()} +- {scores.std()}\")\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "BrKxmjFxq27h"
   },
   "source": [
    "Al poner el parámetro `refit=True` se reentrena el modelo indicando los valores óptimos en sus argumentos. Este reentrenamiento se hace automáticamente y el modelo resultante se encuentra almacenado en `.best_estimator_`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "US-W-Rj-m10G"
   },
   "outputs": [],
   "source": [
    "modelo_final = regr.best_estimator_\n",
    "# Entrenamos con todos los datos para el modelo final\n",
    "_ = modelo_final.fit(X,y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4CY-UM2bHvzR"
   },
   "source": [
    "## Importancia por pureza\n",
    "Calculamos la importancia por la pureza de los nodos. Cuantifica el incremento total en la pureza de los nodos debido a divisiones en las que participa el predictor (promedio de todos los árboles)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "s1s7Nl48peeJ",
    "outputId": "e3957a66-ec1a-4fd9-87f3-a29d90240e1d"
   },
   "outputs": [],
   "source": [
    "importancia_predictores = pd.DataFrame(\n",
    "                            {'predictor': housing.feature_names,\n",
    "                             'importancia': modelo_final.feature_importances_}\n",
    "                            )\n",
    "print(\"Importancia de los predictores en el modelo\")\n",
    "print(\"-------------------------------------------\")\n",
    "importancia_predictores.sort_values('importancia', ascending=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vPzAuxbZMt7n"
   },
   "source": [
    "## Importancia por permutación\n",
    "Identifica la influencia que tiene cada predictor sobre una determinada métrica de evaluación del modelo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "A2jpNEXoz3Zp",
    "outputId": "d4d5923b-c55f-4302-cd8a-654af853dae3"
   },
   "outputs": [],
   "source": [
    "from sklearn.inspection import permutation_importance\n",
    "import multiprocessing\n",
    "\n",
    "importancia = permutation_importance(\n",
    "                estimator    = modelo_final,\n",
    "                X            = X_train,\n",
    "                y            = y_train,\n",
    "                n_repeats    = 5,\n",
    "                scoring      = 'neg_root_mean_squared_error',\n",
    "                n_jobs       = multiprocessing.cpu_count() - 1,\n",
    "                random_state = 42\n",
    "             )\n",
    "\n",
    "# Se almacenan los resultados (media y desviación) en un dataframe\n",
    "df_importancia = pd.DataFrame(\n",
    "                    {k: importancia[k] for k in ['importances_mean', 'importances_std']}\n",
    "                 )\n",
    "# df_importancia['feature'] = X_train.columns\n",
    "df_importancia['feature'] = housing.feature_names\n",
    "df_importancia.sort_values('importances_mean', ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "fEXwhPuk0YsA",
    "outputId": "ebc92e3c-11ff-4c7f-9d81-fb4ac5815f7b"
   },
   "outputs": [],
   "source": [
    "# Gráfico\n",
    "fig, ax = plt.subplots(figsize=(5, 6))\n",
    "df_importancia = df_importancia.sort_values('importances_mean', ascending=True)\n",
    "ax.barh(\n",
    "    df_importancia['feature'],\n",
    "    df_importancia['importances_mean'],\n",
    "    xerr=df_importancia['importances_std'],\n",
    "    align='center',\n",
    "    alpha=0\n",
    ")\n",
    "ax.plot(\n",
    "    df_importancia['importances_mean'],\n",
    "    df_importancia['feature'],\n",
    "    marker=\"D\",\n",
    "    linestyle=\"\",\n",
    "    alpha=0.8,\n",
    "    color=\"r\"\n",
    ")\n",
    "ax.set_title('Importancia de los predictores (train)')\n",
    "ax.set_xlabel('Incremento del error tras la permutación');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xnhcuwuydU0M"
   },
   "source": [
    "# EXTRA (EXTREMLY RANDOMIZED) TREES\n",
    "Son otro tipo de ensemble por bagging muy parecidos a los Random Forest. Se diferencian en:\n",
    "\n",
    "1.   No aplican bootstraping y utilizan todas las muestras disponibles\n",
    "2.   A la hora de elegir el nodo de partición utilizan un valor aleatorio para los atributos disponibles, quedándose con el mejor. Esto hace que sean más rápidos.\n",
    "\n",
    "Hay dos parámetros de interés, pero nosotros no los vamos a usar en la búsqueda de hiperparámetros:\n",
    "- `criterion` ={“squared_error”, “absolute_error”, “friedman_mse”, “poisson”}, default=”squared_error” Mide la calidad de la partición realizada.\n",
    "- `bootstrapbool`, default=False. Si se utiliza bootstraping en la selección de muestras. Con el valor de `False`, se utilizan todas las muestras.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "AL18ygUUiWmD",
    "outputId": "674c686b-d59a-4294-b7ce-fef44361788b"
   },
   "outputs": [],
   "source": [
    "# Quitar comentario si se quiere evaluar el modelo\n",
    "# outer = KFold(n_splits=3, shuffle=True, random_state=42)\n",
    "\n",
    "# espacio de búsqueda\n",
    "param_grid = {'n_estimators': sp_randint(10,200),\n",
    "              'max_features': sp_randint(1,X.shape[1])}\n",
    "\n",
    "inner = KFold(n_splits=3, shuffle=True, random_state=42)\n",
    "\n",
    "budget = 20\n",
    "# Cross-validation (3-fold) para la búsqueda de hiper-parámetros\n",
    "regr = RandomizedSearchCV(ExtraTreesRegressor(),\n",
    "                         param_grid,\n",
    "                         scoring='neg_mean_squared_error',\n",
    "                         refit=True,\n",
    "                         cv=inner,\n",
    "                         n_jobs=-1, verbose=1,\n",
    "                         n_iter=budget\n",
    "                        )\n",
    "np.random.seed(42)\n",
    "regr.fit(X=X_train, y=y_train)\n",
    "\n",
    "# Quitar comentario si se quiere evaluar el modelo\n",
    "\"\"\"\n",
    "scores = -cross_val_score(regr,\n",
    "                            X, y,\n",
    "                            scoring='neg_mean_squared_error',\n",
    "                            cv = outer)\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "CZ4mmojaz5ov",
    "outputId": "a32d66d9-e5df-4ce4-f0a3-46b9ec54d7f5"
   },
   "outputs": [],
   "source": [
    "print(f\"RMSE de RF con búsqueda de hiperparámetros: {rmse(y_test, regr.predict(X=X_test))}\")\n",
    "pintaResultados(regr,50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "XoGI7yIcoYWf",
    "outputId": "20d95a46-1cd4-4742-bcf2-75dc05da96cb"
   },
   "outputs": [],
   "source": [
    "regr.best_params_, -regr.best_score_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "NX4NwtC6fwoz"
   },
   "source": [
    "Al poner el parámetro `refit=True` se reentrena el modelo indicando los valores óptimos en sus argumentos. Este reentrenamiento se hace automáticamente y el modelo resultante se encuentra almacenado en `.best_estimator_`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "8o6QsuW8jIaK"
   },
   "outputs": [],
   "source": [
    "modelo_final_Extra = regr.best_estimator_\n",
    "# Entrenamos con todos los datos para el modelo final\n",
    "_ = modelo_final_Extra.fit(X,y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "id": "0_fZmzOpOCj0"
   },
   "source": [
    "# Selección de características (opcional, no necesario para la práctica)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "id": "dqHBW_tAOCj0"
   },
   "source": [
    "Vamos a conocer una forma de reducir el número de características de entrada de nuestros modelos. En sklearn existe una función `SelectKBest`. Junto con esta función se define otra función (en este caso `f_regression`, ya que estamos ante un problema de regresión). Además, se introduce un parámetro K para indicar que sólo queremos quedarnos con las K variables de entrada para hacer la regresión. `SelectKBest` evalúa el efecto individual de cada una de las variables de entrada y las ordena en función del error que comete.\n",
    "\n",
    "El seleccionar un conjunto de variables, nos permitirá contar con problemas más pequeños (con menos variables de entrada)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "_aGH3Pd5OCj1"
   },
   "outputs": [],
   "source": [
    "scaler = StandardScaler()\n",
    "selector = SelectKBest(f_regression)\n",
    "extratrees = ExtraTreesRegressor(max_features=4, n_estimators = 167 )\n",
    "\n",
    "pipe_select_scale_extratrees = Pipeline([\n",
    "    ('scale', scaler),\n",
    "    ('select', selector),\n",
    "    ('extratrees', extratrees)])\n",
    "\n",
    "# Vamos a comprobar que valor de K sería el mejor\n",
    "param_grid = {'select__k': list(range(1,9))}\n",
    "inner = KFold(n_splits=3, shuffle=True, random_state = 42 )\n",
    "# Como es pequeño podemos usar GridSearch\n",
    "tune_select_scale_extratrees = GridSearchCV(pipe_select_scale_extratrees,\n",
    "                                     param_grid,\n",
    "                                     scoring=\"neg_mean_squared_error\",\n",
    "                                     cv=inner\n",
    "                                     )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "id": "QliHwEjyOCj1"
   },
   "source": [
    "El modelo es entrenado"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "kqOuhsuwOCj1",
    "outputId": "67e01413-8db8-4a87-e131-869f122fef88"
   },
   "outputs": [],
   "source": [
    "np.random.seed(42)\n",
    "tune_select_scale_extratrees.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "0DubPCPIOCj1",
    "outputId": "e5d8cf8b-1620-4832-f0f1-f037001e938a"
   },
   "outputs": [],
   "source": [
    "# Comprobamos el valor de K seleccionado\n",
    "tune_select_scale_extratrees.best_params_, np.sqrt(-tune_select_scale_extratrees.best_score_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "6EnBk35SOCj1",
    "outputId": "5308d3f8-d0dc-40b2-a491-1ee356a3c920"
   },
   "outputs": [],
   "source": [
    "trained_pipeline = tune_select_scale_extratrees.best_estimator_\n",
    "\n",
    "print(f\"Features selected: {trained_pipeline.named_steps['select'].get_support()}\")\n",
    "\n",
    "print(f\"Locations where features selected: {np.where(trained_pipeline.named_steps['select'].get_support())}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "bybetYZlOCj1",
    "outputId": "8fe134f9-46fd-45e7-8525-df026f21389a"
   },
   "outputs": [],
   "source": [
    "# También podemos comprobar el rendimiento para cada número de funciones\n",
    "from pprint import pprint\n",
    "\n",
    "pprint(list(zip(tune_select_scale_extratrees.cv_results_['param_select__k'].data, -tune_select_scale_extratrees.cv_results_['mean_test_score'])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "FU0HEIbuOCj1",
    "outputId": "7c5dd4ae-f10a-4c24-a876-af7163646332"
   },
   "outputs": [],
   "source": [
    "# Pintamos el error en función de los valores de K\n",
    "import matplotlib.pyplot as plt\n",
    "plt.plot(tune_select_scale_extratrees.cv_results_['param_select__k'].data, -tune_select_scale_extratrees.cv_results_['mean_test_score'])\n",
    "plt.ylabel('MSE')\n",
    "plt.xlabel('Number of features')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "id": "w7Y7BOR6OCj1"
   },
   "source": [
    "Pero el resultado importante es la evaluación del modelo con las muestras de test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "9_qk2hzuOCj1",
    "outputId": "741c56a2-94ee-4e54-80f8-f87e29c3713d"
   },
   "outputs": [],
   "source": [
    "# from sklearn.metrics import mean_squared_error\n",
    "print(f\"RMSE de Extra con selección de características: {rmse(y_test, trained_pipeline.predict(X=X_test))}\")\n",
    "pintaResultados(trained_pipeline,50)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "id": "ck0Tu2D-OCj1"
   },
   "source": [
    "Para nuestro caso, no hemos podido reducir el número de variables de entrada."
   ]
  }
 ],
 "metadata": {
  "accelerator": "TPU",
  "colab": {
   "provenance": [],
   "toc_visible": true
  },
  "gpuClass": "standard",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
