{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "HMG0JZbqqbaI"
   },
   "source": [
    "# SVM como clasificador"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7ZPJW_yNzE2D"
   },
   "source": [
    "## SVM lineal\n",
    "En *Scikit Learn* pueden encontrarse tres implementaciones distintas del algoritmo Suport Vector Machine:\n",
    "*   Las clases `sklearn.svm.SVC` y `sklearn.svm.NuSVC` permiten crear modelos SVM de clasificación empleando kernel lineal, polinomial, radial o sigmoide. La diferencia es que `SVC` controla la regularización a través del hiperparámetro `C`, mientras que `NuSVC` lo hace con el número máximo de vectores soporte permitidos.\n",
    "*  La clase `sklearn.svm.LinearSVC` permite ajustar modelos SVM con kernel lineal. Es similar a SVC cuando el parámetro `kernel='linear'`, pero utiliza un algoritmo más rápido.\n",
    "\n",
    "Las mismas implementaciones están disponibles para regresión en las clases: `sklearn.svm.SVR`, `sklearn.svm.NuSVR` y `sklearn.svm.LinearSVR`.\n",
    "\n",
    "Se ajusta primero un modelo SVM con kernel lineal y después uno con kernel radial, y se compara la capacidad de cada uno para clasificar correctamente las observaciones."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "hfnckzrpPtOo"
   },
   "outputs": [],
   "source": [
    "# Tratamiento de datos\n",
    "# ==============================================================================\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Gráficos\n",
    "# ==============================================================================\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib import style\n",
    "import seaborn as sns\n",
    "from mlxtend.plotting import plot_decision_regions\n",
    "\n",
    "# Preprocesado y modelado\n",
    "# ==============================================================================\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.compose import make_column_transformer\n",
    "\n",
    "# Configuración matplotlib\n",
    "# ==============================================================================\n",
    "plt.rcParams['image.cmap'] = \"bwr\"\n",
    "#plt.rcParams['figure.dpi'] = \"100\"\n",
    "plt.rcParams['savefig.bbox'] = \"tight\"\n",
    "style.use('ggplot') or plt.style.use('ggplot')\n",
    "\n",
    "# Configuración warnings\n",
    "# ==============================================================================\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "p-Oe_dSGPxEa"
   },
   "outputs": [],
   "source": [
    "# Datos\n",
    "# ==============================================================================\n",
    "datos = pd.read_csv('./SVM.csv')\n",
    "datos.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "BphChxfrP4Bl"
   },
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(6,4))\n",
    "ax.scatter(datos.X1, datos.X2, c=datos.y);\n",
    "ax.set_title(\"Datos SVM\");"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "z0K89-vsQApm"
   },
   "outputs": [],
   "source": [
    "#from os import X_OK\n",
    "# División de los datos en train y test\n",
    "# ==============================================================================\n",
    "X0 = datos.drop(columns = 'y')\n",
    "y0 = datos['y']\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "                                        X0,\n",
    "                                        y0.values.reshape(-1,1),\n",
    "                                        train_size   = 0.8,\n",
    "                                        random_state = 42,\n",
    "                                        shuffle      = True\n",
    "                                    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "IOESjj9VQULs"
   },
   "outputs": [],
   "source": [
    "# Creación del modelo SVM lineal\n",
    "# ==============================================================================\n",
    "modelo = SVC(C = 100, kernel = 'linear', random_state=42)\n",
    "modelo.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "rZMl4TXhQYn3"
   },
   "source": [
    "Al tratarse de un problema de dos dimensiones, se puede representar las regiones de clasificación."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "yoMSu1XbQcvz"
   },
   "outputs": [],
   "source": [
    "# Representación gráfica de los límites de clasificación\n",
    "# ==============================================================================\n",
    "# Grid de valores\n",
    "x = np.linspace(np.min(X_train.X1), np.max(X_train.X1), 50)\n",
    "y = np.linspace(np.min(X_train.X2), np.max(X_train.X2), 50)\n",
    "Y, X = np.meshgrid(y, x)\n",
    "grid = np.vstack([X.ravel(), Y.ravel()]).T\n",
    "\n",
    "# Predicción valores grid\n",
    "pred_grid = modelo.predict(grid)\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(6,4))\n",
    "ax.scatter(grid[:,0], grid[:,1], c=pred_grid, alpha = 0.2)\n",
    "ax.scatter(X_train.X1, X_train.X2, c=y_train, alpha = 1)\n",
    "\n",
    "# Vectores soporte\n",
    "ax.scatter(\n",
    "    modelo.support_vectors_[:, 0],\n",
    "    modelo.support_vectors_[:, 1],\n",
    "    s=200, linewidth=1,\n",
    "    facecolors='none', edgecolors='black'\n",
    ")\n",
    "\n",
    "# Hiperplano de separación\n",
    "ax.contour(\n",
    "    X,\n",
    "    Y,\n",
    "    modelo.decision_function(grid).reshape(X.shape),\n",
    "    colors = 'k',\n",
    "    levels = [-1, 0, 1],\n",
    "    alpha  = 0.5,\n",
    "    linestyles = ['--', '-', '--']\n",
    ")\n",
    "\n",
    "ax.set_title(\"Resultados clasificación SVM lineal\");"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "sEhhH8DtQilj"
   },
   "source": [
    "Se calcula el porcentaje de aciertos que tiene el modelo al predecir las observaciones de test (accuracy)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "S2ObF0VQQllG"
   },
   "outputs": [],
   "source": [
    "# Predicciones test\n",
    "# ==============================================================================\n",
    "predicciones = modelo.predict(X_test)\n",
    "predicciones"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "O9d5cMnAQpYR"
   },
   "outputs": [],
   "source": [
    "# Accuracy de test del modelo\n",
    "# ==============================================================================\n",
    "accuracy = accuracy_score(\n",
    "            y_true    = y_test,\n",
    "            y_pred    = predicciones,\n",
    "            normalize = True\n",
    "           )\n",
    "print(\"\")\n",
    "print(f\"El accuracy de test es: {100*accuracy}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "amB6eOLCQtLL"
   },
   "source": [
    "## SVM radial\n",
    "Se repite el ajuste del modelo, esta vez empleando un kernel radial y utilizando validación cruzada para identificar el valor óptimo de penalización `C`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "k_5O3DQkZp31"
   },
   "source": [
    "- `C` : float, default=1.0\n",
    "Parámetro de regularización. La fuerza de la regularización es inversamente proporcional a C. Debe ser estrictamente positivo.\n",
    "    \n",
    "- `gamma` : **Se usa en modelos no lineales**. Define cuánta curvatura queremos en la frontera de decisión:\n",
    "  - Gamma alta significa más curvatura.\n",
    "  - Gamma baja significa menos curvatura.\n",
    "\n",
    "  El parámetro `gamma` define hasta dónde llega la influencia de un único ejemplo de entrenamiento, donde valores bajos significan 'lejos' y valores altos significan 'cerca'. Los valores más bajos de `gamma` dan como resultado modelos con menor precisión, al igual que los valores más altos de `gamma`. Son los valores intermedios de `gamma` los que dan un modelo con buenos límites de decisión. Valores de gamma:\n",
    "  - `gamma` = 'scale' el valor será 1/(n_características * X.var())\n",
    "  - `gamma` = 'auto' el valor será 1/n_características\n",
    "  - `gamma` = float (no negativo)\n",
    "\n",
    "\n",
    "- Hay más hiperparámetros, pero estos dos son los importantes:\n",
    "  - https://scikit-learn.org/stable/modules/generated/sklearn.svm.SVR.html\n",
    "  - https://scikit-learn.org/stable/modules/generated/sklearn.svm.SVC.html\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "nNKrSdZpQsZp"
   },
   "outputs": [],
   "source": [
    "# Grid de hiperparámetros\n",
    "# ==============================================================================\n",
    "param_grid = {'C': np.logspace(-5, 7, 20)}\n",
    "\n",
    "# Búsqueda por validación cruzada\n",
    "# ==============================================================================\n",
    "grid = GridSearchCV(\n",
    "        estimator  = SVC(kernel= \"rbf\", gamma='scale'),\n",
    "        param_grid = param_grid,\n",
    "        scoring    = 'accuracy',\n",
    "        n_jobs     = -1,\n",
    "        cv         = 3,\n",
    "        verbose    = 0,\n",
    "        return_train_score = True\n",
    "      )\n",
    "\n",
    "# Se asigna el resultado a _ para que no se imprima por pantalla\n",
    "_ = grid.fit(X = X_train, y = y_train)\n",
    "\n",
    "# Resultados del grid\n",
    "# ==============================================================================\n",
    "resultados = pd.DataFrame(grid.cv_results_)\n",
    "resultados.filter(regex = '(param.*|mean_t|std_t)')\\\n",
    "    .drop(columns = 'params')\\\n",
    "    .sort_values('mean_test_score', ascending = False) \\\n",
    "    .head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "o6w2bSrtQ1C-"
   },
   "outputs": [],
   "source": [
    "# Mejores hiperparámetros por validación cruzada\n",
    "# ==============================================================================\n",
    "print(\"----------------------------------------\")\n",
    "print(\"Mejores hiperparámetros encontrados (cv)\")\n",
    "print(\"----------------------------------------\")\n",
    "print(grid.best_params_, \":\", grid.best_score_, grid.scoring)\n",
    "\n",
    "modelo = grid.best_estimator_\n",
    "C_best = grid.best_params_['C']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "d5b0IlFhQ4wa"
   },
   "outputs": [],
   "source": [
    "# Una función para representar los SVM\n",
    "# ==============================================================================\n",
    "def plotSVC(title):\n",
    "  # Grid de valores\n",
    "  x = np.linspace(np.min(X_train.X1), np.max(X_train.X1), 50)\n",
    "  y = np.linspace(np.min(X_train.X2), np.max(X_train.X2), 50)\n",
    "  Y, X = np.meshgrid(y, x)\n",
    "  grid = np.vstack([X.ravel(), Y.ravel()]).T\n",
    "\n",
    "  # Predicción valores grid\n",
    "  pred_grid = modelo.predict(grid)\n",
    "\n",
    "  fig, ax = plt.subplots(figsize=(6,4))\n",
    "  ax.scatter(grid[:,0], grid[:,1], c=pred_grid, alpha = 0.2)\n",
    "  ax.scatter(X_train.X1, X_train.X2, c=y_train, alpha = 1)\n",
    "\n",
    "  # Vectores soporte\n",
    "  ax.scatter(\n",
    "      modelo.support_vectors_[:, 0],\n",
    "      modelo.support_vectors_[:, 1],\n",
    "      s=200, linewidth=1,\n",
    "      facecolors='none', edgecolors='black'\n",
    "  )\n",
    "\n",
    "  # Hiperplano de separación\n",
    "  ax.contour(\n",
    "      X,\n",
    "      Y,\n",
    "      modelo.decision_function(grid).reshape(X.shape),\n",
    "      colors='k',\n",
    "      levels=[0],\n",
    "      alpha=0.5,\n",
    "      linestyles='-'\n",
    "  )\n",
    "\n",
    "  ax.set_title(title);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "SxOvHir6tImB"
   },
   "outputs": [],
   "source": [
    "plotSVC(\"Parámetro gamma por defecto\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Irj_3H8Kt5pW"
   },
   "source": [
    "Vamos a comprobar el efecto del parámetro gamma. Lo probaremos para varios valores de gamma y fijando el valor de C al encontrado anteriormente."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "uh4VVp_TuCTf"
   },
   "outputs": [],
   "source": [
    "gammas = [0.1, 1, 10, 100]\n",
    "for gamma in gammas:\n",
    "   modelo = SVC(kernel='rbf', C=C_best, gamma=gamma).fit(X_train, y_train)\n",
    "   plotSVC('gamma=' + str(gamma))\n",
    "   predicciones = modelo.predict(X_test)\n",
    "   accuracy = accuracy_score(\n",
    "            y_true    = y_test,\n",
    "            y_pred    = predicciones,\n",
    "            normalize = True\n",
    "           )\n",
    "   print(\"\")\n",
    "   print(f\"El accuracy de test es: {100*accuracy}% con gamma \" + str(gamma))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "wuSBEBPvxV1I"
   },
   "source": [
    "El mejor modelo es con un valor de gamma de 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "90tk9GOpxVc7"
   },
   "outputs": [],
   "source": [
    "modelo = SVC(kernel='rbf', C=C_best, gamma=1).fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "WuqhWPfxQ-21"
   },
   "outputs": [],
   "source": [
    "# Predicciones test\n",
    "# ==============================================================================\n",
    "predicciones = modelo.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "9hzNwzB6RCE_"
   },
   "outputs": [],
   "source": [
    "# Accuracy de test del modelo\n",
    "# ==============================================================================\n",
    "accuracy = accuracy_score(\n",
    "            y_true    = y_test,\n",
    "            y_pred    = predicciones,\n",
    "            normalize = True\n",
    "           )\n",
    "print(\"\")\n",
    "print(f\"El accuracy de test es: {100*accuracy}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "gNYBE1MjRFNs"
   },
   "outputs": [],
   "source": [
    "# Matriz de confusión de las predicciones de test\n",
    "# ==============================================================================\n",
    "confusion_matrix = pd.crosstab(\n",
    "    y_test.ravel(),\n",
    "    predicciones,\n",
    "    rownames=['Real'],\n",
    "    colnames=['Predicción']\n",
    ")\n",
    "confusion_matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8gbn8jKORIul"
   },
   "source": [
    "Con un modelo SVM de kernel radial se consigue clasificar correctamente el 85% de las observaciones de test."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "TUsUJHvgm8bw"
   },
   "outputs": [],
   "source": [
    "# Entregamos el modelo final entrenado con todos los datos\n",
    "modelo_final = SVC(kernel='rbf', C=C_best, gamma=1).fit(X0, y0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "RjO0ZHCkZp30"
   },
   "source": [
    "# SVM como regresor"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5tfYm7QAZp32"
   },
   "source": [
    "Primero, se cargan los datos, las entradas van a X, las salidas a y."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "a22f9-Wr1dBU"
   },
   "outputs": [],
   "source": [
    "# Datos\n",
    "# ==============================================================================\n",
    "datos = pd.read_csv('./Student_Marks.csv')\n",
    "datos.head(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6RIeFQXmDu9t"
   },
   "source": [
    "Queremos buscar una función que en función de las horas diarias de estudio, nos estime la nota media que vamos a sacar."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "7iEixxXwbfqc"
   },
   "outputs": [],
   "source": [
    "train, test = train_test_split(datos, test_size=0.2, random_state=42)\n",
    "\n",
    "# train y test datasets los ordenamos para poder dibujarlo bien\n",
    "train = train.sort_values('time_study')\n",
    "test = test.sort_values('time_study')\n",
    "\n",
    "X_train, X_test = train[['time_study']], test[['time_study']]\n",
    "y_train, y_test = train['Marks'], test['Marks']\n",
    "\n",
    "X = datos.iloc[:,1:2].values.astype(float)\n",
    "y = datos.iloc[:,2:3].values.astype(float)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ewBvEaZUcFjC"
   },
   "outputs": [],
   "source": [
    "y_train.describe(include=\"all\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "I4wsFFnnmYVy"
   },
   "source": [
    "Comprobamos el rango de la variable de salida va de 5.6 a 53.25\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "XQOtMG7hZp35"
   },
   "source": [
    "##SVM lineal"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7rqhsc9OARd3"
   },
   "source": [
    "En primer lugar, definamos nuestra función python para el RMSE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "TM7SzJLHAQhZ"
   },
   "outputs": [],
   "source": [
    "from sklearn import metrics\n",
    "\n",
    "def rmse(y_test, y_test_pred):\n",
    "  \"\"\" Este es mi cálculo del error cuadrático medio \"\"\"\n",
    "  return np.sqrt(metrics.mean_squared_error(y_test, y_test_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9-SI_kTr82RI"
   },
   "source": [
    "En primer lugar, obtenemos el modelo lineal con hiperparámetros por defecto"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "rU6g-6PZ87DS"
   },
   "outputs": [],
   "source": [
    "from sklearn import metrics\n",
    "from sklearn.svm import SVR\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "svr_lineal = SVR(kernel=\"linear\")\n",
    "escalar = StandardScaler()\n",
    "\n",
    "pipe_regr_lineal = Pipeline([\n",
    "    ('escalar', escalar),\n",
    "    ('SVM', svr_lineal)])\n",
    "\n",
    "np.random.seed(42)\n",
    "pipe_regr_lineal.fit(X=X_train, y=y_train)\n",
    "print(f\"RMSE of SVR with default hyper-pars: {rmse(y_test, pipe_regr_lineal.predict(X=X_test))}\")\n",
    "print(f\"Param C: {pipe_regr_lineal['SVM'].C}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "s9rHddlPivhu"
   },
   "source": [
    "Hacemos una búsqueda por el mejor parámetro C con una GridSearch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "GRknuZjUZp39"
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GridSearchCV, KFold\n",
    "from sklearn.model_selection import KFold\n",
    "\n",
    "\n",
    "param_grid = {'SVM__C': [0.1, 1, 10, 100]}\n",
    "\n",
    "\n",
    "inner = KFold(n_splits=3, shuffle=True, random_state=42)\n",
    "\n",
    "hpo_regr_lineal = GridSearchCV(pipe_regr_lineal,\n",
    "                        param_grid,\n",
    "                        scoring='neg_mean_squared_error',\n",
    "                        cv=inner,\n",
    "                        n_jobs=4, verbose=1)\n",
    "\n",
    "# Train the self-adjusting process\n",
    "np.random.seed(42)\n",
    "hpo_regr_lineal.fit(X=X_train, y=y_train)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "rAp6LFbJQ5Ns"
   },
   "source": [
    "Visualicemos:\n",
    "- Los mejores hiperparámetros y su puntuación (¡inner!).\n",
    "- La evaluación del modelo (¡outer!) con los datos de test y los mejores hiperparámetros.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "fZokHzT78udT"
   },
   "outputs": [],
   "source": [
    "print(f\"Best params: {hpo_regr_lineal.best_params_}, best score (inner!): {np.sqrt(-hpo_regr_lineal.best_score_)}\")\n",
    "# Now, the performance of regr is computed on the test partition\n",
    "print(f\"RMSE (outer!) of SVR with hyper-parameter tuning (grid-search): {rmse(y_test, hpo_regr_lineal.predict(X=X_test))}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "I3qFOHemQ5Ns"
   },
   "source": [
    "Observamos que el mejor valor de C es 100 que está en el límite del espacio de búsqueda. Podemos plantearnos ampliar el espacio de búsqueda y ver si mejoran los resultados."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "bEWAXvPVQ5Ns"
   },
   "outputs": [],
   "source": [
    "# Search space\n",
    "param_grid = {'SVM__C': [0.001, 0.01, 1, 10, 100, 1000, 10000, 50000, 100000]}\n",
    "\n",
    "hpo_regr_lineal = GridSearchCV(pipe_regr_lineal,\n",
    "                        param_grid,\n",
    "                        scoring='neg_mean_squared_error',\n",
    "                        cv=inner,\n",
    "                        n_jobs=4, verbose=1)\n",
    "\n",
    "# Train the self-adjusting process\n",
    "np.random.seed(42)\n",
    "hpo_regr_lineal.fit(X=X_train, y=y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "oVVtp2mvQ5Ns"
   },
   "outputs": [],
   "source": [
    "print(f\"Best params: {hpo_regr_lineal.best_params_}, best score (inner!): {np.sqrt(-hpo_regr_lineal.best_score_)}\")\n",
    "# Now, the performance of regr is computed on the test partition\n",
    "print(f\"RMSE (outer!) of SVR with hyper-parameter tuning (grid-search): {rmse(y_test, hpo_regr_lineal.predict(X=X_test))}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "kGSG86flsf-2"
   },
   "source": [
    "##SVM Radial"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "RLxk1UHOfAv4"
   },
   "source": [
    "Pasamos ahora a usar un Kernel radial. Y probamos con los parámetros por defecto"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "q63AloFefUOv"
   },
   "outputs": [],
   "source": [
    "svr_radial = SVR() #por defecto es el Kernel radial\n",
    "\n",
    "pipe_regr_radial_def = Pipeline([\n",
    "    ('escalar', escalar),\n",
    "    ('SVM', svr_radial)])\n",
    "\n",
    "np.random.seed(42)\n",
    "pipe_regr_radial_def.fit(X=X_train, y=y_train)\n",
    "\n",
    "print(f\"Param C: {pipe_regr_radial_def['SVM'].C}, y gamma: {pipe_regr_radial_def['SVM'].gamma}\")\n",
    "print(f\"RMSE of SVR with default hyper-pars: {rmse(y_test, pipe_regr_radial_def.predict(X=X_test))}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "eKtAZ0pFs1dK"
   },
   "source": [
    "## Búsqueda de parámetros en Grid"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "cDuH6boJh11C"
   },
   "source": [
    "Hacemos una búsqueda Grid para el mejor parámetro"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "7Rm7ACynh1Ta"
   },
   "outputs": [],
   "source": [
    "param_grid = {'SVM__C': [0.001, 0.01, 1, 10, 100, 1000, 10000, 100000],\n",
    "              'SVM__gamma': [0.00001, 0.0001, 0.001, 0.01, 0.1, 1]}\n",
    "\n",
    "\n",
    "inner = KFold(n_splits=3, shuffle=True, random_state=42)\n",
    "\n",
    "hpo_regr_radial = GridSearchCV(pipe_regr_radial_def,\n",
    "                               param_grid,\n",
    "                               scoring='neg_mean_squared_error',\n",
    "                               cv=inner,\n",
    "                               n_jobs=4, verbose=1)\n",
    "\n",
    "# Train the self-adjusting process\n",
    "np.random.seed(42)\n",
    "hpo_regr_radial.fit(X=X_train, y=y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "366oCan5iI-u"
   },
   "outputs": [],
   "source": [
    "print(f\"Best params: {hpo_regr_radial.best_params_}, best score (inner!): {np.sqrt(-hpo_regr_radial.best_score_)}\")\n",
    "# Now, the performance of regr is computed on the test partition\n",
    "print(f\"RMSE (outer!) of SVR with hyper-parameter tuning (grid-search): {rmse(y_test, hpo_regr_radial.predict(X=X_test))}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Hk6rBqZSZp4I"
   },
   "source": [
    "### Búsqueda de parámetros en Random"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "w7bZU45-Zp4I"
   },
   "source": [
    "Ahora, vamos a utilizar **Randomized Search** en lugar de gridsearch. Sólo se probarán 20 combinaciones de valores de hiperparámetros (budget=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "urjn49EAZp4J"
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import RandomizedSearchCV, KFold\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn import metrics\n",
    "from sklearn.svm import SVR\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "\n",
    "budget = 20\n",
    "hpo_regr_radial = RandomizedSearchCV(pipe_regr_radial_def,\n",
    "                              param_grid,\n",
    "                              scoring='neg_mean_squared_error',\n",
    "                              cv=inner,\n",
    "                              n_jobs=-1, verbose=1,\n",
    "                              n_iter=budget\n",
    "                             )\n",
    "np.random.seed(42)\n",
    "hpo_regr_radial.fit(X=X_train, y=y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "x2JJyJAn-6Ci"
   },
   "outputs": [],
   "source": [
    "print(f\"Best params: {hpo_regr_radial.best_params_}, best score (inner!): {np.sqrt(-hpo_regr_radial.best_score_)}\")\n",
    "# Now, the performance of regr is computed on the test partition\n",
    "print(f\"RMSE (outer!) of SVR with hyper-parameter tuning (grid-search): {rmse(y_test, hpo_regr_radial.predict(X=X_test))}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "NFSOSFLZQ5Nt"
   },
   "source": [
    "Hemos obtenido los mismos resultados, pero explorando menos posibilidades que con grid-search."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "N5hGE6XFQ5Nt"
   },
   "source": [
    "Para **Randomized Search**, podemos definir el espacio de búsqueda con distribuciones estadísticas, en lugar de utilizar valores particulares como hacíamos antes. A continuación puedes ver cómo utilizar una distribución `loguniform`.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "yk_1OVCHQ5Nt"
   },
   "outputs": [],
   "source": [
    "#from sklearn.utils.fixes import loguniform\n",
    "from scipy.stats import loguniform\n",
    "\n",
    "\n",
    "# Search space\n",
    "param_grid = {'SVM__C': loguniform(1e-1, 1e4),\n",
    "              'SVM__gamma': loguniform(1e-5, 1e1)}\n",
    "\n",
    "hpo_regr_radial = RandomizedSearchCV(pipe_regr_radial_def,\n",
    "                            param_grid,\n",
    "                            scoring='neg_mean_squared_error',\n",
    "                            cv=inner,\n",
    "                            n_jobs=4, verbose=0,\n",
    "                            n_iter=budget\n",
    "                        )\n",
    "np.random.seed(42)\n",
    "hpo_regr_radial.fit(X=X_train, y=y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "CRppQBOiA9fI"
   },
   "outputs": [],
   "source": [
    "print(f\"Best params: {hpo_regr_radial.best_params_}, best score (inner!): {np.sqrt(-hpo_regr_radial.best_score_)}\")\n",
    "# Now, the performance of regr is computed on the test partition\n",
    "print(f\"RMSE (outer!) of SVR with hyper-parameter tuning (grid-search): {rmse(y_test, hpo_regr_radial.predict(X=X_test))}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "NsnpfHGouI-r"
   },
   "source": [
    "## Comparativa entre lineal y radial"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "DrW3kz6k64Fy"
   },
   "source": [
    "Obtenemos y dibujamos las estimaciones para tres casos:\n",
    "\n",
    "\n",
    "*   Estimador con kernel lineal\n",
    "*   Estimador con kernel radial óptimo\n",
    "*   Estimador con kernel radial y parámetro gamma bajo\n",
    "*   Estimador con kernel radial y parámetro gamma alto\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "rVLym6sUuFor"
   },
   "outputs": [],
   "source": [
    "from matplotlib import pyplot as plt\n",
    "\n",
    "bestC = hpo_regr_radial.best_params_['SVM__C']\n",
    "\n",
    "#Usamos un modelo con un gamma baja\n",
    "pipe_regr_radial_gamma_baja = Pipeline([\n",
    "    ('escalar', escalar),\n",
    "    ('SVM', SVR(gamma=0.01))])\n",
    "\n",
    "#Usamos un modelo con un gamma alta\n",
    "pipe_regr_radial_gamma_alta = Pipeline([\n",
    "    ('escalar', escalar),\n",
    "    ('SVM', SVR(gamma=10))])\n",
    "\n",
    "np.random.seed(42)\n",
    "pipe_regr_radial_gamma_baja.fit(X=X_train, y=y_train)\n",
    "pipe_regr_radial_gamma_alta.fit(X=X_train, y=y_train)\n",
    "\n",
    "#### Predicciones ####\n",
    "train['linear_svr_pred'] = hpo_regr_lineal.predict(X_train)\n",
    "train['rbf_svr_pred_def'] = pipe_regr_radial_def.predict(X_train)\n",
    "train['rbf_svr_pred_opt'] = hpo_regr_radial.predict(X_train)\n",
    "train['rbf_svr_pred_baja'] = pipe_regr_radial_gamma_baja.predict(X_train)\n",
    "train['rbf_svr_pred_alta'] = pipe_regr_radial_gamma_alta.predict(X_train)\n",
    "\n",
    "#### Visualización ####\n",
    "plt.scatter(train['time_study'], train['Marks'])\n",
    "plt.plot(train['time_study'], train['linear_svr_pred'], color = 'orange', label = 'linear SVR')\n",
    "plt.plot(train['time_study'], train['rbf_svr_pred_def'], color = 'red',  label = 'rbf SVR defecto')\n",
    "plt.plot(train['time_study'], train['rbf_svr_pred_opt'], color = 'green', linestyle='--', linewidth=3, label = 'rbf SVR óptima')\n",
    "plt.plot(train['time_study'], train['rbf_svr_pred_baja'], color = 'blue', label = 'rbf SVR gamma baja')\n",
    "plt.plot(train['time_study'], train['rbf_svr_pred_alta'], color = 'yellow', label = 'rbf SVR gamma alta')\n",
    "plt.legend()\n",
    "plt.xlabel('Tiempo de estudio')\n",
    "plt.ylabel('Puntuación')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6p-w0M2n7UNK"
   },
   "source": [
    "Se puede observar en verde como el estimador radial óptimo es el mejor."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "LdfqhEw9Zp4W"
   },
   "source": [
    "## Obtención de los dos modelos para el cliente"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "O5w47vRHZp4X"
   },
   "source": [
    "Por último, necesitamos un modelo final, podemos obtenerlo entrenando `hpo_regr_radial` a todos los datos disponibles."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "0Kvg0RX-Zp4X"
   },
   "outputs": [],
   "source": [
    "np.random.seed(42)\n",
    "\n",
    "regrFinal_radial = hpo_regr_radial.fit(X,y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2XeQqmjrZp4e"
   },
   "source": [
    "## Bayesian Optimization (plus)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "YNwHbEIsZp4f"
   },
   "source": [
    "Para ello se utilizará OPTUNA. **Holdout** para la evaluación del modelo y **3-fold crossvalidation** para el ajuste de hiperparámetros (con **Model Based Optimization** ).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "e3KyrtMiivd9"
   },
   "outputs": [],
   "source": [
    "#Para acceder a diferentes distribuciones y otros métodos\n",
    "%pip install optuna\n",
    "#Para acceder a optuna.integration.OptunaSearchCV y permitir la integración con Scikit-Learn\n",
    "%pip install --upgrade optuna-integration[sklearn]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "qmxEk_w4cEb7"
   },
   "outputs": [],
   "source": [
    "import optuna\n",
    "from optuna.integration import OptunaSearchCV\n",
    "\n",
    "# Search space\n",
    "param_grid = {\n",
    "    'SVM__C': optuna.distributions.FloatDistribution(1e-1, 1e4, log=True),\n",
    "    'SVM__gamma': optuna.distributions.FloatDistribution(1e-4, 1e0, log=True)\n",
    "}\n",
    "\n",
    "hpo_regr_radial = OptunaSearchCV(pipe_regr_radial_def,\n",
    "                    param_grid,\n",
    "                    scoring='neg_mean_squared_error',\n",
    "                    #n_trials=budget,\n",
    "                    n_trials=50,\n",
    "                    cv=inner,\n",
    "                    n_jobs=1, verbose=1,\n",
    "                    timeout=600,\n",
    "                    random_state=42\n",
    "                    )\n",
    "\n",
    "np.random.seed(42)\n",
    "hpo_regr_radial.fit(X=X_train, y=y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ZIqeUTQ6Zp4k"
   },
   "outputs": [],
   "source": [
    "print(f\"Best params: {hpo_regr_radial.best_params_}, best score (inner!): {np.sqrt(-hpo_regr_radial.best_score_)}\")\n",
    "# Now, the performance of regr is computed on the test partition\n",
    "print(f\"RMSE (outer!) of SVR with hyper-parameter tuning (grid-search): {rmse(y_test, hpo_regr_radial.predict(X=X_test))}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2YSNFqSaF6Qx"
   },
   "source": [
    "Podemos comprobar si la optimización ha convergido"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "-bTKrvUWKgDR"
   },
   "outputs": [],
   "source": [
    "from optuna.visualization.matplotlib import plot_optimization_history\n",
    "fig = plt.figure(figsize=(25,20))\n",
    "trial = hpo_regr_radial.study_\n",
    "plot_optimization_history(trial)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vGtj4Zc0jKZz"
   },
   "source": [
    "También podemos visualizar la importancia de los distintos parámetros y el modelo final."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "n3_WqMA9jSQs"
   },
   "outputs": [],
   "source": [
    "from optuna.visualization.matplotlib import plot_param_importances\n",
    "fig = plt.figure(figsize=(25,20))\n",
    "trial = hpo_regr_radial.study_\n",
    "plot_param_importances(trial)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "nZ6ds0e3bcB8"
   },
   "outputs": [],
   "source": [
    "from optuna.visualization import plot_parallel_coordinate\n",
    "\n",
    "optuna_study = hpo_regr_radial.study_\n",
    "\n",
    "plot_parallel_coordinate(optuna_study)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "E--lognScago"
   },
   "outputs": [],
   "source": [
    "from optuna.visualization import plot_contour\n",
    "\n",
    "plot_contour(optuna_study, params=[\"SVM__C\", \"SVM__gamma\"])"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
